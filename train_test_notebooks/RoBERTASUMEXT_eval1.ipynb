{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RoBERTASUMEXT_eval1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlEffL_vvt0q",
        "outputId": "d80675d5-6e5d-4e73-eeb3-e50ca119a1c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp9RhWPtdXBw",
        "outputId": "1e893602-0250-4091-a2b3-022079e0e2da"
      },
      "source": [
        "# GPU Settings\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon May  3 06:43:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF-eIXqnmFd2",
        "outputId": "0f26ff55-7e0b-4e79-9b94-494eb45aab78"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqWYHEVEwMDV",
        "outputId": "150a39ff-2827-4505-a760-5f2dece836e7"
      },
      "source": [
        "cd /content/drive/MyDrive/nn4nlp_project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u536tcr0yqdC",
        "outputId": "8190ce70-84e8-4b89-c7b9-38cc90990ca1"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/   \u001b[01;34mlogs\u001b[0m/    \u001b[01;34mpyrouge\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34mxsum_abs_baseline\u001b[0m/\n",
            "\u001b[01;34mfiles\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34mrefs\u001b[0m/     \u001b[01;34mtemp\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGdCcpqQwZ4K",
        "outputId": "1065c8b8-7d39-43da-ee98-56a82b44d5ec"
      },
      "source": [
        "cd files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ8Gi0QZFWej",
        "outputId": "e818a988-db5e-460f-a7cf-d2c6c30b0554"
      },
      "source": [
        "!unzip bertext_cnndm_transformer.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bertext_cnndm_transformer.zip\n",
            "replace bertext_cnndm_transformer.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB_v6Lh8RHs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6812f38-0ae3-4102-f8ae-037378c7945e"
      },
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 14.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 12.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 40kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp37-none-any.whl size=191613 sha256=5a58e3a1e6fe01106fff1a429865128310c0acc3ef6b1331d88a7c52c052805c\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n",
            "Collecting https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "\u001b[K     - 460kB 8.8MB/s\n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): pyrouge==0.1.3 from https://github.com/bheinzerling/pyrouge/archive/master.zip in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: pyrouge\n",
            "  Building wheel for pyrouge (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyrouge: filename=pyrouge-0.1.3-cp37-none-any.whl size=191913 sha256=9ea052f0ea1a88ce24bec725a97fbb01fb3572793fc69b0935fa320808e74a8d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-69frwe30/wheels/70/02/b4/a23b5feb5980a5eb940441cb04ec1e17d5f18344138efbecf8\n",
            "Successfully built pyrouge\n",
            "Requirement already satisfied: pyrouge in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Name: pyrouge\n",
            "Version: 0.1.3\n",
            "Summary: A Python wrapper for the ROUGE summarization evaluation package.\n",
            "Home-page: https://github.com/noutenki/pyrouge\n",
            "Author: Benjamin Heinzerling, Anders Johannsen\n",
            "Author-email: benjamin.heinzerling@h-its.org\n",
            "License: LICENSE.txt\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "fatal: destination path 'pyrouge' already exists and is not an empty directory.\n",
            "2021-05-03 06:43:35,307 [MainThread  ] [INFO ]  Set ROUGE home directory to pyrouge/tools/ROUGE-1.5.5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWpax6fMKQFF"
      },
      "source": [
        "!chmod 777 '/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1eBg0X0LBzZ"
      },
      "source": [
        "!chmod 777 'pyrouge/tools/ROUGE-1.5.5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyJTYO4JXn7C"
      },
      "source": [
        "!chmod 777 'pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UVuNBqSc7vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba7d124-d58a-49b4-ac86-d603c44546bb"
      },
      "source": [
        "!pyrouge_set_rouge_path '/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 06:43:36,442 [MainThread  ] [INFO ]  Set ROUGE home directory to /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCfbL52EaVxr",
        "outputId": "9b9d2fc2-08c3-48a8-b39c-8e21bc1f228d"
      },
      "source": [
        "!python3 -m pyrouge.test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 06:43:36,827 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpcqpoluex/rouge_conf.xml\n",
            "F2021-05-03 06:43:36,934 [MainThread  ] [INFO ]  Processing files in data/SL2003_models_plain_text.\n",
            "2021-05-03 06:43:36,934 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-06.html.\n",
            "2021-05-03 06:43:36,934 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-09.html.\n",
            "2021-05-03 06:43:36,935 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-13.html.\n",
            "2021-05-03 06:43:36,935 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-18.html.\n",
            "2021-05-03 06:43:36,935 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-21.html.\n",
            "2021-05-03 06:43:36,935 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-17.html.\n",
            "2021-05-03 06:43:36,936 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-01.html.\n",
            "2021-05-03 06:43:36,936 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-07.html.\n",
            "2021-05-03 06:43:36,936 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-08.html.\n",
            "2021-05-03 06:43:36,936 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-20.html.\n",
            "2021-05-03 06:43:36,936 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-15.html.\n",
            "2021-05-03 06:43:36,937 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-22.html.\n",
            "2021-05-03 06:43:36,937 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-12.html.\n",
            "2021-05-03 06:43:36,937 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-02.html.\n",
            "2021-05-03 06:43:36,937 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-14.html.\n",
            "2021-05-03 06:43:36,937 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-19.html.\n",
            "2021-05-03 06:43:36,938 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-16.html.\n",
            "2021-05-03 06:43:36,938 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-25.html.\n",
            "2021-05-03 06:43:36,938 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-23.html.\n",
            "2021-05-03 06:43:36,938 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-11.html.\n",
            "2021-05-03 06:43:36,939 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-03.html.\n",
            "2021-05-03 06:43:36,939 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-05.html.\n",
            "2021-05-03 06:43:36,939 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-24.html.\n",
            "2021-05-03 06:43:36,939 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-04.html.\n",
            "2021-05-03 06:43:36,939 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-10.html.\n",
            "2021-05-03 06:43:36,940 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp0chod6pm.\n",
            ".2021-05-03 06:43:36,957 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmp983owecc/rouge_conf.xml\n",
            "2021-05-03 06:43:36,957 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmp983owecc/rouge_conf.xml\n",
            "Can't locate XML/Parser.pm in @INC (you may need to install the XML::Parser module) (@INC contains: /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5 /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.26.1 /usr/local/share/perl/5.26.1 /usr/lib/x86_64-linux-gnu/perl5/5.26 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.26 /usr/share/perl/5.26 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base) at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/XML/DOM.pm line 41.\n",
            "BEGIN failed--compilation aborted at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/XML/DOM.pm line 70.\n",
            "Compilation failed in require at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl line 177.\n",
            "BEGIN failed--compilation aborted at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl line 177.\n",
            "E2021-05-03 06:43:38,460 [MainThread  ] [INFO ]  Processing files in data/SL2003_models_rouge_format.\n",
            "2021-05-03 06:43:38,461 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-06.html.\n",
            "/usr/local/bin/pyrouge_convert_rouge_format_to_plain_text:14: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 14 of the file /usr/local/bin/pyrouge_convert_rouge_format_to_plain_text. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(html)\n",
            "2021-05-03 06:43:38,464 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-09.html.\n",
            "2021-05-03 06:43:38,465 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-13.html.\n",
            "2021-05-03 06:43:38,466 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-18.html.\n",
            "2021-05-03 06:43:38,468 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-21.html.\n",
            "2021-05-03 06:43:38,469 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-17.html.\n",
            "2021-05-03 06:43:38,470 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-01.html.\n",
            "2021-05-03 06:43:38,471 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-07.html.\n",
            "2021-05-03 06:43:38,472 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-08.html.\n",
            "2021-05-03 06:43:38,473 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-20.html.\n",
            "2021-05-03 06:43:38,473 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-15.html.\n",
            "2021-05-03 06:43:38,474 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-22.html.\n",
            "2021-05-03 06:43:38,475 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-12.html.\n",
            "2021-05-03 06:43:38,475 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-02.html.\n",
            "2021-05-03 06:43:38,476 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-14.html.\n",
            "2021-05-03 06:43:38,477 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-19.html.\n",
            "2021-05-03 06:43:38,477 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-16.html.\n",
            "2021-05-03 06:43:38,478 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-25.html.\n",
            "2021-05-03 06:43:38,479 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-23.html.\n",
            "2021-05-03 06:43:38,479 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-11.html.\n",
            "2021-05-03 06:43:38,480 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-03.html.\n",
            "2021-05-03 06:43:38,481 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-05.html.\n",
            "2021-05-03 06:43:38,481 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-24.html.\n",
            "2021-05-03 06:43:38,482 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-04.html.\n",
            "2021-05-03 06:43:38,483 [MainThread  ] [INFO ]  Processing SL.P.10.R.A.SL062003-10.html.\n",
            "2021-05-03 06:43:38,483 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmperdpcij2.\n",
            ".E.2021-05-03 06:43:38,591 [MainThread  ] [INFO ]  Writing summaries.\n",
            "2021-05-03 06:43:38,592 [MainThread  ] [INFO ]  Processing summaries. Saving system files to /tmp/tmp24jqx_ww/system and model files to /tmp/tmp24jqx_ww/model.\n",
            "2021-05-03 06:43:38,592 [MainThread  ] [INFO ]  Processing files in data/systems_plain.\n",
            "2021-05-03 06:43:38,592 [MainThread  ] [INFO ]  Processing D30001.M.100.T.A.\n",
            "2021-05-03 06:43:38,592 [MainThread  ] [INFO ]  Processing D30003.M.100.T.A.\n",
            "2021-05-03 06:43:38,593 [MainThread  ] [INFO ]  Processing D30002.M.100.T.A.\n",
            "2021-05-03 06:43:38,593 [MainThread  ] [INFO ]  Processing D30005.M.100.T.A.\n",
            "2021-05-03 06:43:38,593 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp24jqx_ww/system.\n",
            "2021-05-03 06:43:38,593 [MainThread  ] [INFO ]  Processing files in data/models_plain.\n",
            "2021-05-03 06:43:38,593 [MainThread  ] [INFO ]  Processing D30005.M.100.T.G.\n",
            "2021-05-03 06:43:38,594 [MainThread  ] [INFO ]  Processing D30003.M.100.T.B.\n",
            "2021-05-03 06:43:38,594 [MainThread  ] [INFO ]  Processing D30001.M.100.T.C.\n",
            "2021-05-03 06:43:38,594 [MainThread  ] [INFO ]  Processing D30003.M.100.T.F.\n",
            "2021-05-03 06:43:38,594 [MainThread  ] [INFO ]  Processing D30003.M.100.T.C.\n",
            "2021-05-03 06:43:38,595 [MainThread  ] [INFO ]  Processing D30002.M.100.T.E.\n",
            "2021-05-03 06:43:38,595 [MainThread  ] [INFO ]  Processing D30001.M.100.T.D.\n",
            "2021-05-03 06:43:38,595 [MainThread  ] [INFO ]  Processing D30002.M.100.T.B.\n",
            "2021-05-03 06:43:38,595 [MainThread  ] [INFO ]  Processing D30005.M.100.T.B.\n",
            "2021-05-03 06:43:38,596 [MainThread  ] [INFO ]  Processing D30005.M.100.T.C.\n",
            "2021-05-03 06:43:38,596 [MainThread  ] [INFO ]  Processing D30001.M.100.T.B.\n",
            "2021-05-03 06:43:38,596 [MainThread  ] [INFO ]  Processing D30002.M.100.T.C.\n",
            "2021-05-03 06:43:38,596 [MainThread  ] [INFO ]  Saved processed files to /tmp/tmp24jqx_ww/model.\n",
            "2021-05-03 06:43:38,598 [MainThread  ] [INFO ]  Written ROUGE configuration to /tmp/tmpp8wu6pjt/rouge_conf.xml\n",
            "2021-05-03 06:43:38,598 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -2 -1 -U -r 1000 -n 4 -w 1.2 -a -m /tmp/tmpp8wu6pjt/rouge_conf.xml\n",
            "Can't locate XML/Parser.pm in @INC (you may need to install the XML::Parser module) (@INC contains: /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5 /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.26.1 /usr/local/share/perl/5.26.1 /usr/lib/x86_64-linux-gnu/perl5/5.26 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.26 /usr/share/perl/5.26 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base) at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/XML/DOM.pm line 41.\n",
            "BEGIN failed--compilation aborted at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/XML/DOM.pm line 70.\n",
            "Compilation failed in require at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl line 177.\n",
            "BEGIN failed--compilation aborted at /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl line 177.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pyrouge_evaluate_plain_text_files\", line 25, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/bin/pyrouge_evaluate_plain_text_files\", line 21, in main\n",
            "    output = rouge.convert_and_evaluate(args.system_id, args.split_sents)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/Rouge155.py\", line 361, in convert_and_evaluate\n",
            "    rouge_output = self.evaluate(system_id, rouge_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/Rouge155.py\", line 336, in evaluate\n",
            "    rouge_output = check_output(command).decode(\"UTF-8\")\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl', '-e', '/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data', '-c', '95', '-2', '-1', '-U', '-r', '1000', '-n', '4', '-w', '1.2', '-a', '-m', '/tmp/tmpp8wu6pjt/rouge_conf.xml']' returned non-zero exit status 2.\n",
            "E.E..\n",
            "======================================================================\n",
            "ERROR: test_evaluation (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 156, in test_evaluation\n",
            "    pyrouge_output = rouge.evaluate(system_id=11).strip()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/Rouge155.py\", line 336, in evaluate\n",
            "    rouge_output = check_output(command).decode(\"UTF-8\")\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl', '-e', '/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data', '-c', '95', '-2', '-1', '-U', '-r', '1000', '-n', '4', '-w', '1.2', '-a', '-m', '/tmp/tmp983owecc/rouge_conf.xml']' returned non-zero exit status 2.\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_options (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 218, in test_options\n",
            "    pyrouge_output = check_output_clean(pyrouge_command)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 17, in <lambda>\n",
            "    check_output_clean = lambda c: check_output(c).decode(\"UTF-8\").strip()\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 488, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 800, in __init__\n",
            "    restore_signals, start_new_session)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1551, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyrouge_evaluate_plain_text_files.py': 'pyrouge_evaluate_plain_text_files.py'\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_rouge_for_plain_text (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 173, in test_rouge_for_plain_text\n",
            "    pyrouge_output = check_output_clean(pyrouge_command.split())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 17, in <lambda>\n",
            "    check_output_clean = lambda c: check_output(c).decode(\"UTF-8\").strip()\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 512, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['pyrouge_evaluate_plain_text_files', '-m', 'data/models_plain', '-s', 'data/systems_plain', '-sfp', 'D(\\\\d+).M.100.T.A', '-mfp', 'D#ID#.M.100.T.[A-Z]', '-id', '1']' returned non-zero exit status 1.\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_write_config (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 197, in test_write_config\n",
            "    check_output(command.split())\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 411, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 488, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 800, in __init__\n",
            "    restore_signals, start_new_session)\n",
            "  File \"/usr/lib/python3.7/subprocess.py\", line 1551, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'pyrouge_write_config_file.py': 'pyrouge_write_config_file.py'\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_config_file (pyrouge.tests.Rouge155_test.PyrougeTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge/tests/Rouge155_test.py\", line 147, in test_config_file\n",
            "    add_data_path(\"ROUGE-test_11.xml\")))\n",
            "AssertionError: False is not true\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 11 tests in 1.838s\n",
            "\n",
            "FAILED (failures=1, errors=4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ-h1h45Lgj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494bb242-9596-4ac8-a7d2-20ea5f4e6826"
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 7.7MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/8f/42959300c543b4d34bc9f9b54954471a33384c181084ed84f070763d7f37/boto3-1.17.62-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.62\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/60/ba830f93176fdc23166043298173ee2aecd5cf150f1ede51d6506f021deb/botocore-1.20.62-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 34.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.62->boto3->pytorch-transformers) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.62 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed boto3-1.17.62 botocore-1.20.62 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVmZnEYgQnyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c12a1e-e887-4705-e186-bf30df6bcfdc"
      },
      "source": [
        "!sudo apt-get install libxml-parser-perl\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl netbase perl-openssl-defaults\n",
            "Suggested packages:\n",
            "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
            "The following NEW packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl libxml-parser-perl netbase perl-openssl-defaults\n",
            "0 upgraded, 31 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 1,713 kB of archives.\n",
            "After this operation, 5,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 netbase all 5.4 [12.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-listing-perl all 6.04-1 [9,774 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-form-perl all 6.03-1 [23.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tree-perl all 5.07-1 [200 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-cookies-perl all 6.04-1 [17.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-daemon-perl all 6.01-1 [17.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-negotiate-perl all 6.00-2 [13.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 perl-openssl-defaults amd64 3build1 [7,012 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnet-ssleay-perl amd64 1.84-1ubuntu0.2 [283 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libio-socket-ssl-perl all 2.060-3~ubuntu18.04.1 [173 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-http-perl all 6.17-1 [22.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-robotrules-perl all 6.01-1 [14.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libwww-perl all 6.31-1ubuntu0.1 [137 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-protocol-https-perl all 6.07-2 [8,284 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5,948 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmailtools-perl all 2.18-1 [74.0 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxml-parser-perl amd64 2.44-2build3 [199 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
            "Fetched 1,713 kB in 2s (1,041 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 31.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../00-netbase_5.4_all.deb ...\n",
            "Unpacking netbase (5.4) ...\n",
            "Selecting previously unselected package libdata-dump-perl.\n",
            "Preparing to unpack .../01-libdata-dump-perl_1.23-1_all.deb ...\n",
            "Unpacking libdata-dump-perl (1.23-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../02-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../03-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../04-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libfile-listing-perl.\n",
            "Preparing to unpack .../05-libfile-listing-perl_6.04-1_all.deb ...\n",
            "Unpacking libfile-listing-perl (6.04-1) ...\n",
            "Selecting previously unselected package libfont-afm-perl.\n",
            "Preparing to unpack .../06-libfont-afm-perl_1.20-2_all.deb ...\n",
            "Unpacking libfont-afm-perl (1.20-2) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../07-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../08-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../09-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../10-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../11-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../12-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libhtml-form-perl.\n",
            "Preparing to unpack .../13-libhtml-form-perl_6.03-1_all.deb ...\n",
            "Unpacking libhtml-form-perl (6.03-1) ...\n",
            "Selecting previously unselected package libhtml-tree-perl.\n",
            "Preparing to unpack .../14-libhtml-tree-perl_5.07-1_all.deb ...\n",
            "Unpacking libhtml-tree-perl (5.07-1) ...\n",
            "Selecting previously unselected package libhtml-format-perl.\n",
            "Preparing to unpack .../15-libhtml-format-perl_2.12-1_all.deb ...\n",
            "Unpacking libhtml-format-perl (2.12-1) ...\n",
            "Selecting previously unselected package libhttp-cookies-perl.\n",
            "Preparing to unpack .../16-libhttp-cookies-perl_6.04-1_all.deb ...\n",
            "Unpacking libhttp-cookies-perl (6.04-1) ...\n",
            "Selecting previously unselected package libhttp-daemon-perl.\n",
            "Preparing to unpack .../17-libhttp-daemon-perl_6.01-1_all.deb ...\n",
            "Unpacking libhttp-daemon-perl (6.01-1) ...\n",
            "Selecting previously unselected package libhttp-negotiate-perl.\n",
            "Preparing to unpack .../18-libhttp-negotiate-perl_6.00-2_all.deb ...\n",
            "Unpacking libhttp-negotiate-perl (6.00-2) ...\n",
            "Selecting previously unselected package perl-openssl-defaults:amd64.\n",
            "Preparing to unpack .../19-perl-openssl-defaults_3build1_amd64.deb ...\n",
            "Unpacking perl-openssl-defaults:amd64 (3build1) ...\n",
            "Selecting previously unselected package libnet-ssleay-perl.\n",
            "Preparing to unpack .../20-libnet-ssleay-perl_1.84-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libio-socket-ssl-perl.\n",
            "Preparing to unpack .../21-libio-socket-ssl-perl_2.060-3~ubuntu18.04.1_all.deb ...\n",
            "Unpacking libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libnet-http-perl.\n",
            "Preparing to unpack .../22-libnet-http-perl_6.17-1_all.deb ...\n",
            "Unpacking libnet-http-perl (6.17-1) ...\n",
            "Selecting previously unselected package libtry-tiny-perl.\n",
            "Preparing to unpack .../23-libtry-tiny-perl_0.30-1_all.deb ...\n",
            "Unpacking libtry-tiny-perl (0.30-1) ...\n",
            "Selecting previously unselected package libwww-robotrules-perl.\n",
            "Preparing to unpack .../24-libwww-robotrules-perl_6.01-1_all.deb ...\n",
            "Unpacking libwww-robotrules-perl (6.01-1) ...\n",
            "Selecting previously unselected package libwww-perl.\n",
            "Preparing to unpack .../25-libwww-perl_6.31-1ubuntu0.1_all.deb ...\n",
            "Unpacking libwww-perl (6.31-1ubuntu0.1) ...\n",
            "Selecting previously unselected package liblwp-protocol-https-perl.\n",
            "Preparing to unpack .../26-liblwp-protocol-https-perl_6.07-2_all.deb ...\n",
            "Unpacking liblwp-protocol-https-perl (6.07-2) ...\n",
            "Selecting previously unselected package libnet-smtp-ssl-perl.\n",
            "Preparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
            "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Selecting previously unselected package libmailtools-perl.\n",
            "Preparing to unpack .../28-libmailtools-perl_2.18-1_all.deb ...\n",
            "Unpacking libmailtools-perl (2.18-1) ...\n",
            "Selecting previously unselected package libxml-parser-perl.\n",
            "Preparing to unpack .../29-libxml-parser-perl_2.44-2build3_amd64.deb ...\n",
            "Unpacking libxml-parser-perl (2.44-2build3) ...\n",
            "Selecting previously unselected package libauthen-sasl-perl.\n",
            "Preparing to unpack .../30-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
            "Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libtry-tiny-perl (0.30-1) ...\n",
            "Setting up libfont-afm-perl (1.20-2) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up perl-openssl-defaults:amd64 (3build1) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libdata-dump-perl (1.23-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libnet-http-perl (6.17-1) ...\n",
            "Setting up libwww-robotrules-perl (6.01-1) ...\n",
            "Setting up libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up netbase (5.4) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
            "Setting up libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
            "Setting up libhtml-tree-perl (5.07-1) ...\n",
            "Setting up libfile-listing-perl (6.04-1) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libhttp-negotiate-perl (6.00-2) ...\n",
            "Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Setting up libhtml-format-perl (2.12-1) ...\n",
            "Setting up libhttp-cookies-perl (6.04-1) ...\n",
            "Setting up libhttp-daemon-perl (6.01-1) ...\n",
            "Setting up libhtml-form-perl (6.03-1) ...\n",
            "Setting up libmailtools-perl (2.18-1) ...\n",
            "Setting up liblwp-protocol-https-perl (6.07-2) ...\n",
            "Setting up libwww-perl (6.31-1ubuntu0.1) ...\n",
            "Setting up libxml-parser-perl (2.44-2build3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.0.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8YN8N3BLVuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f11883-d6d7-4463-8152-2c33ff8974c8"
      },
      "source": [
        "cd pyrouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7QOzsTKLbGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf583e2-377d-4806-839a-2415401e3b1b"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mbuild\u001b[0m/  \u001b[01;34mdist\u001b[0m/    \u001b[01;34mpyrouge\u001b[0m/           README.md  \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/   LICENSE  \u001b[01;34mpyrouge.egg-info\u001b[0m/  setup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGLhuq24LaZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cba5362-a8a4-49d6-a99b-8b655474072d"
      },
      "source": [
        "!sudo python setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing pyrouge.egg-info/PKG-INFO\n",
            "writing dependency_links to pyrouge.egg-info/dependency_links.txt\n",
            "writing requirements to pyrouge.egg-info/requires.txt\n",
            "writing top-level names to pyrouge.egg-info/top_level.txt\n",
            "adding license file 'LICENSE' (matched pattern 'LICEN[CS]E*')\n",
            "writing manifest file 'pyrouge.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pyrouge\n",
            "copying build/lib/pyrouge/__init__.py -> build/bdist.linux-x86_64/egg/pyrouge\n",
            "copying build/lib/pyrouge/base.py -> build/bdist.linux-x86_64/egg/pyrouge\n",
            "copying build/lib/pyrouge/rouge.py -> build/bdist.linux-x86_64/egg/pyrouge\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pyrouge/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pyrouge/base.py to base.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pyrouge/rouge.py to rouge.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pyrouge.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pyrouge.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pyrouge.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pyrouge.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pyrouge.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pyrouge.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "creating 'dist/pyrouge-0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pyrouge-0.1-py3.7.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/pyrouge-0.1-py3.7.egg\n",
            "Extracting pyrouge-0.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding pyrouge 0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/pyrouge-0.1-py3.7.egg\n",
            "Processing dependencies for pyrouge==0.1\n",
            "Searching for pandas==1.1.5\n",
            "Best match: pandas 1.1.5\n",
            "Adding pandas 1.1.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for beautifulsoup4==4.6.3\n",
            "Best match: beautifulsoup4 4.6.3\n",
            "Adding beautifulsoup4 4.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for pyrouge==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR5q5BLjLqyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba80df8-a5ce-42ab-f61f-5926a8170fa3"
      },
      "source": [
        "cd tools/ROUGE-1.5.5/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edGMEAVzLxbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39f884b-f0b6-421e-bc18-1210409d8382"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  README.txt  RELEASE-NOTE.txt  \u001b[01;32mROUGE-1.5.5.pl\u001b[0m*  runROUGE-test.pl  \u001b[01;34mXML\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iqeome7L7KA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce28c28-3a73-4979-c6d5-c064946f47a3"
      },
      "source": [
        "cd data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95fr8koOMEk3"
      },
      "source": [
        "!ln -sf WordNet-2.0-Exceptions/WordNet-2.0.exc.db WordNet-2.0.exc.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2be3kW9sMW49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f899e20-36dc-45be-95be-79cb068a016c"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "smart_common_words.txt   \u001b[0m\u001b[01;36mWordNet-2.0.exc.db\u001b[0m@\n",
            "\u001b[01;34mWordNet-1.6-Exceptions\u001b[0m/  \u001b[01;34mWordNet-2.0-Exceptions\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPYNYNJIMeuy"
      },
      "source": [
        "!rm WordNet-2.0.exc.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT2A4lYcNI4L"
      },
      "source": [
        "!perl ./WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4O9Z6dCNR2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ab08ef-2163-4dd1-8ac5-f7a2ba632199"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfnPudjgbVOn",
        "outputId": "429274cb-31a6-4d18-a0ee-7b6f04e47ba6"
      },
      "source": [
        "!python train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/bert_data_cnndm_final/cnndm -log_file ../logs/test_ext_bert_cnndm.log -model_path ../data/trained_models/ -sep_optim true -use_interval true -visible_gpus 0,1,2 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/ext_bert_cnndm -test_from ../data/trained_models/bertext_cnndm_transformer.pt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'train.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O4Uhdl-oakj"
      },
      "source": [
        "## CNN Abs Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt_oAhc8onSQ",
        "outputId": "7d0bb7a1-0760-41aa-a161-307fb2e5eb3f"
      },
      "source": [
        "cd trained_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/data/trained_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvrnqCZuocv4",
        "outputId": "33a24a19-a62e-42b4-c8d9-6e9a3e2e8014"
      },
      "source": [
        "!unzip bertsumextabs_cnndm_final_model.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bertsumextabs_cnndm_final_model.zip\n",
            "  inflating: model_step_148000.pt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cARe5eTOpz5G",
        "outputId": "137bdf9b-1281-49ef-83c7-e54e0fcebf44"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cal_rouge.py                        presumm_evaluation.ipynb\n",
            "'Copy of presumm_evaluation.ipynb'   \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            " distributed.py                      \u001b[01;34mpyrouge\u001b[0m/\n",
            " \u001b[01;34mmodels\u001b[0m/                             script_PreSumm.ipynb\n",
            " \u001b[01;34mothers\u001b[0m/                             train_abstractive.py\n",
            " post_stats.py                       train_extractive.py\n",
            " \u001b[01;34mprepro\u001b[0m/                             train.py\n",
            " preprocess.py                       \u001b[01;34mtranslate\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlCqhp8_okdW",
        "outputId": "6a8ce8cc-18f2-4d64-f483-217631400b07"
      },
      "source": [
        "!python train.py -task abs -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/bert_data_cnndm_final/cnndm -log_file ../logs/test_abs_bert_cnndm.log -model_path ../data/trained_models/ -sep_optim true -use_interval true -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/abs_bert_cnndm -test_from ../data/trained_models/model_step_148000.pt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "[2021-04-05 07:07:15,754 INFO] Loading checkpoint from ../data/trained_models/model_step_148000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/bert_data_cnndm_final/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_abs_bert_cnndm.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/abs_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='abs', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/model_step_148000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-04-05 07:07:18,685 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "[2021-04-05 07:07:18,686 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2021-04-05 07:07:18,724 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "[2021-04-05 07:07:27,030 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.0.bert.pt, number of examples: 2001\n",
            "[2021-04-05 07:07:27,073 INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at ../temp/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 135, in <module>\n",
            "    test_abs(args, device_id, cp, step)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_abstractive.py\", line 225, in test_abs\n",
            "    predictor.translate(test_iter, step)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/predictor.py\", line 149, in translate\n",
            "    batch_data = self.translate_batch(batch)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/predictor.py\", line 218, in translate_batch\n",
            "    min_length=self.min_length)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/predictor.py\", line 331, in _fast_translate_batch\n",
            "    [alive_seq.index_select(0, select_indices),\n",
            "RuntimeError: \"index_select_out_cuda_impl\" not implemented for 'Float'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCfwKiRUzX4L",
        "outputId": "c8d8b044-521e-4867-f162-ce39b77af5bc"
      },
      "source": [
        "!python train.py -task abs -mode validate -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/bert_data_cnndm_final/cnndm -log_file ../logs/val_abs_bert_cnndm -model_path ../data/models/model_step_148000.pt -sep_optim true -use_interval true -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/abs_bert_cnndm "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 124, in <module>\n",
            "    validate_abs(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_abstractive.py\", line 165, in validate_abs\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6p7g3YeTYeR"
      },
      "source": [
        "## Test on my trained ext model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JURsDVC6UdmT",
        "outputId": "0b0f010a-2374-46f0-ec47-3e24485c73c6"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cal_rouge.py                        presumm_evaluation.ipynb\n",
            "'Copy of presumm_evaluation.ipynb'   \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
            " distributed.py                      \u001b[01;34mpyrouge\u001b[0m/\n",
            " \u001b[01;34mmodels\u001b[0m/                             script_PreSumm.ipynb\n",
            " \u001b[01;34mothers\u001b[0m/                             train_abstractive.py\n",
            " post_stats.py                       train_extractive.py\n",
            " \u001b[01;34mprepro\u001b[0m/                             train.py\n",
            " preprocess.py                       \u001b[01;34mtranslate\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrg6TKTPTiPX",
        "outputId": "dcaa4e13-eb6b-418a-df1e-15bfaaea8f5e"
      },
      "source": [
        "!python train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/bert_data_cnndm_final/cnndm -log_file ../logs/test_my_ext_bert_cnndm.log -model_path ../data/trained_models/ -sep_optim true -use_interval true -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_ext_bert_cnndm -test_from ../data/trained_models/model_step_50000.pt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-04-07 03:14:25,135 INFO] Loading checkpoint from ../data/trained_models/model_step_50000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/bert_data_cnndm_final/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='bert', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_ext_bert_cnndm.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/model_step_50000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-04-07 03:14:37,903 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "[2021-04-07 03:14:38,404 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2021-04-07 03:14:38,698 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "[2021-04-07 03:15:04,360 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-04-07 03:15:04,368 INFO] * number of parameters: 120512513\n",
            "[2021-04-07 03:16:23,823 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.1.bert.pt, number of examples: 2001\n",
            "[2021-04-07 03:17:47,639 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.2.bert.pt, number of examples: 2001\n",
            "[2021-04-07 03:19:11,869 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.3.bert.pt, number of examples: 2001\n",
            "[2021-04-07 03:20:35,922 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.4.bert.pt, number of examples: 2000\n",
            "[2021-04-07 03:22:00,378 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.5.bert.pt, number of examples: 1485\n",
            "11489\n",
            "11489\n",
            "2021-04-07 03:26:24,131 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-04-07 03:26:24,131 INFO] Writing summaries.\n",
            "2021-04-07 03:26:24,136 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmpxasrixd1/system and model files to ../temp/tmpxasrixd1/model.\n",
            "[2021-04-07 03:26:24,136 INFO] Processing summaries. Saving system files to ../temp/tmpxasrixd1/system and model files to ../temp/tmpxasrixd1/model.\n",
            "2021-04-07 03:26:24,136 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-04-07-03-23-02/candidate/.\n",
            "[2021-04-07 03:26:24,136 INFO] Processing files in ../temp/rouge-tmp-2021-04-07-03-23-02/candidate/.\n",
            "2021-04-07 03:29:29,868 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpxasrixd1/system.\n",
            "[2021-04-07 03:29:29,868 INFO] Saved processed files to ../temp/tmpxasrixd1/system.\n",
            "2021-04-07 03:29:29,869 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-04-07-03-23-02/reference/.\n",
            "[2021-04-07 03:29:29,869 INFO] Processing files in ../temp/rouge-tmp-2021-04-07-03-23-02/reference/.\n",
            "2021-04-07 03:32:39,143 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpxasrixd1/model.\n",
            "[2021-04-07 03:32:39,143 INFO] Saved processed files to ../temp/tmpxasrixd1/model.\n",
            "2021-04-07 03:32:39,347 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmpmkhb_4he/rouge_conf.xml\n",
            "[2021-04-07 03:32:39,347 INFO] Written ROUGE configuration to ../temp/tmpmkhb_4he/rouge_conf.xml\n",
            "2021-04-07 03:32:39,348 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpmkhb_4he/rouge_conf.xml\n",
            "[2021-04-07 03:32:39,348 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpmkhb_4he/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.51367 (95%-conf.int. 0.51102 - 0.51627)\n",
            "1 ROUGE-1 Average_P: 0.37345 (95%-conf.int. 0.37102 - 0.37604)\n",
            "1 ROUGE-1 Average_F: 0.41754 (95%-conf.int. 0.41527 - 0.41979)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.23346 (95%-conf.int. 0.23059 - 0.23617)\n",
            "1 ROUGE-2 Average_P: 0.17089 (95%-conf.int. 0.16864 - 0.17319)\n",
            "1 ROUGE-2 Average_F: 0.19022 (95%-conf.int. 0.18786 - 0.19249)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.46956 (95%-conf.int. 0.46691 - 0.47219)\n",
            "1 ROUGE-L Average_P: 0.34212 (95%-conf.int. 0.33976 - 0.34464)\n",
            "1 ROUGE-L Average_F: 0.38216 (95%-conf.int. 0.37995 - 0.38439)\n",
            "\n",
            "[2021-04-07 03:37:01,535 INFO] Rouges at step 50000 \n",
            ">> ROUGE-F(1/2/3/l): 41.75/19.02/38.22\n",
            "ROUGE-R(1/2/3/l): 51.37/23.35/46.96\n",
            "\n",
            "[2021-04-07 03:37:01,536 INFO] Validation xent: 5.82127 at step 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQvOaraPabqe"
      },
      "source": [
        "## Test on my baseline model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsOyOTjoagDI",
        "outputId": "514b9969-f4a2-45ed-b928-1e709007f948"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne3U9dt-adwe",
        "outputId": "b1e454a0-fbe8-4359-c7b2-ae2f7e6eb30d"
      },
      "source": [
        "!python train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/bert_data_cnndm_final/cnndm -log_file ../logs/test_my_baseline_ext_bert_cnndm.log -model_path ../data/trained_models/Daniella -sep_optim true -use_interval true -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_baseline_ext_bert_cnndm -test_from ../data/trained_models/Daniella/baseline_model_step_50000.pt "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-04-12 04:50:38,847 INFO] Loading checkpoint from ../data/trained_models/Daniella/baseline_model_step_50000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/bert_data_cnndm_final/cnndm', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='baseline', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=2, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_baseline_ext_bert_cnndm.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/Daniella', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_baseline_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/Daniella/baseline_model_step_50000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-04-12 04:50:46,831 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at ../temp/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "[2021-04-12 04:50:47,003 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2021-04-12 04:50:47,055 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at ../temp/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "[2021-04-12 04:51:13,749 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-04-12 04:51:13,755 INFO] * number of parameters: 35456513\n",
            "[2021-04-12 04:51:25,523 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.1.bert.pt, number of examples: 2001\n",
            "[2021-04-12 04:51:37,209 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.2.bert.pt, number of examples: 2001\n",
            "[2021-04-12 04:51:49,284 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.3.bert.pt, number of examples: 2001\n",
            "[2021-04-12 04:52:01,449 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.4.bert.pt, number of examples: 2000\n",
            "[2021-04-12 04:52:14,527 INFO] Loading test dataset from ../data/preprocessed_data/bert_data_cnndm_final/cnndm.test.5.bert.pt, number of examples: 1485\n",
            "11489\n",
            "11489\n",
            "2021-04-12 04:55:45,484 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-04-12 04:55:45,484 INFO] Writing summaries.\n",
            "2021-04-12 04:55:45,490 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmpnpr61nx2/system and model files to ../temp/tmpnpr61nx2/model.\n",
            "[2021-04-12 04:55:45,490 INFO] Processing summaries. Saving system files to ../temp/tmpnpr61nx2/system and model files to ../temp/tmpnpr61nx2/model.\n",
            "2021-04-12 04:55:45,490 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-04-12-04-52-23/candidate/.\n",
            "[2021-04-12 04:55:45,490 INFO] Processing files in ../temp/rouge-tmp-2021-04-12-04-52-23/candidate/.\n",
            "2021-04-12 04:58:51,683 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpnpr61nx2/system.\n",
            "[2021-04-12 04:58:51,683 INFO] Saved processed files to ../temp/tmpnpr61nx2/system.\n",
            "2021-04-12 04:58:51,684 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-04-12-04-52-23/reference/.\n",
            "[2021-04-12 04:58:51,684 INFO] Processing files in ../temp/rouge-tmp-2021-04-12-04-52-23/reference/.\n",
            "2021-04-12 05:01:58,976 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpnpr61nx2/model.\n",
            "[2021-04-12 05:01:58,976 INFO] Saved processed files to ../temp/tmpnpr61nx2/model.\n",
            "2021-04-12 05:01:59,215 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmpe1s1gge2/rouge_conf.xml\n",
            "[2021-04-12 05:01:59,215 INFO] Written ROUGE configuration to ../temp/tmpe1s1gge2/rouge_conf.xml\n",
            "2021-04-12 05:01:59,215 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpe1s1gge2/rouge_conf.xml\n",
            "[2021-04-12 05:01:59,215 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpe1s1gge2/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.52861 (95%-conf.int. 0.52571 - 0.53164)\n",
            "1 ROUGE-1 Average_P: 0.34843 (95%-conf.int. 0.34605 - 0.35075)\n",
            "1 ROUGE-1 Average_F: 0.40596 (95%-conf.int. 0.40376 - 0.40817)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.23481 (95%-conf.int. 0.23206 - 0.23767)\n",
            "1 ROUGE-2 Average_P: 0.15480 (95%-conf.int. 0.15277 - 0.15691)\n",
            "1 ROUGE-2 Average_F: 0.18010 (95%-conf.int. 0.17795 - 0.18235)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.48029 (95%-conf.int. 0.47749 - 0.48312)\n",
            "1 ROUGE-L Average_P: 0.31709 (95%-conf.int. 0.31492 - 0.31941)\n",
            "1 ROUGE-L Average_F: 0.36921 (95%-conf.int. 0.36702 - 0.37143)\n",
            "\n",
            "[2021-04-12 05:06:23,938 INFO] Rouges at step 50000 \n",
            ">> ROUGE-F(1/2/3/l): 40.60/18.01/36.92\n",
            "ROUGE-R(1/2/3/l): 52.86/23.48/48.03\n",
            "\n",
            "[2021-04-12 05:06:23,939 INFO] Validation xent: 5.96931 at step 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDEEwwBgS1WA",
        "outputId": "619e0976-290f-4a20-b576-046cc603dbae"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " base.py                             \u001b[0m\u001b[01;34mprepro\u001b[0m/\n",
            " \u001b[01;34mbert_data\u001b[0m/                          preprocess.py\n",
            " bert_data_cnndm_final.zip           presumm_evaluation.ipynb\n",
            " bertext_cnndm_transformer.pt        \u001b[01;34m__pycache__\u001b[0m/\n",
            " bertext_cnndm_transformer.zip       \u001b[01;34mpyrouge\u001b[0m/\n",
            " \u001b[01;34mBertSum\u001b[0m/                            script_PreSumm.ipynb\n",
            " cal_rouge.py                        train_abstractive.py\n",
            "'Copy of presumm_evaluation.ipynb'   train_extractive.py\n",
            " distributed.py                      train.py\n",
            " \u001b[01;34mmodels\u001b[0m/                             train_xsum.ipynb\n",
            " \u001b[01;34mothers\u001b[0m/                             \u001b[01;34mtranslate\u001b[0m/\n",
            " post_stats.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnYrxXdVpVeW"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9uY9iyksXQB",
        "outputId": "a58ff705-9e58-450b-b020-88aec9aeccce"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6taYpIcTHFd",
        "outputId": "d36e0b96-dc65-4ba9-f523-b299f3db2fae"
      },
      "source": [
        "!python3 train.py -task ext -mode train -bert_data_path ../data/preprocessed_data/bert_data_cnndm_final/cnndm -ext_dropout 0.1 -model_path ../models -lr 2e-3 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_bert_cnndm -use_interval true -warmup_steps 10000 -max_pos 800"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 9, in <module>\n",
            "    from others.logging import init_logger\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7f7hbh6tbxR"
      },
      "source": [
        "## Train Roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K41dDavtdVG",
        "outputId": "3533794c-c5e2-42a5-a308-a2971e99a81a"
      },
      "source": [
        "!python3 train.py -task ext -encoder roberta -mode train -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -ext_dropout 0.1 -model_path ../data/trained_models/Daniella -lr 2e-3 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_roberta_cnndm -use_interval true -warmup_steps 10000 -max_pos 700"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "----TRAIN EXTRACTIVE----\n",
            "[2021-04-30 02:12:41,428 INFO] Device ID 0\n",
            "[2021-04-30 02:12:42,208 INFO] Device cuda\n",
            "[2021-04-30 02:12:42,584 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-04-30 02:12:42,914 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-04-30 02:12:43,281 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-04-30 02:13:02,907 INFO] ExtSummarizer(\n",
            "  (bert): Roberta(\n",
            "    (model): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(702, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ext_layer): ExtTransformerEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-04-30 02:13:02,924 INFO] * number of parameters: 135821057\n",
            "[2021-04-30 02:13:02,924 INFO] Start training...\n",
            "[2021-04-30 02:13:06,357 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.123.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:13:27,764 INFO] Step 50/50000; xent: 4.64; lr: 0.0000001;  39 docs/s;     21 sec\n",
            "[2021-04-30 02:13:48,938 INFO] Step 100/50000; xent: 4.26; lr: 0.0000002;  43 docs/s;     43 sec\n",
            "[2021-04-30 02:14:10,131 INFO] Step 150/50000; xent: 4.06; lr: 0.0000003;  41 docs/s;     64 sec\n",
            "[2021-04-30 02:14:31,346 INFO] Step 200/50000; xent: 4.02; lr: 0.0000004;  41 docs/s;     85 sec\n",
            "[2021-04-30 02:14:45,962 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.91.bert.pt, number of examples: 1995\n",
            "[2021-04-30 02:14:54,549 INFO] Step 250/50000; xent: 3.94; lr: 0.0000005;  38 docs/s;    108 sec\n",
            "[2021-04-30 02:15:15,820 INFO] Step 300/50000; xent: 3.88; lr: 0.0000006;  39 docs/s;    129 sec\n",
            "[2021-04-30 02:15:37,021 INFO] Step 350/50000; xent: 3.77; lr: 0.0000007;  42 docs/s;    151 sec\n",
            "[2021-04-30 02:15:58,296 INFO] Step 400/50000; xent: 3.72; lr: 0.0000008;  40 docs/s;    172 sec\n",
            "[2021-04-30 02:16:19,602 INFO] Step 450/50000; xent: 3.67; lr: 0.0000009;  40 docs/s;    193 sec\n",
            "[2021-04-30 02:16:26,171 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.39.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:16:42,453 INFO] Step 500/50000; xent: 3.51; lr: 0.0000010;  37 docs/s;    216 sec\n",
            "[2021-04-30 02:17:03,763 INFO] Step 550/50000; xent: 3.64; lr: 0.0000011;  40 docs/s;    237 sec\n",
            "[2021-04-30 02:17:24,953 INFO] Step 600/50000; xent: 3.47; lr: 0.0000012;  43 docs/s;    259 sec\n",
            "[2021-04-30 02:17:46,213 INFO] Step 650/50000; xent: 3.50; lr: 0.0000013;  41 docs/s;    280 sec\n",
            "[2021-04-30 02:18:06,894 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.6.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:18:09,579 INFO] Step 700/50000; xent: 3.60; lr: 0.0000014;  37 docs/s;    303 sec\n",
            "[2021-04-30 02:18:30,863 INFO] Step 750/50000; xent: 3.60; lr: 0.0000015;  40 docs/s;    325 sec\n",
            "[2021-04-30 02:18:52,134 INFO] Step 800/50000; xent: 3.69; lr: 0.0000016;  40 docs/s;    346 sec\n",
            "[2021-04-30 02:19:13,453 INFO] Step 850/50000; xent: 3.59; lr: 0.0000017;  41 docs/s;    367 sec\n",
            "[2021-04-30 02:19:34,685 INFO] Step 900/50000; xent: 3.56; lr: 0.0000018;  41 docs/s;    388 sec\n",
            "[2021-04-30 02:19:47,769 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.81.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:19:58,177 INFO] Step 950/50000; xent: 3.44; lr: 0.0000019;  36 docs/s;    412 sec\n",
            "[2021-04-30 02:20:19,488 INFO] Step 1000/50000; xent: 3.52; lr: 0.0000020;  41 docs/s;    433 sec\n",
            "[2021-04-30 02:20:19,492 INFO] Saving checkpoint ../data/trained_models/Daniella/model_step_1000.pt\n",
            "[2021-04-30 02:20:47,004 INFO] Step 1050/50000; xent: 3.44; lr: 0.0000021;  32 docs/s;    461 sec\n",
            "[2021-04-30 02:21:08,484 INFO] Step 1100/50000; xent: 3.52; lr: 0.0000022;  40 docs/s;    482 sec\n",
            "[2021-04-30 02:21:29,795 INFO] Step 1150/50000; xent: 3.61; lr: 0.0000023;  40 docs/s;    503 sec\n",
            "[2021-04-30 02:21:34,759 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.98.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:21:52,723 INFO] Step 1200/50000; xent: 3.56; lr: 0.0000024;  38 docs/s;    526 sec\n",
            "[2021-04-30 02:22:14,018 INFO] Step 1250/50000; xent: 3.52; lr: 0.0000025;  39 docs/s;    548 sec\n",
            "[2021-04-30 02:22:35,208 INFO] Step 1300/50000; xent: 3.53; lr: 0.0000026;  43 docs/s;    569 sec\n",
            "[2021-04-30 02:22:56,536 INFO] Step 1350/50000; xent: 3.44; lr: 0.0000027;  41 docs/s;    590 sec\n",
            "[2021-04-30 02:23:15,761 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.93.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:23:20,577 INFO] Step 1400/50000; xent: 3.57; lr: 0.0000028;  36 docs/s;    614 sec\n",
            "[2021-04-30 02:23:41,962 INFO] Step 1450/50000; xent: 3.44; lr: 0.0000029;  39 docs/s;    636 sec\n",
            "[2021-04-30 02:24:03,206 INFO] Step 1500/50000; xent: 3.53; lr: 0.0000030;  40 docs/s;    657 sec\n",
            "[2021-04-30 02:24:24,655 INFO] Step 1550/50000; xent: 3.47; lr: 0.0000031;  40 docs/s;    678 sec\n",
            "[2021-04-30 02:24:45,721 INFO] Step 1600/50000; xent: 3.50; lr: 0.0000032;  41 docs/s;    699 sec\n",
            "[2021-04-30 02:24:57,377 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.63.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:25:08,910 INFO] Step 1650/50000; xent: 3.51; lr: 0.0000033;  37 docs/s;    723 sec\n",
            "[2021-04-30 02:25:30,204 INFO] Step 1700/50000; xent: 3.44; lr: 0.0000034;  41 docs/s;    744 sec\n",
            "[2021-04-30 02:25:51,821 INFO] Step 1750/50000; xent: 3.48; lr: 0.0000035;  39 docs/s;    765 sec\n",
            "[2021-04-30 02:26:13,158 INFO] Step 1800/50000; xent: 3.54; lr: 0.0000036;  41 docs/s;    787 sec\n",
            "[2021-04-30 02:26:34,416 INFO] Step 1850/50000; xent: 3.54; lr: 0.0000037;  40 docs/s;    808 sec\n",
            "[2021-04-30 02:26:38,566 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.15.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:26:58,779 INFO] Step 1900/50000; xent: 3.54; lr: 0.0000038;  36 docs/s;    832 sec\n",
            "[2021-04-30 02:27:19,955 INFO] Step 1950/50000; xent: 3.42; lr: 0.0000039;  41 docs/s;    854 sec\n",
            "[2021-04-30 02:27:41,132 INFO] Step 2000/50000; xent: 3.43; lr: 0.0000040;  41 docs/s;    875 sec\n",
            "[2021-04-30 02:27:41,134 INFO] Saving checkpoint ../data/trained_models/Daniella/model_step_2000.pt\n",
            "[2021-04-30 02:28:08,983 INFO] Step 2050/50000; xent: 3.52; lr: 0.0000041;  31 docs/s;    903 sec\n",
            "[2021-04-30 02:28:26,498 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.64.bert.pt, number of examples: 1998\n",
            "[2021-04-30 02:28:33,045 INFO] Step 2100/50000; xent: 3.46; lr: 0.0000042;  36 docs/s;    927 sec\n",
            "[2021-04-30 02:28:54,342 INFO] Step 2150/50000; xent: 3.60; lr: 0.0000043;  40 docs/s;    948 sec\n",
            "[2021-04-30 02:29:15,647 INFO] Step 2200/50000; xent: 3.47; lr: 0.0000044;  40 docs/s;    969 sec\n",
            "[2021-04-30 02:29:36,935 INFO] Step 2250/50000; xent: 3.45; lr: 0.0000045;  41 docs/s;    991 sec\n",
            "[2021-04-30 02:29:58,230 INFO] Step 2300/50000; xent: 3.40; lr: 0.0000046;  40 docs/s;   1012 sec\n",
            "[2021-04-30 02:30:07,359 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.28.bert.pt, number of examples: 1999\n",
            "[2021-04-30 02:30:21,512 INFO] Step 2350/50000; xent: 3.39; lr: 0.0000047;  38 docs/s;   1035 sec\n",
            "[2021-04-30 02:30:43,007 INFO] Step 2400/50000; xent: 3.43; lr: 0.0000048;  40 docs/s;   1057 sec\n",
            "[2021-04-30 02:31:04,377 INFO] Step 2450/50000; xent: 3.40; lr: 0.0000049;  39 docs/s;   1078 sec\n",
            "[2021-04-30 02:31:25,803 INFO] Step 2500/50000; xent: 3.53; lr: 0.0000050;  40 docs/s;   1099 sec\n",
            "[2021-04-30 02:31:47,007 INFO] Step 2550/50000; xent: 3.55; lr: 0.0000051;  40 docs/s;   1121 sec\n",
            "[2021-04-30 02:31:48,567 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.32.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:32:09,951 INFO] Step 2600/50000; xent: 3.51; lr: 0.0000052;  37 docs/s;   1144 sec\n",
            "[2021-04-30 02:32:31,188 INFO] Step 2650/50000; xent: 3.46; lr: 0.0000053;  40 docs/s;   1165 sec\n",
            "[2021-04-30 02:32:52,576 INFO] Step 2700/50000; xent: 3.35; lr: 0.0000054;  42 docs/s;   1186 sec\n",
            "[2021-04-30 02:33:14,242 INFO] Step 2750/50000; xent: 3.44; lr: 0.0000055;  41 docs/s;   1208 sec\n",
            "[2021-04-30 02:33:29,419 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.111.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:33:37,605 INFO] Step 2800/50000; xent: 3.37; lr: 0.0000056;  38 docs/s;   1231 sec\n",
            "[2021-04-30 02:33:58,858 INFO] Step 2850/50000; xent: 3.50; lr: 0.0000057;  40 docs/s;   1252 sec\n",
            "[2021-04-30 02:34:20,062 INFO] Step 2900/50000; xent: 3.50; lr: 0.0000058;  41 docs/s;   1274 sec\n",
            "[2021-04-30 02:34:41,489 INFO] Step 2950/50000; xent: 3.45; lr: 0.0000059;  39 docs/s;   1295 sec\n",
            "[2021-04-30 02:35:02,521 INFO] Step 3000/50000; xent: 3.42; lr: 0.0000060;  42 docs/s;   1316 sec\n",
            "[2021-04-30 02:35:02,524 INFO] Saving checkpoint ../data/trained_models/Daniella/model_step_3000.pt\n",
            "[2021-04-30 02:35:16,377 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.49.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:35:33,108 INFO] Step 3050/50000; xent: 3.48; lr: 0.0000061;  28 docs/s;   1347 sec\n",
            "[2021-04-30 02:35:54,866 INFO] Step 3100/50000; xent: 3.40; lr: 0.0000062;  41 docs/s;   1369 sec\n",
            "[2021-04-30 02:36:16,050 INFO] Step 3150/50000; xent: 3.39; lr: 0.0000063;  41 docs/s;   1390 sec\n",
            "[2021-04-30 02:36:37,333 INFO] Step 3200/50000; xent: 3.42; lr: 0.0000064;  41 docs/s;   1411 sec\n",
            "[2021-04-30 02:36:57,667 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.59.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:37:00,760 INFO] Step 3250/50000; xent: 3.44; lr: 0.0000065;  37 docs/s;   1434 sec\n",
            "[2021-04-30 02:37:22,073 INFO] Step 3300/50000; xent: 3.49; lr: 0.0000066;  41 docs/s;   1456 sec\n",
            "[2021-04-30 02:37:43,302 INFO] Step 3350/50000; xent: 3.41; lr: 0.0000067;  40 docs/s;   1477 sec\n",
            "[2021-04-30 02:38:04,667 INFO] Step 3400/50000; xent: 3.48; lr: 0.0000068;  41 docs/s;   1498 sec\n",
            "[2021-04-30 02:38:26,200 INFO] Step 3450/50000; xent: 3.41; lr: 0.0000069;  39 docs/s;   1520 sec\n",
            "[2021-04-30 02:38:38,164 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.84.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:38:48,911 INFO] Step 3500/50000; xent: 3.38; lr: 0.0000070;  39 docs/s;   1543 sec\n",
            "[2021-04-30 02:39:10,124 INFO] Step 3550/50000; xent: 3.36; lr: 0.0000071;  41 docs/s;   1564 sec\n",
            "[2021-04-30 02:39:31,407 INFO] Step 3600/50000; xent: 3.50; lr: 0.0000072;  41 docs/s;   1585 sec\n",
            "[2021-04-30 02:39:52,602 INFO] Step 3650/50000; xent: 3.53; lr: 0.0000073;  41 docs/s;   1606 sec\n",
            "[2021-04-30 02:40:13,932 INFO] Step 3700/50000; xent: 3.46; lr: 0.0000074;  40 docs/s;   1628 sec\n",
            "[2021-04-30 02:40:18,160 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.137.bert.pt, number of examples: 2001\n",
            "[2021-04-30 02:40:36,768 INFO] Step 3750/50000; xent: 3.50; lr: 0.0000075;  37 docs/s;   1650 sec\n",
            "[2021-04-30 02:40:58,118 INFO] Step 3800/50000; xent: 3.56; lr: 0.0000076;  40 docs/s;   1672 sec\n",
            "[2021-04-30 02:41:19,357 INFO] Step 3850/50000; xent: 3.40; lr: 0.0000077;  41 docs/s;   1693 sec\n",
            "[2021-04-30 02:41:40,481 INFO] Step 3900/50000; xent: 3.47; lr: 0.0000078;  41 docs/s;   1714 sec\n",
            "[2021-04-30 02:41:59,636 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.13.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:42:03,927 INFO] Step 3950/50000; xent: 3.44; lr: 0.0000079;  37 docs/s;   1738 sec\n",
            "[2021-04-30 02:42:25,269 INFO] Step 4000/50000; xent: 3.37; lr: 0.0000080;  41 docs/s;   1759 sec\n",
            "[2021-04-30 02:42:25,272 INFO] Saving checkpoint ../data/trained_models/Daniella/model_step_4000.pt\n",
            "[2021-04-30 02:42:52,720 INFO] Step 4050/50000; xent: 3.36; lr: 0.0000081;  32 docs/s;   1786 sec\n",
            "[2021-04-30 02:43:14,696 INFO] Step 4100/50000; xent: 3.33; lr: 0.0000082;  39 docs/s;   1808 sec\n",
            "[2021-04-30 02:43:35,910 INFO] Step 4150/50000; xent: 3.51; lr: 0.0000083;  41 docs/s;   1830 sec\n",
            "[2021-04-30 02:43:46,016 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.135.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:43:58,773 INFO] Step 4200/50000; xent: 3.38; lr: 0.0000084;  38 docs/s;   1852 sec\n",
            "[2021-04-30 02:44:20,100 INFO] Step 4250/50000; xent: 3.47; lr: 0.0000085;  40 docs/s;   1874 sec\n",
            "[2021-04-30 02:44:41,332 INFO] Step 4300/50000; xent: 3.42; lr: 0.0000086;  41 docs/s;   1895 sec\n",
            "[2021-04-30 02:45:02,502 INFO] Step 4350/50000; xent: 3.44; lr: 0.0000087;  41 docs/s;   1916 sec\n",
            "[2021-04-30 02:45:23,777 INFO] Step 4400/50000; xent: 3.50; lr: 0.0000088;  39 docs/s;   1937 sec\n",
            "[2021-04-30 02:45:26,807 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.71.bert.pt, number of examples: 1999\n",
            "[2021-04-30 02:45:47,117 INFO] Step 4450/50000; xent: 3.39; lr: 0.0000089;  38 docs/s;   1961 sec\n",
            "[2021-04-30 02:46:08,353 INFO] Step 4500/50000; xent: 3.43; lr: 0.0000090;  39 docs/s;   1982 sec\n",
            "[2021-04-30 02:46:29,518 INFO] Step 4550/50000; xent: 3.46; lr: 0.0000091;  41 docs/s;   2003 sec\n",
            "[2021-04-30 02:46:50,714 INFO] Step 4600/50000; xent: 3.37; lr: 0.0000092;  41 docs/s;   2024 sec\n",
            "[2021-04-30 02:47:07,320 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.9.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:47:13,735 INFO] Step 4650/50000; xent: 3.42; lr: 0.0000093;  37 docs/s;   2047 sec\n",
            "[2021-04-30 02:47:34,988 INFO] Step 4700/50000; xent: 3.40; lr: 0.0000094;  41 docs/s;   2069 sec\n",
            "[2021-04-30 02:47:56,446 INFO] Step 4750/50000; xent: 3.45; lr: 0.0000095;  40 docs/s;   2090 sec\n",
            "[2021-04-30 02:48:18,093 INFO] Step 4800/50000; xent: 3.49; lr: 0.0000096;  39 docs/s;   2112 sec\n",
            "[2021-04-30 02:48:39,400 INFO] Step 4850/50000; xent: 3.29; lr: 0.0000097;  41 docs/s;   2133 sec\n",
            "[2021-04-30 02:48:48,041 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.105.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:49:01,989 INFO] Step 4900/50000; xent: 3.48; lr: 0.0000098;  39 docs/s;   2156 sec\n",
            "[2021-04-30 02:49:23,341 INFO] Step 4950/50000; xent: 3.55; lr: 0.0000099;  40 docs/s;   2177 sec\n",
            "[2021-04-30 02:49:44,668 INFO] Step 5000/50000; xent: 3.47; lr: 0.0000100;  40 docs/s;   2198 sec\n",
            "[2021-04-30 02:49:44,671 INFO] Saving checkpoint ../data/trained_models/Daniella/model_step_5000.pt\n",
            "[2021-04-30 02:50:13,116 INFO] Step 5050/50000; xent: 3.48; lr: 0.0000101;  30 docs/s;   2227 sec\n",
            "[2021-04-30 02:50:36,629 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.50.bert.pt, number of examples: 1999\n",
            "[2021-04-30 02:50:37,171 INFO] Step 5100/50000; xent: 3.56; lr: 0.0000102;  35 docs/s;   2251 sec\n",
            "[2021-04-30 02:50:58,719 INFO] Step 5150/50000; xent: 3.42; lr: 0.0000103;  39 docs/s;   2272 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv_7YX3wsyIu",
        "outputId": "8ad8a728-f78e-4a3b-98ee-561678b401f1"
      },
      "source": [
        "!python3 train.py -task ext -encoder roberta -mode train -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -ext_dropout 0.1 -model_path ../data/trained_models/Tushar -lr 1e-4 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_roberta_cnndm -use_interval true -warmup_steps 10000 -max_pos 514"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "----TRAIN EXTRACTIVE----\n",
            "[2021-04-30 02:57:46,299 INFO] Device ID 0\n",
            "[2021-04-30 02:57:46,972 INFO] Device cuda\n",
            "[2021-04-30 02:57:47,354 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-04-30 02:57:47,730 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-04-30 02:57:48,095 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-04-30 02:58:13,281 INFO] ExtSummarizer(\n",
            "  (bert): Roberta(\n",
            "    (model): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(516, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ext_layer): ExtTransformerEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-04-30 02:58:13,382 INFO] * number of parameters: 135678209\n",
            "[2021-04-30 02:58:13,382 INFO] Start training...\n",
            "[2021-04-30 02:58:18,306 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.123.bert.pt, number of examples: 2000\n",
            "[2021-04-30 02:58:53,705 INFO] Step 50/50000; xent: 4.99; lr: 0.0000000;  29 docs/s;     35 sec\n",
            "[2021-04-30 02:59:28,750 INFO] Step 100/50000; xent: 4.74; lr: 0.0000000;  30 docs/s;     70 sec\n",
            "[2021-04-30 03:00:04,094 INFO] Step 150/50000; xent: 4.31; lr: 0.0000000;  30 docs/s;    106 sec\n",
            "[2021-04-30 03:00:34,561 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.91.bert.pt, number of examples: 1995\n",
            "[2021-04-30 03:00:40,981 INFO] Step 200/50000; xent: 3.95; lr: 0.0000000;  28 docs/s;    143 sec\n",
            "[2021-04-30 03:01:16,090 INFO] Step 250/50000; xent: 3.71; lr: 0.0000000;  29 docs/s;    178 sec\n",
            "[2021-04-30 03:01:51,121 INFO] Step 300/50000; xent: 3.53; lr: 0.0000000;  29 docs/s;    213 sec\n",
            "[2021-04-30 03:02:26,032 INFO] Step 350/50000; xent: 3.44; lr: 0.0000000;  30 docs/s;    248 sec\n",
            "[2021-04-30 03:02:52,665 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.39.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:03:04,028 INFO] Step 400/50000; xent: 3.38; lr: 0.0000000;  27 docs/s;    286 sec\n",
            "[2021-04-30 03:03:39,120 INFO] Step 450/50000; xent: 3.38; lr: 0.0000000;  29 docs/s;    321 sec\n",
            "[2021-04-30 03:04:14,228 INFO] Step 500/50000; xent: 3.30; lr: 0.0000001;  31 docs/s;    356 sec\n",
            "[2021-04-30 03:04:49,333 INFO] Step 550/50000; xent: 3.30; lr: 0.0000001;  29 docs/s;    391 sec\n",
            "[2021-04-30 03:05:10,965 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.6.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:05:27,925 INFO] Step 600/50000; xent: 3.36; lr: 0.0000001;  26 docs/s;    430 sec\n",
            "[2021-04-30 03:06:03,072 INFO] Step 650/50000; xent: 3.43; lr: 0.0000001;  29 docs/s;    465 sec\n",
            "[2021-04-30 03:06:38,152 INFO] Step 700/50000; xent: 3.36; lr: 0.0000001;  29 docs/s;    500 sec\n",
            "[2021-04-30 03:07:13,310 INFO] Step 750/50000; xent: 3.30; lr: 0.0000001;  30 docs/s;    535 sec\n",
            "[2021-04-30 03:07:27,658 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.81.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:07:50,170 INFO] Step 800/50000; xent: 3.29; lr: 0.0000001;  28 docs/s;    572 sec\n",
            "[2021-04-30 03:08:25,257 INFO] Step 850/50000; xent: 3.25; lr: 0.0000001;  30 docs/s;    607 sec\n",
            "[2021-04-30 03:09:00,322 INFO] Step 900/50000; xent: 3.32; lr: 0.0000001;  29 docs/s;    642 sec\n",
            "[2021-04-30 03:09:35,038 INFO] Step 950/50000; xent: 3.25; lr: 0.0000001;  30 docs/s;    677 sec\n",
            "[2021-04-30 03:09:44,793 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.98.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:10:12,291 INFO] Step 1000/50000; xent: 3.23; lr: 0.0000001;  28 docs/s;    714 sec\n",
            "[2021-04-30 03:10:12,305 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_1000.pt\n",
            "[2021-04-30 03:10:56,459 INFO] Step 1050/50000; xent: 3.28; lr: 0.0000001;  23 docs/s;    758 sec\n",
            "[2021-04-30 03:11:31,607 INFO] Step 1100/50000; xent: 3.27; lr: 0.0000001;  30 docs/s;    793 sec\n",
            "[2021-04-30 03:12:06,551 INFO] Step 1150/50000; xent: 3.29; lr: 0.0000001;  29 docs/s;    828 sec\n",
            "[2021-04-30 03:12:10,723 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.93.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:12:43,485 INFO] Step 1200/50000; xent: 3.16; lr: 0.0000001;  28 docs/s;    865 sec\n",
            "[2021-04-30 03:13:18,593 INFO] Step 1250/50000; xent: 3.23; lr: 0.0000001;  29 docs/s;    900 sec\n",
            "[2021-04-30 03:13:53,711 INFO] Step 1300/50000; xent: 3.21; lr: 0.0000001;  29 docs/s;    935 sec\n",
            "[2021-04-30 03:14:29,036 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.63.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:14:31,254 INFO] Step 1350/50000; xent: 3.25; lr: 0.0000001;  27 docs/s;    973 sec\n",
            "[2021-04-30 03:15:06,083 INFO] Step 1400/50000; xent: 3.22; lr: 0.0000001;  30 docs/s;   1008 sec\n",
            "[2021-04-30 03:15:41,212 INFO] Step 1450/50000; xent: 3.23; lr: 0.0000001;  29 docs/s;   1043 sec\n",
            "[2021-04-30 03:16:16,297 INFO] Step 1500/50000; xent: 3.18; lr: 0.0000002;  30 docs/s;   1078 sec\n",
            "[2021-04-30 03:16:45,673 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.15.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:16:53,507 INFO] Step 1550/50000; xent: 3.22; lr: 0.0000002;  28 docs/s;   1115 sec\n",
            "[2021-04-30 03:17:28,472 INFO] Step 1600/50000; xent: 3.23; lr: 0.0000002;  30 docs/s;   1150 sec\n",
            "[2021-04-30 03:18:03,572 INFO] Step 1650/50000; xent: 3.14; lr: 0.0000002;  30 docs/s;   1185 sec\n",
            "[2021-04-30 03:18:38,675 INFO] Step 1700/50000; xent: 3.11; lr: 0.0000002;  29 docs/s;   1220 sec\n",
            "[2021-04-30 03:19:03,356 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.64.bert.pt, number of examples: 1998\n",
            "[2021-04-30 03:19:16,142 INFO] Step 1750/50000; xent: 3.15; lr: 0.0000002;  27 docs/s;   1258 sec\n",
            "[2021-04-30 03:19:51,073 INFO] Step 1800/50000; xent: 3.19; lr: 0.0000002;  29 docs/s;   1293 sec\n",
            "[2021-04-30 03:20:26,101 INFO] Step 1850/50000; xent: 3.10; lr: 0.0000002;  31 docs/s;   1328 sec\n",
            "[2021-04-30 03:21:01,277 INFO] Step 1900/50000; xent: 3.18; lr: 0.0000002;  29 docs/s;   1363 sec\n",
            "[2021-04-30 03:21:20,472 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.28.bert.pt, number of examples: 1999\n",
            "[2021-04-30 03:21:38,885 INFO] Step 1950/50000; xent: 3.08; lr: 0.0000002;  27 docs/s;   1401 sec\n",
            "[2021-04-30 03:22:13,991 INFO] Step 2000/50000; xent: 3.12; lr: 0.0000002;  30 docs/s;   1436 sec\n",
            "[2021-04-30 03:22:14,005 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_2000.pt\n",
            "[2021-04-30 03:22:58,392 INFO] Step 2050/50000; xent: 3.17; lr: 0.0000002;  24 docs/s;   1480 sec\n",
            "[2021-04-30 03:23:33,517 INFO] Step 2100/50000; xent: 3.11; lr: 0.0000002;  29 docs/s;   1515 sec\n",
            "[2021-04-30 03:23:47,704 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.32.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:24:10,744 INFO] Step 2150/50000; xent: 3.11; lr: 0.0000002;  28 docs/s;   1552 sec\n",
            "[2021-04-30 03:24:45,817 INFO] Step 2200/50000; xent: 3.08; lr: 0.0000002;  30 docs/s;   1588 sec\n",
            "[2021-04-30 03:25:20,804 INFO] Step 2250/50000; xent: 3.04; lr: 0.0000002;  31 docs/s;   1622 sec\n",
            "[2021-04-30 03:25:55,776 INFO] Step 2300/50000; xent: 3.09; lr: 0.0000002;  30 docs/s;   1657 sec\n",
            "[2021-04-30 03:26:04,378 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.111.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:26:33,249 INFO] Step 2350/50000; xent: 3.11; lr: 0.0000002;  28 docs/s;   1695 sec\n",
            "[2021-04-30 03:27:08,241 INFO] Step 2400/50000; xent: 3.11; lr: 0.0000002;  30 docs/s;   1730 sec\n",
            "[2021-04-30 03:27:43,292 INFO] Step 2450/50000; xent: 3.07; lr: 0.0000002;  30 docs/s;   1765 sec\n",
            "[2021-04-30 03:28:18,165 INFO] Step 2500/50000; xent: 3.07; lr: 0.0000003;  30 docs/s;   1800 sec\n",
            "[2021-04-30 03:28:21,949 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.49.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:28:56,428 INFO] Step 2550/50000; xent: 3.07; lr: 0.0000003;  27 docs/s;   1838 sec\n",
            "[2021-04-30 03:29:31,404 INFO] Step 2600/50000; xent: 3.09; lr: 0.0000003;  30 docs/s;   1873 sec\n",
            "[2021-04-30 03:30:06,434 INFO] Step 2650/50000; xent: 3.06; lr: 0.0000003;  29 docs/s;   1908 sec\n",
            "[2021-04-30 03:30:38,914 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.59.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:30:43,923 INFO] Step 2700/50000; xent: 3.06; lr: 0.0000003;  28 docs/s;   1946 sec\n",
            "[2021-04-30 03:31:18,965 INFO] Step 2750/50000; xent: 3.06; lr: 0.0000003;  29 docs/s;   1981 sec\n",
            "[2021-04-30 03:31:53,815 INFO] Step 2800/50000; xent: 3.14; lr: 0.0000003;  30 docs/s;   2016 sec\n",
            "[2021-04-30 03:32:28,906 INFO] Step 2850/50000; xent: 3.02; lr: 0.0000003;  29 docs/s;   2051 sec\n",
            "[2021-04-30 03:32:56,205 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.84.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:33:06,115 INFO] Step 2900/50000; xent: 3.06; lr: 0.0000003;  27 docs/s;   2088 sec\n",
            "[2021-04-30 03:33:41,204 INFO] Step 2950/50000; xent: 3.06; lr: 0.0000003;  29 docs/s;   2123 sec\n",
            "[2021-04-30 03:34:16,196 INFO] Step 3000/50000; xent: 3.15; lr: 0.0000003;  31 docs/s;   2158 sec\n",
            "[2021-04-30 03:34:16,211 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_3000.pt\n",
            "[2021-04-30 03:35:00,326 INFO] Step 3050/50000; xent: 3.06; lr: 0.0000003;  23 docs/s;   2202 sec\n",
            "[2021-04-30 03:35:22,853 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.137.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:35:37,695 INFO] Step 3100/50000; xent: 3.07; lr: 0.0000003;  28 docs/s;   2239 sec\n",
            "[2021-04-30 03:36:12,806 INFO] Step 3150/50000; xent: 3.07; lr: 0.0000003;  29 docs/s;   2274 sec\n",
            "[2021-04-30 03:36:47,652 INFO] Step 3200/50000; xent: 3.19; lr: 0.0000003;  29 docs/s;   2309 sec\n",
            "[2021-04-30 03:37:22,674 INFO] Step 3250/50000; xent: 3.13; lr: 0.0000003;  30 docs/s;   2344 sec\n",
            "[2021-04-30 03:37:41,040 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.13.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:38:00,092 INFO] Step 3300/50000; xent: 3.01; lr: 0.0000003;  28 docs/s;   2382 sec\n",
            "[2021-04-30 03:38:35,054 INFO] Step 3350/50000; xent: 3.09; lr: 0.0000003;  29 docs/s;   2417 sec\n",
            "[2021-04-30 03:39:10,084 INFO] Step 3400/50000; xent: 2.99; lr: 0.0000003;  31 docs/s;   2452 sec\n",
            "[2021-04-30 03:39:45,138 INFO] Step 3450/50000; xent: 3.06; lr: 0.0000003;  30 docs/s;   2487 sec\n",
            "[2021-04-30 03:39:56,688 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.135.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:40:22,104 INFO] Step 3500/50000; xent: 3.10; lr: 0.0000003;  27 docs/s;   2524 sec\n",
            "[2021-04-30 03:40:57,261 INFO] Step 3550/50000; xent: 3.06; lr: 0.0000004;  30 docs/s;   2559 sec\n",
            "[2021-04-30 03:41:32,273 INFO] Step 3600/50000; xent: 3.11; lr: 0.0000004;  30 docs/s;   2594 sec\n",
            "[2021-04-30 03:42:07,210 INFO] Step 3650/50000; xent: 3.12; lr: 0.0000004;  30 docs/s;   2629 sec\n",
            "[2021-04-30 03:42:14,674 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.71.bert.pt, number of examples: 1999\n",
            "[2021-04-30 03:42:44,820 INFO] Step 3700/50000; xent: 3.07; lr: 0.0000004;  27 docs/s;   2667 sec\n",
            "[2021-04-30 03:43:19,969 INFO] Step 3750/50000; xent: 3.03; lr: 0.0000004;  30 docs/s;   2702 sec\n",
            "[2021-04-30 03:43:55,143 INFO] Step 3800/50000; xent: 3.07; lr: 0.0000004;  30 docs/s;   2737 sec\n",
            "[2021-04-30 03:44:31,905 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.9.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:44:32,725 INFO] Step 3850/50000; xent: 3.06; lr: 0.0000004;  28 docs/s;   2774 sec\n",
            "[2021-04-30 03:45:07,852 INFO] Step 3900/50000; xent: 3.03; lr: 0.0000004;  30 docs/s;   2810 sec\n",
            "[2021-04-30 03:45:42,788 INFO] Step 3950/50000; xent: 3.04; lr: 0.0000004;  29 docs/s;   2844 sec\n",
            "[2021-04-30 03:46:17,893 INFO] Step 4000/50000; xent: 3.06; lr: 0.0000004;  30 docs/s;   2880 sec\n",
            "[2021-04-30 03:46:17,907 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_4000.pt\n",
            "[2021-04-30 03:46:59,152 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.105.bert.pt, number of examples: 2000\n",
            "[2021-04-30 03:47:04,923 INFO] Step 4050/50000; xent: 3.02; lr: 0.0000004;  22 docs/s;   2927 sec\n",
            "[2021-04-30 03:47:40,046 INFO] Step 4100/50000; xent: 3.13; lr: 0.0000004;  30 docs/s;   2962 sec\n",
            "[2021-04-30 03:48:15,264 INFO] Step 4150/50000; xent: 3.19; lr: 0.0000004;  30 docs/s;   2997 sec\n",
            "[2021-04-30 03:48:50,329 INFO] Step 4200/50000; xent: 3.05; lr: 0.0000004;  30 docs/s;   3032 sec\n",
            "[2021-04-30 03:49:17,351 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.50.bert.pt, number of examples: 1999\n",
            "[2021-04-30 03:49:27,986 INFO] Step 4250/50000; xent: 3.14; lr: 0.0000004;  27 docs/s;   3070 sec\n",
            "[2021-04-30 03:50:03,034 INFO] Step 4300/50000; xent: 3.10; lr: 0.0000004;  30 docs/s;   3105 sec\n",
            "[2021-04-30 03:50:37,987 INFO] Step 4350/50000; xent: 3.04; lr: 0.0000004;  30 docs/s;   3140 sec\n",
            "[2021-04-30 03:51:13,051 INFO] Step 4400/50000; xent: 3.10; lr: 0.0000004;  29 docs/s;   3175 sec\n",
            "[2021-04-30 03:51:34,615 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.106.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:51:50,181 INFO] Step 4450/50000; xent: 3.11; lr: 0.0000004;  28 docs/s;   3212 sec\n",
            "[2021-04-30 03:52:25,391 INFO] Step 4500/50000; xent: 3.14; lr: 0.0000004;  31 docs/s;   3247 sec\n",
            "[2021-04-30 03:53:00,550 INFO] Step 4550/50000; xent: 3.05; lr: 0.0000005;  29 docs/s;   3282 sec\n",
            "[2021-04-30 03:53:35,746 INFO] Step 4600/50000; xent: 3.01; lr: 0.0000005;  29 docs/s;   3317 sec\n",
            "[2021-04-30 03:53:51,401 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.38.bert.pt, number of examples: 2001\n",
            "[2021-04-30 03:54:12,626 INFO] Step 4650/50000; xent: 3.06; lr: 0.0000005;  28 docs/s;   3354 sec\n",
            "[2021-04-30 03:54:47,875 INFO] Step 4700/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;   3390 sec\n",
            "[2021-04-30 03:55:23,098 INFO] Step 4750/50000; xent: 3.08; lr: 0.0000005;  30 docs/s;   3425 sec\n",
            "[2021-04-30 03:55:58,103 INFO] Step 4800/50000; xent: 3.10; lr: 0.0000005;  29 docs/s;   3460 sec\n",
            "[2021-04-30 03:56:09,100 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.24.bert.pt, number of examples: 1999\n",
            "[2021-04-30 03:56:35,344 INFO] Step 4850/50000; xent: 3.00; lr: 0.0000005;  28 docs/s;   3497 sec\n",
            "[2021-04-30 03:57:10,311 INFO] Step 4900/50000; xent: 3.09; lr: 0.0000005;  30 docs/s;   3532 sec\n",
            "[2021-04-30 03:57:45,632 INFO] Step 4950/50000; xent: 3.14; lr: 0.0000005;  29 docs/s;   3567 sec\n",
            "[2021-04-30 03:58:20,607 INFO] Step 5000/50000; xent: 3.05; lr: 0.0000005;  30 docs/s;   3602 sec\n",
            "[2021-04-30 03:58:20,610 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_5000.pt\n",
            "[2021-04-30 03:58:34,687 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.143.bert.pt, number of examples: 1084\n",
            "[2021-04-30 03:59:07,343 INFO] Step 5050/50000; xent: 3.02; lr: 0.0000005;  23 docs/s;   3649 sec\n",
            "[2021-04-30 03:59:42,419 INFO] Step 5100/50000; xent: 3.05; lr: 0.0000005;  29 docs/s;   3684 sec\n",
            "[2021-04-30 03:59:52,188 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.94.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:00:20,379 INFO] Step 5150/50000; xent: 2.97; lr: 0.0000005;  27 docs/s;   3722 sec\n",
            "[2021-04-30 04:00:55,361 INFO] Step 5200/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;   3757 sec\n",
            "[2021-04-30 04:01:30,425 INFO] Step 5250/50000; xent: 3.14; lr: 0.0000005;  30 docs/s;   3792 sec\n",
            "[2021-04-30 04:02:05,348 INFO] Step 5300/50000; xent: 3.03; lr: 0.0000005;  30 docs/s;   3827 sec\n",
            "[2021-04-30 04:02:08,700 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.62.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:02:42,300 INFO] Step 5350/50000; xent: 3.09; lr: 0.0000005;  28 docs/s;   3864 sec\n",
            "[2021-04-30 04:03:17,443 INFO] Step 5400/50000; xent: 3.10; lr: 0.0000005;  30 docs/s;   3899 sec\n",
            "[2021-04-30 04:03:52,625 INFO] Step 5450/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;   3934 sec\n",
            "[2021-04-30 04:04:25,592 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.139.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:04:29,894 INFO] Step 5500/50000; xent: 3.04; lr: 0.0000006;  27 docs/s;   3972 sec\n",
            "[2021-04-30 04:05:05,028 INFO] Step 5550/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;   4007 sec\n",
            "[2021-04-30 04:05:40,059 INFO] Step 5600/50000; xent: 2.94; lr: 0.0000006;  30 docs/s;   4042 sec\n",
            "[2021-04-30 04:06:15,050 INFO] Step 5650/50000; xent: 3.13; lr: 0.0000006;  30 docs/s;   4077 sec\n",
            "[2021-04-30 04:06:42,761 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.88.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:06:51,969 INFO] Step 5700/50000; xent: 3.08; lr: 0.0000006;  29 docs/s;   4114 sec\n",
            "[2021-04-30 04:07:26,898 INFO] Step 5750/50000; xent: 3.04; lr: 0.0000006;  29 docs/s;   4149 sec\n",
            "[2021-04-30 04:08:01,966 INFO] Step 5800/50000; xent: 3.03; lr: 0.0000006;  30 docs/s;   4184 sec\n",
            "[2021-04-30 04:08:37,099 INFO] Step 5850/50000; xent: 3.04; lr: 0.0000006;  30 docs/s;   4219 sec\n",
            "[2021-04-30 04:09:00,573 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.92.bert.pt, number of examples: 1998\n",
            "[2021-04-30 04:09:14,759 INFO] Step 5900/50000; xent: 3.17; lr: 0.0000006;  27 docs/s;   4256 sec\n",
            "[2021-04-30 04:09:49,955 INFO] Step 5950/50000; xent: 3.03; lr: 0.0000006;  29 docs/s;   4292 sec\n",
            "[2021-04-30 04:10:24,855 INFO] Step 6000/50000; xent: 3.01; lr: 0.0000006;  29 docs/s;   4327 sec\n",
            "[2021-04-30 04:10:24,870 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_6000.pt\n",
            "[2021-04-30 04:11:12,682 INFO] Step 6050/50000; xent: 3.07; lr: 0.0000006;  23 docs/s;   4374 sec\n",
            "[2021-04-30 04:11:29,832 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.68.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:11:49,616 INFO] Step 6100/50000; xent: 3.08; lr: 0.0000006;  27 docs/s;   4411 sec\n",
            "[2021-04-30 04:12:24,702 INFO] Step 6150/50000; xent: 3.01; lr: 0.0000006;  30 docs/s;   4446 sec\n",
            "[2021-04-30 04:12:59,817 INFO] Step 6200/50000; xent: 3.06; lr: 0.0000006;  30 docs/s;   4482 sec\n",
            "[2021-04-30 04:13:34,906 INFO] Step 6250/50000; xent: 2.99; lr: 0.0000006;  30 docs/s;   4517 sec\n",
            "[2021-04-30 04:13:47,220 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.0.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:14:11,903 INFO] Step 6300/50000; xent: 2.96; lr: 0.0000006;  28 docs/s;   4554 sec\n",
            "[2021-04-30 04:14:47,076 INFO] Step 6350/50000; xent: 3.00; lr: 0.0000006;  30 docs/s;   4589 sec\n",
            "[2021-04-30 04:15:22,155 INFO] Step 6400/50000; xent: 3.06; lr: 0.0000006;  29 docs/s;   4624 sec\n",
            "[2021-04-30 04:15:57,080 INFO] Step 6450/50000; xent: 3.11; lr: 0.0000006;  30 docs/s;   4659 sec\n",
            "[2021-04-30 04:16:05,273 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.83.bert.pt, number of examples: 1999\n",
            "[2021-04-30 04:16:34,900 INFO] Step 6500/50000; xent: 3.07; lr: 0.0000007;  28 docs/s;   4697 sec\n",
            "[2021-04-30 04:17:09,934 INFO] Step 6550/50000; xent: 2.98; lr: 0.0000007;  30 docs/s;   4732 sec\n",
            "[2021-04-30 04:17:45,116 INFO] Step 6600/50000; xent: 3.02; lr: 0.0000007;  30 docs/s;   4767 sec\n",
            "[2021-04-30 04:18:21,349 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.138.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:18:22,169 INFO] Step 6650/50000; xent: 3.05; lr: 0.0000007;  28 docs/s;   4804 sec\n",
            "[2021-04-30 04:18:57,446 INFO] Step 6700/50000; xent: 3.05; lr: 0.0000007;  29 docs/s;   4839 sec\n",
            "[2021-04-30 04:19:32,648 INFO] Step 6750/50000; xent: 3.02; lr: 0.0000007;  31 docs/s;   4874 sec\n",
            "[2021-04-30 04:20:07,840 INFO] Step 6800/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;   4910 sec\n",
            "[2021-04-30 04:20:38,637 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.72.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:20:45,075 INFO] Step 6850/50000; xent: 3.04; lr: 0.0000007;  28 docs/s;   4947 sec\n",
            "[2021-04-30 04:21:20,279 INFO] Step 6900/50000; xent: 3.02; lr: 0.0000007;  29 docs/s;   4982 sec\n",
            "[2021-04-30 04:21:55,585 INFO] Step 6950/50000; xent: 3.09; lr: 0.0000007;  30 docs/s;   5017 sec\n",
            "[2021-04-30 04:22:30,824 INFO] Step 7000/50000; xent: 3.02; lr: 0.0000007;  29 docs/s;   5053 sec\n",
            "[2021-04-30 04:22:30,839 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_7000.pt\n",
            "[2021-04-30 04:23:05,824 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.89.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:23:17,630 INFO] Step 7050/50000; xent: 2.98; lr: 0.0000007;  22 docs/s;   5099 sec\n",
            "[2021-04-30 04:23:52,832 INFO] Step 7100/50000; xent: 3.02; lr: 0.0000007;  30 docs/s;   5135 sec\n",
            "[2021-04-30 04:24:28,090 INFO] Step 7150/50000; xent: 3.06; lr: 0.0000007;  29 docs/s;   5170 sec\n",
            "[2021-04-30 04:25:03,422 INFO] Step 7200/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;   5205 sec\n",
            "[2021-04-30 04:25:23,609 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.51.bert.pt, number of examples: 1999\n",
            "[2021-04-30 04:25:40,634 INFO] Step 7250/50000; xent: 2.98; lr: 0.0000007;  28 docs/s;   5242 sec\n",
            "[2021-04-30 04:26:15,985 INFO] Step 7300/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;   5278 sec\n",
            "[2021-04-30 04:26:51,226 INFO] Step 7350/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;   5313 sec\n",
            "[2021-04-30 04:27:26,477 INFO] Step 7400/50000; xent: 3.02; lr: 0.0000007;  30 docs/s;   5348 sec\n",
            "[2021-04-30 04:27:42,125 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.35.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:28:04,151 INFO] Step 7450/50000; xent: 3.09; lr: 0.0000007;  27 docs/s;   5386 sec\n",
            "[2021-04-30 04:28:39,335 INFO] Step 7500/50000; xent: 3.11; lr: 0.0000008;  29 docs/s;   5421 sec\n",
            "[2021-04-30 04:29:14,578 INFO] Step 7550/50000; xent: 2.97; lr: 0.0000008;  29 docs/s;   5456 sec\n",
            "[2021-04-30 04:29:49,582 INFO] Step 7600/50000; xent: 3.04; lr: 0.0000008;  31 docs/s;   5491 sec\n",
            "[2021-04-30 04:30:00,452 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.37.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:30:27,347 INFO] Step 7650/50000; xent: 2.96; lr: 0.0000008;  28 docs/s;   5529 sec\n",
            "[2021-04-30 04:31:02,403 INFO] Step 7700/50000; xent: 2.94; lr: 0.0000008;  30 docs/s;   5564 sec\n",
            "[2021-04-30 04:31:37,517 INFO] Step 7750/50000; xent: 3.04; lr: 0.0000008;  29 docs/s;   5599 sec\n",
            "[2021-04-30 04:32:12,574 INFO] Step 7800/50000; xent: 3.01; lr: 0.0000008;  29 docs/s;   5634 sec\n",
            "[2021-04-30 04:32:17,423 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.80.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:32:49,835 INFO] Step 7850/50000; xent: 3.05; lr: 0.0000008;  28 docs/s;   5672 sec\n",
            "[2021-04-30 04:33:24,774 INFO] Step 7900/50000; xent: 3.13; lr: 0.0000008;  29 docs/s;   5706 sec\n",
            "[2021-04-30 04:33:59,886 INFO] Step 7950/50000; xent: 2.97; lr: 0.0000008;  30 docs/s;   5742 sec\n",
            "[2021-04-30 04:34:35,619 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.52.bert.pt, number of examples: 1999\n",
            "[2021-04-30 04:34:37,843 INFO] Step 8000/50000; xent: 3.01; lr: 0.0000008;  27 docs/s;   5780 sec\n",
            "[2021-04-30 04:34:37,858 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_8000.pt\n",
            "[2021-04-30 04:35:23,222 INFO] Step 8050/50000; xent: 2.97; lr: 0.0000008;  23 docs/s;   5825 sec\n",
            "[2021-04-30 04:35:58,388 INFO] Step 8100/50000; xent: 3.02; lr: 0.0000008;  29 docs/s;   5860 sec\n",
            "[2021-04-30 04:36:33,461 INFO] Step 8150/50000; xent: 2.97; lr: 0.0000008;  29 docs/s;   5895 sec\n",
            "[2021-04-30 04:37:02,405 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.12.bert.pt, number of examples: 1998\n",
            "[2021-04-30 04:37:10,217 INFO] Step 8200/50000; xent: 3.08; lr: 0.0000008;  28 docs/s;   5932 sec\n",
            "[2021-04-30 04:37:45,284 INFO] Step 8250/50000; xent: 3.01; lr: 0.0000008;  30 docs/s;   5967 sec\n",
            "[2021-04-30 04:38:20,459 INFO] Step 8300/50000; xent: 3.02; lr: 0.0000008;  29 docs/s;   6002 sec\n",
            "[2021-04-30 04:38:55,529 INFO] Step 8350/50000; xent: 3.04; lr: 0.0000008;  30 docs/s;   6037 sec\n",
            "[2021-04-30 04:39:19,167 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.114.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:39:32,637 INFO] Step 8400/50000; xent: 2.95; lr: 0.0000008;  28 docs/s;   6074 sec\n",
            "[2021-04-30 04:40:07,653 INFO] Step 8450/50000; xent: 3.11; lr: 0.0000008;  29 docs/s;   6109 sec\n",
            "[2021-04-30 04:40:42,844 INFO] Step 8500/50000; xent: 3.12; lr: 0.0000008;  29 docs/s;   6145 sec\n",
            "[2021-04-30 04:41:18,042 INFO] Step 8550/50000; xent: 2.94; lr: 0.0000009;  30 docs/s;   6180 sec\n",
            "[2021-04-30 04:41:35,943 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.86.bert.pt, number of examples: 1999\n",
            "[2021-04-30 04:41:55,052 INFO] Step 8600/50000; xent: 3.02; lr: 0.0000009;  28 docs/s;   6217 sec\n",
            "[2021-04-30 04:42:30,185 INFO] Step 8650/50000; xent: 3.13; lr: 0.0000009;  30 docs/s;   6252 sec\n",
            "[2021-04-30 04:43:05,328 INFO] Step 8700/50000; xent: 2.94; lr: 0.0000009;  30 docs/s;   6287 sec\n",
            "[2021-04-30 04:43:40,189 INFO] Step 8750/50000; xent: 3.02; lr: 0.0000009;  30 docs/s;   6322 sec\n",
            "[2021-04-30 04:43:54,033 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.113.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:44:18,017 INFO] Step 8800/50000; xent: 3.05; lr: 0.0000009;  27 docs/s;   6360 sec\n",
            "[2021-04-30 04:44:53,183 INFO] Step 8850/50000; xent: 2.98; lr: 0.0000009;  30 docs/s;   6395 sec\n",
            "[2021-04-30 04:45:28,302 INFO] Step 8900/50000; xent: 3.05; lr: 0.0000009;  29 docs/s;   6430 sec\n",
            "[2021-04-30 04:46:03,242 INFO] Step 8950/50000; xent: 3.04; lr: 0.0000009;  30 docs/s;   6465 sec\n",
            "[2021-04-30 04:46:11,354 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.96.bert.pt, number of examples: 1999\n",
            "[2021-04-30 04:46:40,318 INFO] Step 9000/50000; xent: 3.04; lr: 0.0000009;  28 docs/s;   6502 sec\n",
            "[2021-04-30 04:46:40,335 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_9000.pt\n",
            "[2021-04-30 04:47:24,981 INFO] Step 9050/50000; xent: 3.01; lr: 0.0000009;  23 docs/s;   6547 sec\n",
            "[2021-04-30 04:48:00,091 INFO] Step 9100/50000; xent: 3.07; lr: 0.0000009;  30 docs/s;   6582 sec\n",
            "[2021-04-30 04:48:35,199 INFO] Step 9150/50000; xent: 3.03; lr: 0.0000009;  30 docs/s;   6617 sec\n",
            "[2021-04-30 04:48:37,083 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.131.bert.pt, number of examples: 2001\n",
            "[2021-04-30 04:49:12,133 INFO] Step 9200/50000; xent: 3.05; lr: 0.0000009;  28 docs/s;   6654 sec\n",
            "[2021-04-30 04:49:47,242 INFO] Step 9250/50000; xent: 3.11; lr: 0.0000009;  29 docs/s;   6689 sec\n",
            "[2021-04-30 04:50:22,392 INFO] Step 9300/50000; xent: 3.03; lr: 0.0000009;  30 docs/s;   6724 sec\n",
            "[2021-04-30 04:50:53,826 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.97.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:50:59,603 INFO] Step 9350/50000; xent: 3.04; lr: 0.0000009;  28 docs/s;   6761 sec\n",
            "[2021-04-30 04:51:34,684 INFO] Step 9400/50000; xent: 3.01; lr: 0.0000009;  30 docs/s;   6796 sec\n",
            "[2021-04-30 04:52:09,844 INFO] Step 9450/50000; xent: 2.99; lr: 0.0000009;  30 docs/s;   6832 sec\n",
            "[2021-04-30 04:52:45,038 INFO] Step 9500/50000; xent: 3.07; lr: 0.0000010;  29 docs/s;   6867 sec\n",
            "[2021-04-30 04:53:11,461 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.55.bert.pt, number of examples: 1999\n",
            "[2021-04-30 04:53:22,119 INFO] Step 9550/50000; xent: 3.07; lr: 0.0000010;  28 docs/s;   6904 sec\n",
            "[2021-04-30 04:53:57,305 INFO] Step 9600/50000; xent: 2.99; lr: 0.0000010;  30 docs/s;   6939 sec\n",
            "[2021-04-30 04:54:32,433 INFO] Step 9650/50000; xent: 3.01; lr: 0.0000010;  30 docs/s;   6974 sec\n",
            "[2021-04-30 04:55:07,558 INFO] Step 9700/50000; xent: 3.06; lr: 0.0000010;  29 docs/s;   7009 sec\n",
            "[2021-04-30 04:55:29,072 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.128.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:55:45,349 INFO] Step 9750/50000; xent: 3.08; lr: 0.0000010;  27 docs/s;   7047 sec\n",
            "[2021-04-30 04:56:20,456 INFO] Step 9800/50000; xent: 2.96; lr: 0.0000010;  30 docs/s;   7082 sec\n",
            "[2021-04-30 04:56:55,330 INFO] Step 9850/50000; xent: 3.01; lr: 0.0000010;  30 docs/s;   7117 sec\n",
            "[2021-04-30 04:57:30,325 INFO] Step 9900/50000; xent: 3.08; lr: 0.0000010;  29 docs/s;   7152 sec\n",
            "[2021-04-30 04:57:45,807 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.104.bert.pt, number of examples: 2000\n",
            "[2021-04-30 04:58:07,440 INFO] Step 9950/50000; xent: 2.94; lr: 0.0000010;  28 docs/s;   7189 sec\n",
            "[2021-04-30 04:58:42,620 INFO] Step 10000/50000; xent: 3.08; lr: 0.0000010;  29 docs/s;   7224 sec\n",
            "[2021-04-30 04:58:42,635 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_10000.pt\n",
            "[2021-04-30 04:59:27,658 INFO] Step 10050/50000; xent: 2.96; lr: 0.0000010;  24 docs/s;   7269 sec\n",
            "[2021-04-30 05:00:02,821 INFO] Step 10100/50000; xent: 2.97; lr: 0.0000010;  29 docs/s;   7305 sec\n",
            "[2021-04-30 05:00:12,270 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.118.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:00:39,910 INFO] Step 10150/50000; xent: 3.06; lr: 0.0000010;  28 docs/s;   7342 sec\n",
            "[2021-04-30 05:01:15,027 INFO] Step 10200/50000; xent: 2.95; lr: 0.0000010;  30 docs/s;   7377 sec\n",
            "[2021-04-30 05:01:50,046 INFO] Step 10250/50000; xent: 3.07; lr: 0.0000010;  30 docs/s;   7412 sec\n",
            "[2021-04-30 05:02:25,311 INFO] Step 10300/50000; xent: 2.91; lr: 0.0000010;  29 docs/s;   7447 sec\n",
            "[2021-04-30 05:02:29,275 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.47.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:03:02,419 INFO] Step 10350/50000; xent: 2.99; lr: 0.0000010;  28 docs/s;   7484 sec\n",
            "[2021-04-30 05:03:37,555 INFO] Step 10400/50000; xent: 3.02; lr: 0.0000010;  29 docs/s;   7519 sec\n",
            "[2021-04-30 05:04:12,533 INFO] Step 10450/50000; xent: 3.11; lr: 0.0000010;  30 docs/s;   7554 sec\n",
            "[2021-04-30 05:04:46,542 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.8.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:04:49,467 INFO] Step 10500/50000; xent: 3.02; lr: 0.0000010;  28 docs/s;   7591 sec\n",
            "[2021-04-30 05:05:24,711 INFO] Step 10550/50000; xent: 2.99; lr: 0.0000010;  30 docs/s;   7626 sec\n",
            "[2021-04-30 05:05:59,913 INFO] Step 10600/50000; xent: 3.00; lr: 0.0000010;  30 docs/s;   7662 sec\n",
            "[2021-04-30 05:06:35,110 INFO] Step 10650/50000; xent: 3.06; lr: 0.0000010;  29 docs/s;   7697 sec\n",
            "[2021-04-30 05:07:04,137 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.11.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:07:12,461 INFO] Step 10700/50000; xent: 2.95; lr: 0.0000010;  28 docs/s;   7734 sec\n",
            "[2021-04-30 05:07:47,578 INFO] Step 10750/50000; xent: 2.91; lr: 0.0000010;  30 docs/s;   7769 sec\n",
            "[2021-04-30 05:08:22,753 INFO] Step 10800/50000; xent: 3.02; lr: 0.0000010;  30 docs/s;   7804 sec\n",
            "[2021-04-30 05:08:57,688 INFO] Step 10850/50000; xent: 2.94; lr: 0.0000010;  30 docs/s;   7839 sec\n",
            "[2021-04-30 05:09:21,242 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.66.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:09:34,715 INFO] Step 10900/50000; xent: 3.10; lr: 0.0000010;  28 docs/s;   7876 sec\n",
            "[2021-04-30 05:10:09,826 INFO] Step 10950/50000; xent: 3.04; lr: 0.0000010;  30 docs/s;   7912 sec\n",
            "[2021-04-30 05:10:44,971 INFO] Step 11000/50000; xent: 3.03; lr: 0.0000010;  30 docs/s;   7947 sec\n",
            "[2021-04-30 05:10:44,988 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_11000.pt\n",
            "[2021-04-30 05:11:36,407 INFO] Step 11050/50000; xent: 3.07; lr: 0.0000010;  20 docs/s;   7998 sec\n",
            "[2021-04-30 05:11:54,873 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.36.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:12:13,749 INFO] Step 11100/50000; xent: 2.99; lr: 0.0000009;  27 docs/s;   8035 sec\n",
            "[2021-04-30 05:12:48,974 INFO] Step 11150/50000; xent: 3.01; lr: 0.0000009;  30 docs/s;   8071 sec\n",
            "[2021-04-30 05:13:24,207 INFO] Step 11200/50000; xent: 2.97; lr: 0.0000009;  30 docs/s;   8106 sec\n",
            "[2021-04-30 05:13:59,472 INFO] Step 11250/50000; xent: 2.94; lr: 0.0000009;  29 docs/s;   8141 sec\n",
            "[2021-04-30 05:14:13,386 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.73.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:14:37,523 INFO] Step 11300/50000; xent: 3.11; lr: 0.0000009;  27 docs/s;   8179 sec\n",
            "[2021-04-30 05:15:12,508 INFO] Step 11350/50000; xent: 2.98; lr: 0.0000009;  30 docs/s;   8214 sec\n",
            "[2021-04-30 05:15:47,723 INFO] Step 11400/50000; xent: 3.03; lr: 0.0000009;  30 docs/s;   8249 sec\n",
            "[2021-04-30 05:16:23,085 INFO] Step 11450/50000; xent: 3.00; lr: 0.0000009;  29 docs/s;   8285 sec\n",
            "[2021-04-30 05:16:31,275 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.33.bert.pt, number of examples: 1997\n",
            "[2021-04-30 05:17:00,122 INFO] Step 11500/50000; xent: 3.05; lr: 0.0000009;  28 docs/s;   8322 sec\n",
            "[2021-04-30 05:17:35,381 INFO] Step 11550/50000; xent: 3.08; lr: 0.0000009;  29 docs/s;   8357 sec\n",
            "[2021-04-30 05:18:10,584 INFO] Step 11600/50000; xent: 3.03; lr: 0.0000009;  30 docs/s;   8392 sec\n",
            "[2021-04-30 05:18:45,841 INFO] Step 11650/50000; xent: 3.02; lr: 0.0000009;  30 docs/s;   8428 sec\n",
            "[2021-04-30 05:18:48,536 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.121.bert.pt, number of examples: 1999\n",
            "[2021-04-30 05:19:23,159 INFO] Step 11700/50000; xent: 3.08; lr: 0.0000009;  28 docs/s;   8465 sec\n",
            "[2021-04-30 05:19:58,401 INFO] Step 11750/50000; xent: 3.01; lr: 0.0000009;  29 docs/s;   8500 sec\n",
            "[2021-04-30 05:20:33,477 INFO] Step 11800/50000; xent: 3.04; lr: 0.0000009;  30 docs/s;   8535 sec\n",
            "[2021-04-30 05:21:07,279 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.136.bert.pt, number of examples: 1998\n",
            "[2021-04-30 05:21:12,337 INFO] Step 11850/50000; xent: 2.95; lr: 0.0000009;  27 docs/s;   8574 sec\n",
            "[2021-04-30 05:21:47,629 INFO] Step 11900/50000; xent: 3.01; lr: 0.0000009;  29 docs/s;   8609 sec\n",
            "[2021-04-30 05:22:22,811 INFO] Step 11950/50000; xent: 2.93; lr: 0.0000009;  30 docs/s;   8645 sec\n",
            "[2021-04-30 05:22:58,093 INFO] Step 12000/50000; xent: 3.07; lr: 0.0000009;  30 docs/s;   8680 sec\n",
            "[2021-04-30 05:22:58,110 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_12000.pt\n",
            "[2021-04-30 05:23:36,658 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.20.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:23:46,992 INFO] Step 12050/50000; xent: 2.96; lr: 0.0000009;  21 docs/s;   8729 sec\n",
            "[2021-04-30 05:24:22,034 INFO] Step 12100/50000; xent: 3.06; lr: 0.0000009;  29 docs/s;   8764 sec\n",
            "[2021-04-30 05:24:57,267 INFO] Step 12150/50000; xent: 3.05; lr: 0.0000009;  30 docs/s;   8799 sec\n",
            "[2021-04-30 05:25:32,461 INFO] Step 12200/50000; xent: 2.99; lr: 0.0000009;  30 docs/s;   8834 sec\n",
            "[2021-04-30 05:25:54,168 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.61.bert.pt, number of examples: 1999\n",
            "[2021-04-30 05:26:09,775 INFO] Step 12250/50000; xent: 3.01; lr: 0.0000009;  28 docs/s;   8871 sec\n",
            "[2021-04-30 05:26:45,045 INFO] Step 12300/50000; xent: 3.05; lr: 0.0000009;  29 docs/s;   8907 sec\n",
            "[2021-04-30 05:27:20,346 INFO] Step 12350/50000; xent: 3.07; lr: 0.0000009;  29 docs/s;   8942 sec\n",
            "[2021-04-30 05:27:55,478 INFO] Step 12400/50000; xent: 2.97; lr: 0.0000009;  30 docs/s;   8977 sec\n",
            "[2021-04-30 05:28:11,852 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.45.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:28:32,406 INFO] Step 12450/50000; xent: 2.95; lr: 0.0000009;  28 docs/s;   9014 sec\n",
            "[2021-04-30 05:29:07,642 INFO] Step 12500/50000; xent: 3.02; lr: 0.0000009;  30 docs/s;   9049 sec\n",
            "[2021-04-30 05:29:42,967 INFO] Step 12550/50000; xent: 2.98; lr: 0.0000009;  29 docs/s;   9085 sec\n",
            "[2021-04-30 05:30:18,266 INFO] Step 12600/50000; xent: 3.00; lr: 0.0000009;  29 docs/s;   9120 sec\n",
            "[2021-04-30 05:30:29,840 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.134.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:30:55,413 INFO] Step 12650/50000; xent: 2.98; lr: 0.0000009;  27 docs/s;   9157 sec\n",
            "[2021-04-30 05:31:30,677 INFO] Step 12700/50000; xent: 3.07; lr: 0.0000009;  29 docs/s;   9192 sec\n",
            "[2021-04-30 05:32:05,935 INFO] Step 12750/50000; xent: 2.98; lr: 0.0000009;  30 docs/s;   9228 sec\n",
            "[2021-04-30 05:32:40,715 INFO] Step 12800/50000; xent: 3.08; lr: 0.0000009;  31 docs/s;   9262 sec\n",
            "[2021-04-30 05:32:46,745 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.21.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:33:17,842 INFO] Step 12850/50000; xent: 3.07; lr: 0.0000009;  28 docs/s;   9300 sec\n",
            "[2021-04-30 05:33:53,106 INFO] Step 12900/50000; xent: 3.01; lr: 0.0000009;  29 docs/s;   9335 sec\n",
            "[2021-04-30 05:34:28,395 INFO] Step 12950/50000; xent: 3.00; lr: 0.0000009;  30 docs/s;   9370 sec\n",
            "[2021-04-30 05:35:05,008 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.43.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:35:05,826 INFO] Step 13000/50000; xent: 2.94; lr: 0.0000009;  27 docs/s;   9408 sec\n",
            "[2021-04-30 05:35:05,842 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_13000.pt\n",
            "[2021-04-30 05:35:51,431 INFO] Step 13050/50000; xent: 3.07; lr: 0.0000009;  23 docs/s;   9453 sec\n",
            "[2021-04-30 05:36:26,667 INFO] Step 13100/50000; xent: 3.03; lr: 0.0000009;  29 docs/s;   9488 sec\n",
            "[2021-04-30 05:37:01,970 INFO] Step 13150/50000; xent: 3.01; lr: 0.0000009;  30 docs/s;   9524 sec\n",
            "[2021-04-30 05:37:32,602 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.141.bert.pt, number of examples: 1998\n",
            "[2021-04-30 05:37:39,087 INFO] Step 13200/50000; xent: 2.97; lr: 0.0000009;  28 docs/s;   9561 sec\n",
            "[2021-04-30 05:38:14,305 INFO] Step 13250/50000; xent: 3.03; lr: 0.0000009;  29 docs/s;   9596 sec\n",
            "[2021-04-30 05:38:49,604 INFO] Step 13300/50000; xent: 3.04; lr: 0.0000009;  30 docs/s;   9631 sec\n",
            "[2021-04-30 05:39:24,842 INFO] Step 13350/50000; xent: 2.94; lr: 0.0000009;  30 docs/s;   9667 sec\n",
            "[2021-04-30 05:39:49,901 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.16.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:40:02,021 INFO] Step 13400/50000; xent: 3.01; lr: 0.0000009;  28 docs/s;   9704 sec\n",
            "[2021-04-30 05:40:37,296 INFO] Step 13450/50000; xent: 2.99; lr: 0.0000009;  29 docs/s;   9739 sec\n",
            "[2021-04-30 05:41:12,567 INFO] Step 13500/50000; xent: 3.01; lr: 0.0000009;  30 docs/s;   9774 sec\n",
            "[2021-04-30 05:41:47,789 INFO] Step 13550/50000; xent: 2.92; lr: 0.0000009;  29 docs/s;   9809 sec\n",
            "[2021-04-30 05:42:07,917 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.116.bert.pt, number of examples: 1997\n",
            "[2021-04-30 05:42:25,012 INFO] Step 13600/50000; xent: 3.09; lr: 0.0000009;  27 docs/s;   9847 sec\n",
            "[2021-04-30 05:43:00,344 INFO] Step 13650/50000; xent: 3.07; lr: 0.0000009;  29 docs/s;   9882 sec\n",
            "[2021-04-30 05:43:35,580 INFO] Step 13700/50000; xent: 2.98; lr: 0.0000009;  31 docs/s;   9917 sec\n",
            "[2021-04-30 05:44:10,774 INFO] Step 13750/50000; xent: 2.99; lr: 0.0000009;  29 docs/s;   9952 sec\n",
            "[2021-04-30 05:44:26,163 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.76.bert.pt, number of examples: 2001\n",
            "[2021-04-30 05:44:48,885 INFO] Step 13800/50000; xent: 3.02; lr: 0.0000009;  27 docs/s;   9991 sec\n",
            "[2021-04-30 05:45:24,232 INFO] Step 13850/50000; xent: 2.97; lr: 0.0000008;  29 docs/s;  10026 sec\n",
            "[2021-04-30 05:45:59,470 INFO] Step 13900/50000; xent: 2.92; lr: 0.0000008;  30 docs/s;  10061 sec\n",
            "[2021-04-30 05:46:34,766 INFO] Step 13950/50000; xent: 2.98; lr: 0.0000008;  29 docs/s;  10096 sec\n",
            "[2021-04-30 05:46:45,397 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.19.bert.pt, number of examples: 1999\n",
            "[2021-04-30 05:47:12,230 INFO] Step 14000/50000; xent: 3.00; lr: 0.0000008;  28 docs/s;  10134 sec\n",
            "[2021-04-30 05:47:12,247 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_14000.pt\n",
            "[2021-04-30 05:48:01,791 INFO] Step 14050/50000; xent: 3.01; lr: 0.0000008;  21 docs/s;  10183 sec\n",
            "[2021-04-30 05:48:37,248 INFO] Step 14100/50000; xent: 3.02; lr: 0.0000008;  29 docs/s;  10219 sec\n",
            "[2021-04-30 05:49:12,434 INFO] Step 14150/50000; xent: 3.05; lr: 0.0000008;  29 docs/s;  10254 sec\n",
            "[2021-04-30 05:49:17,417 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.107.bert.pt, number of examples: 1999\n",
            "[2021-04-30 05:49:49,741 INFO] Step 14200/50000; xent: 3.01; lr: 0.0000008;  28 docs/s;  10291 sec\n",
            "[2021-04-30 05:50:25,043 INFO] Step 14250/50000; xent: 3.03; lr: 0.0000008;  30 docs/s;  10327 sec\n",
            "[2021-04-30 05:51:00,259 INFO] Step 14300/50000; xent: 2.91; lr: 0.0000008;  30 docs/s;  10362 sec\n",
            "[2021-04-30 05:51:35,320 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.125.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:51:37,556 INFO] Step 14350/50000; xent: 3.07; lr: 0.0000008;  27 docs/s;  10399 sec\n",
            "[2021-04-30 05:52:12,631 INFO] Step 14400/50000; xent: 3.05; lr: 0.0000008;  30 docs/s;  10434 sec\n",
            "[2021-04-30 05:52:47,853 INFO] Step 14450/50000; xent: 2.95; lr: 0.0000008;  30 docs/s;  10470 sec\n",
            "[2021-04-30 05:53:23,123 INFO] Step 14500/50000; xent: 3.00; lr: 0.0000008;  30 docs/s;  10505 sec\n",
            "[2021-04-30 05:53:52,433 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.142.bert.pt, number of examples: 1996\n",
            "[2021-04-30 05:54:00,329 INFO] Step 14550/50000; xent: 3.01; lr: 0.0000008;  28 docs/s;  10542 sec\n",
            "[2021-04-30 05:54:35,499 INFO] Step 14600/50000; xent: 2.98; lr: 0.0000008;  30 docs/s;  10577 sec\n",
            "[2021-04-30 05:55:10,825 INFO] Step 14650/50000; xent: 3.04; lr: 0.0000008;  29 docs/s;  10613 sec\n",
            "[2021-04-30 05:55:45,910 INFO] Step 14700/50000; xent: 3.03; lr: 0.0000008;  30 docs/s;  10648 sec\n",
            "[2021-04-30 05:56:09,003 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.41.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:56:23,221 INFO] Step 14750/50000; xent: 3.03; lr: 0.0000008;  28 docs/s;  10685 sec\n",
            "[2021-04-30 05:56:58,508 INFO] Step 14800/50000; xent: 3.00; lr: 0.0000008;  30 docs/s;  10720 sec\n",
            "[2021-04-30 05:57:33,512 INFO] Step 14850/50000; xent: 2.97; lr: 0.0000008;  29 docs/s;  10755 sec\n",
            "[2021-04-30 05:58:08,873 INFO] Step 14900/50000; xent: 2.94; lr: 0.0000008;  30 docs/s;  10791 sec\n",
            "[2021-04-30 05:58:26,566 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.44.bert.pt, number of examples: 2000\n",
            "[2021-04-30 05:58:46,412 INFO] Step 14950/50000; xent: 2.98; lr: 0.0000008;  28 docs/s;  10828 sec\n",
            "[2021-04-30 05:59:21,507 INFO] Step 15000/50000; xent: 2.95; lr: 0.0000008;  29 docs/s;  10863 sec\n",
            "[2021-04-30 05:59:21,524 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_15000.pt\n",
            "[2021-04-30 06:00:06,488 INFO] Step 15050/50000; xent: 3.04; lr: 0.0000008;  23 docs/s;  10908 sec\n",
            "[2021-04-30 06:00:41,518 INFO] Step 15100/50000; xent: 3.01; lr: 0.0000008;  30 docs/s;  10943 sec\n",
            "[2021-04-30 06:00:53,311 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.95.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:01:18,755 INFO] Step 15150/50000; xent: 2.96; lr: 0.0000008;  28 docs/s;  10980 sec\n",
            "[2021-04-30 06:01:53,907 INFO] Step 15200/50000; xent: 2.98; lr: 0.0000008;  29 docs/s;  11016 sec\n",
            "[2021-04-30 06:02:29,135 INFO] Step 15250/50000; xent: 3.01; lr: 0.0000008;  30 docs/s;  11051 sec\n",
            "[2021-04-30 06:03:04,089 INFO] Step 15300/50000; xent: 3.06; lr: 0.0000008;  30 docs/s;  11086 sec\n",
            "[2021-04-30 06:03:10,333 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.79.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:03:41,421 INFO] Step 15350/50000; xent: 3.01; lr: 0.0000008;  28 docs/s;  11123 sec\n",
            "[2021-04-30 06:04:16,607 INFO] Step 15400/50000; xent: 2.96; lr: 0.0000008;  30 docs/s;  11158 sec\n",
            "[2021-04-30 06:04:51,758 INFO] Step 15450/50000; xent: 3.03; lr: 0.0000008;  29 docs/s;  11193 sec\n",
            "[2021-04-30 06:05:27,599 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.110.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:05:29,124 INFO] Step 15500/50000; xent: 3.02; lr: 0.0000008;  28 docs/s;  11231 sec\n",
            "[2021-04-30 06:06:04,305 INFO] Step 15550/50000; xent: 3.00; lr: 0.0000008;  31 docs/s;  11266 sec\n",
            "[2021-04-30 06:06:39,587 INFO] Step 15600/50000; xent: 2.98; lr: 0.0000008;  29 docs/s;  11301 sec\n",
            "[2021-04-30 06:07:14,662 INFO] Step 15650/50000; xent: 3.00; lr: 0.0000008;  29 docs/s;  11336 sec\n",
            "[2021-04-30 06:07:45,336 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.23.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:07:51,785 INFO] Step 15700/50000; xent: 3.02; lr: 0.0000008;  28 docs/s;  11373 sec\n",
            "[2021-04-30 06:08:26,763 INFO] Step 15750/50000; xent: 3.07; lr: 0.0000008;  30 docs/s;  11408 sec\n",
            "[2021-04-30 06:09:01,987 INFO] Step 15800/50000; xent: 3.01; lr: 0.0000008;  29 docs/s;  11444 sec\n",
            "[2021-04-30 06:09:37,156 INFO] Step 15850/50000; xent: 2.96; lr: 0.0000008;  30 docs/s;  11479 sec\n",
            "[2021-04-30 06:10:02,283 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.53.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:10:14,240 INFO] Step 15900/50000; xent: 2.96; lr: 0.0000008;  29 docs/s;  11516 sec\n",
            "[2021-04-30 06:10:49,472 INFO] Step 15950/50000; xent: 3.05; lr: 0.0000008;  29 docs/s;  11551 sec\n",
            "[2021-04-30 06:11:24,649 INFO] Step 16000/50000; xent: 2.99; lr: 0.0000008;  30 docs/s;  11586 sec\n",
            "[2021-04-30 06:11:24,667 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_16000.pt\n",
            "[2021-04-30 06:12:15,719 INFO] Step 16050/50000; xent: 3.06; lr: 0.0000008;  20 docs/s;  11637 sec\n",
            "[2021-04-30 06:12:34,901 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.27.bert.pt, number of examples: 1999\n",
            "[2021-04-30 06:12:52,614 INFO] Step 16100/50000; xent: 2.99; lr: 0.0000008;  28 docs/s;  11674 sec\n",
            "[2021-04-30 06:13:27,796 INFO] Step 16150/50000; xent: 2.99; lr: 0.0000008;  31 docs/s;  11709 sec\n",
            "[2021-04-30 06:14:03,015 INFO] Step 16200/50000; xent: 3.04; lr: 0.0000008;  29 docs/s;  11745 sec\n",
            "[2021-04-30 06:14:38,032 INFO] Step 16250/50000; xent: 2.99; lr: 0.0000008;  29 docs/s;  11780 sec\n",
            "[2021-04-30 06:14:51,925 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.14.bert.pt, number of examples: 1998\n",
            "[2021-04-30 06:15:15,224 INFO] Step 16300/50000; xent: 2.97; lr: 0.0000008;  28 docs/s;  11817 sec\n",
            "[2021-04-30 06:15:50,301 INFO] Step 16350/50000; xent: 3.08; lr: 0.0000008;  29 docs/s;  11852 sec\n",
            "[2021-04-30 06:16:25,509 INFO] Step 16400/50000; xent: 2.96; lr: 0.0000008;  30 docs/s;  11887 sec\n",
            "[2021-04-30 06:17:00,519 INFO] Step 16450/50000; xent: 2.96; lr: 0.0000008;  30 docs/s;  11922 sec\n",
            "[2021-04-30 06:17:09,854 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.82.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:17:38,181 INFO] Step 16500/50000; xent: 3.00; lr: 0.0000008;  27 docs/s;  11960 sec\n",
            "[2021-04-30 06:18:13,307 INFO] Step 16550/50000; xent: 2.94; lr: 0.0000008;  30 docs/s;  11995 sec\n",
            "[2021-04-30 06:18:48,332 INFO] Step 16600/50000; xent: 3.07; lr: 0.0000008;  30 docs/s;  12030 sec\n",
            "[2021-04-30 06:19:23,522 INFO] Step 16650/50000; xent: 3.05; lr: 0.0000008;  29 docs/s;  12065 sec\n",
            "[2021-04-30 06:19:27,349 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.18.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:20:00,546 INFO] Step 16700/50000; xent: 3.02; lr: 0.0000008;  29 docs/s;  12102 sec\n",
            "[2021-04-30 06:20:35,802 INFO] Step 16750/50000; xent: 2.98; lr: 0.0000008;  29 docs/s;  12137 sec\n",
            "[2021-04-30 06:21:10,952 INFO] Step 16800/50000; xent: 3.01; lr: 0.0000008;  29 docs/s;  12173 sec\n",
            "[2021-04-30 06:21:44,325 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.87.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:21:47,958 INFO] Step 16850/50000; xent: 2.98; lr: 0.0000008;  29 docs/s;  12210 sec\n",
            "[2021-04-30 06:22:23,176 INFO] Step 16900/50000; xent: 2.94; lr: 0.0000008;  29 docs/s;  12245 sec\n",
            "[2021-04-30 06:22:58,283 INFO] Step 16950/50000; xent: 3.00; lr: 0.0000008;  30 docs/s;  12280 sec\n",
            "[2021-04-30 06:23:33,456 INFO] Step 17000/50000; xent: 2.98; lr: 0.0000008;  30 docs/s;  12315 sec\n",
            "[2021-04-30 06:23:33,474 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_17000.pt\n",
            "[2021-04-30 06:24:10,553 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.2.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:24:19,970 INFO] Step 17050/50000; xent: 3.02; lr: 0.0000008;  22 docs/s;  12362 sec\n",
            "[2021-04-30 06:24:55,211 INFO] Step 17100/50000; xent: 2.99; lr: 0.0000008;  30 docs/s;  12397 sec\n",
            "[2021-04-30 06:25:30,361 INFO] Step 17150/50000; xent: 3.04; lr: 0.0000008;  30 docs/s;  12432 sec\n",
            "[2021-04-30 06:26:05,475 INFO] Step 17200/50000; xent: 3.01; lr: 0.0000008;  29 docs/s;  12467 sec\n",
            "[2021-04-30 06:26:28,434 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.69.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:26:42,599 INFO] Step 17250/50000; xent: 2.99; lr: 0.0000008;  27 docs/s;  12504 sec\n",
            "[2021-04-30 06:27:17,869 INFO] Step 17300/50000; xent: 3.05; lr: 0.0000008;  30 docs/s;  12540 sec\n",
            "[2021-04-30 06:27:52,990 INFO] Step 17350/50000; xent: 3.02; lr: 0.0000008;  30 docs/s;  12575 sec\n",
            "[2021-04-30 06:28:28,250 INFO] Step 17400/50000; xent: 2.98; lr: 0.0000008;  30 docs/s;  12610 sec\n",
            "[2021-04-30 06:28:45,348 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.25.bert.pt, number of examples: 1999\n",
            "[2021-04-30 06:29:05,112 INFO] Step 17450/50000; xent: 2.95; lr: 0.0000008;  29 docs/s;  12647 sec\n",
            "[2021-04-30 06:29:40,235 INFO] Step 17500/50000; xent: 3.04; lr: 0.0000008;  30 docs/s;  12682 sec\n",
            "[2021-04-30 06:30:15,288 INFO] Step 17550/50000; xent: 2.95; lr: 0.0000008;  29 docs/s;  12717 sec\n",
            "[2021-04-30 06:30:50,550 INFO] Step 17600/50000; xent: 3.05; lr: 0.0000008;  29 docs/s;  12752 sec\n",
            "[2021-04-30 06:31:02,648 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.22.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:31:27,198 INFO] Step 17650/50000; xent: 3.01; lr: 0.0000008;  27 docs/s;  12789 sec\n",
            "[2021-04-30 06:32:02,361 INFO] Step 17700/50000; xent: 3.06; lr: 0.0000008;  30 docs/s;  12824 sec\n",
            "[2021-04-30 06:32:37,532 INFO] Step 17750/50000; xent: 2.99; lr: 0.0000008;  29 docs/s;  12859 sec\n",
            "[2021-04-30 06:33:12,831 INFO] Step 17800/50000; xent: 3.01; lr: 0.0000007;  30 docs/s;  12895 sec\n",
            "[2021-04-30 06:33:19,999 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.127.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:33:50,421 INFO] Step 17850/50000; xent: 2.97; lr: 0.0000007;  27 docs/s;  12932 sec\n",
            "[2021-04-30 06:34:25,566 INFO] Step 17900/50000; xent: 3.00; lr: 0.0000007;  30 docs/s;  12967 sec\n",
            "[2021-04-30 06:35:00,462 INFO] Step 17950/50000; xent: 2.98; lr: 0.0000007;  29 docs/s;  13002 sec\n",
            "[2021-04-30 06:35:36,547 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.57.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:35:37,365 INFO] Step 18000/50000; xent: 2.91; lr: 0.0000007;  28 docs/s;  13039 sec\n",
            "[2021-04-30 06:35:37,381 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_18000.pt\n",
            "[2021-04-30 06:36:22,677 INFO] Step 18050/50000; xent: 2.99; lr: 0.0000007;  24 docs/s;  13084 sec\n",
            "[2021-04-30 06:36:57,903 INFO] Step 18100/50000; xent: 2.96; lr: 0.0000007;  29 docs/s;  13120 sec\n",
            "[2021-04-30 06:37:33,214 INFO] Step 18150/50000; xent: 2.95; lr: 0.0000007;  30 docs/s;  13155 sec\n",
            "[2021-04-30 06:38:03,732 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.99.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:38:10,162 INFO] Step 18200/50000; xent: 3.03; lr: 0.0000007;  28 docs/s;  13192 sec\n",
            "[2021-04-30 06:38:45,407 INFO] Step 18250/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  13227 sec\n",
            "[2021-04-30 06:39:20,586 INFO] Step 18300/50000; xent: 2.96; lr: 0.0000007;  30 docs/s;  13262 sec\n",
            "[2021-04-30 06:39:55,860 INFO] Step 18350/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;  13298 sec\n",
            "[2021-04-30 06:40:20,824 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.65.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:40:32,941 INFO] Step 18400/50000; xent: 3.04; lr: 0.0000007;  28 docs/s;  13335 sec\n",
            "[2021-04-30 06:41:08,184 INFO] Step 18450/50000; xent: 3.01; lr: 0.0000007;  29 docs/s;  13370 sec\n",
            "[2021-04-30 06:41:43,444 INFO] Step 18500/50000; xent: 2.99; lr: 0.0000007;  30 docs/s;  13405 sec\n",
            "[2021-04-30 06:42:18,598 INFO] Step 18550/50000; xent: 2.96; lr: 0.0000007;  31 docs/s;  13440 sec\n",
            "[2021-04-30 06:42:37,079 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.119.bert.pt, number of examples: 1998\n",
            "[2021-04-30 06:42:55,478 INFO] Step 18600/50000; xent: 3.05; lr: 0.0000007;  28 docs/s;  13477 sec\n",
            "[2021-04-30 06:43:30,735 INFO] Step 18650/50000; xent: 3.10; lr: 0.0000007;  30 docs/s;  13512 sec\n",
            "[2021-04-30 06:44:05,839 INFO] Step 18700/50000; xent: 3.04; lr: 0.0000007;  29 docs/s;  13548 sec\n",
            "[2021-04-30 06:44:41,161 INFO] Step 18750/50000; xent: 2.99; lr: 0.0000007;  30 docs/s;  13583 sec\n",
            "[2021-04-30 06:44:54,178 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.70.bert.pt, number of examples: 1999\n",
            "[2021-04-30 06:45:18,210 INFO] Step 18800/50000; xent: 3.04; lr: 0.0000007;  28 docs/s;  13620 sec\n",
            "[2021-04-30 06:45:53,400 INFO] Step 18850/50000; xent: 2.94; lr: 0.0000007;  30 docs/s;  13655 sec\n",
            "[2021-04-30 06:46:28,341 INFO] Step 18900/50000; xent: 2.89; lr: 0.0000007;  30 docs/s;  13690 sec\n",
            "[2021-04-30 06:47:03,608 INFO] Step 18950/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;  13725 sec\n",
            "[2021-04-30 06:47:12,034 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.54.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:47:41,019 INFO] Step 19000/50000; xent: 2.98; lr: 0.0000007;  28 docs/s;  13763 sec\n",
            "[2021-04-30 06:47:41,036 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_19000.pt\n",
            "[2021-04-30 06:48:31,770 INFO] Step 19050/50000; xent: 2.99; lr: 0.0000007;  21 docs/s;  13813 sec\n",
            "[2021-04-30 06:49:06,753 INFO] Step 19100/50000; xent: 3.01; lr: 0.0000007;  29 docs/s;  13848 sec\n",
            "[2021-04-30 06:49:41,873 INFO] Step 19150/50000; xent: 2.99; lr: 0.0000007;  30 docs/s;  13884 sec\n",
            "[2021-04-30 06:49:44,654 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.103.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:50:19,259 INFO] Step 19200/50000; xent: 2.94; lr: 0.0000007;  28 docs/s;  13921 sec\n",
            "[2021-04-30 06:50:54,323 INFO] Step 19250/50000; xent: 3.01; lr: 0.0000007;  29 docs/s;  13956 sec\n",
            "[2021-04-30 06:51:29,563 INFO] Step 19300/50000; xent: 2.96; lr: 0.0000007;  30 docs/s;  13991 sec\n",
            "[2021-04-30 06:52:01,770 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.1.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:52:06,848 INFO] Step 19350/50000; xent: 3.11; lr: 0.0000007;  28 docs/s;  14029 sec\n",
            "[2021-04-30 06:52:42,072 INFO] Step 19400/50000; xent: 3.02; lr: 0.0000007;  30 docs/s;  14064 sec\n",
            "[2021-04-30 06:53:17,030 INFO] Step 19450/50000; xent: 3.02; lr: 0.0000007;  30 docs/s;  14099 sec\n",
            "[2021-04-30 06:53:52,226 INFO] Step 19500/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;  14134 sec\n",
            "[2021-04-30 06:54:18,422 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.3.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:54:29,081 INFO] Step 19550/50000; xent: 2.94; lr: 0.0000007;  28 docs/s;  14171 sec\n",
            "[2021-04-30 06:55:03,968 INFO] Step 19600/50000; xent: 3.05; lr: 0.0000007;  30 docs/s;  14206 sec\n",
            "[2021-04-30 06:55:39,008 INFO] Step 19650/50000; xent: 3.03; lr: 0.0000007;  30 docs/s;  14241 sec\n",
            "[2021-04-30 06:56:14,109 INFO] Step 19700/50000; xent: 3.00; lr: 0.0000007;  29 docs/s;  14276 sec\n",
            "[2021-04-30 06:56:36,086 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.10.bert.pt, number of examples: 2000\n",
            "[2021-04-30 06:56:50,941 INFO] Step 19750/50000; xent: 3.00; lr: 0.0000007;  28 docs/s;  14313 sec\n",
            "[2021-04-30 06:57:26,018 INFO] Step 19800/50000; xent: 3.10; lr: 0.0000007;  29 docs/s;  14348 sec\n",
            "[2021-04-30 06:58:01,057 INFO] Step 19850/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  14383 sec\n",
            "[2021-04-30 06:58:35,917 INFO] Step 19900/50000; xent: 3.04; lr: 0.0000007;  29 docs/s;  14418 sec\n",
            "[2021-04-30 06:58:52,453 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.78.bert.pt, number of examples: 2001\n",
            "[2021-04-30 06:59:12,924 INFO] Step 19950/50000; xent: 3.00; lr: 0.0000007;  28 docs/s;  14455 sec\n",
            "[2021-04-30 06:59:47,767 INFO] Step 20000/50000; xent: 2.98; lr: 0.0000007;  29 docs/s;  14489 sec\n",
            "[2021-04-30 06:59:47,784 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_20000.pt\n",
            "[2021-04-30 07:00:38,009 INFO] Step 20050/50000; xent: 3.00; lr: 0.0000007;  21 docs/s;  14540 sec\n",
            "[2021-04-30 07:01:13,155 INFO] Step 20100/50000; xent: 2.96; lr: 0.0000007;  30 docs/s;  14575 sec\n",
            "[2021-04-30 07:01:24,990 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.112.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:01:50,147 INFO] Step 20150/50000; xent: 3.01; lr: 0.0000007;  28 docs/s;  14612 sec\n",
            "[2021-04-30 07:02:25,260 INFO] Step 20200/50000; xent: 3.02; lr: 0.0000007;  29 docs/s;  14647 sec\n",
            "[2021-04-30 07:03:00,324 INFO] Step 20250/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  14682 sec\n",
            "[2021-04-30 07:03:35,192 INFO] Step 20300/50000; xent: 2.99; lr: 0.0000007;  29 docs/s;  14717 sec\n",
            "[2021-04-30 07:03:41,913 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.102.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:04:12,212 INFO] Step 20350/50000; xent: 2.97; lr: 0.0000007;  29 docs/s;  14754 sec\n",
            "[2021-04-30 07:04:47,215 INFO] Step 20400/50000; xent: 2.96; lr: 0.0000007;  30 docs/s;  14789 sec\n",
            "[2021-04-30 07:05:22,292 INFO] Step 20450/50000; xent: 2.96; lr: 0.0000007;  29 docs/s;  14824 sec\n",
            "[2021-04-30 07:05:58,334 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.40.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:05:59,146 INFO] Step 20500/50000; xent: 3.00; lr: 0.0000007;  28 docs/s;  14861 sec\n",
            "[2021-04-30 07:06:34,203 INFO] Step 20550/50000; xent: 3.06; lr: 0.0000007;  30 docs/s;  14896 sec\n",
            "[2021-04-30 07:07:09,052 INFO] Step 20600/50000; xent: 3.00; lr: 0.0000007;  29 docs/s;  14931 sec\n",
            "[2021-04-30 07:07:44,144 INFO] Step 20650/50000; xent: 2.99; lr: 0.0000007;  30 docs/s;  14966 sec\n",
            "[2021-04-30 07:08:14,814 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.122.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:08:21,233 INFO] Step 20700/50000; xent: 2.99; lr: 0.0000007;  28 docs/s;  15003 sec\n",
            "[2021-04-30 07:08:56,379 INFO] Step 20750/50000; xent: 2.99; lr: 0.0000007;  29 docs/s;  15038 sec\n",
            "[2021-04-30 07:09:31,469 INFO] Step 20800/50000; xent: 2.93; lr: 0.0000007;  30 docs/s;  15073 sec\n",
            "[2021-04-30 07:10:06,570 INFO] Step 20850/50000; xent: 2.99; lr: 0.0000007;  30 docs/s;  15108 sec\n",
            "[2021-04-30 07:10:32,321 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.133.bert.pt, number of examples: 1999\n",
            "[2021-04-30 07:10:43,701 INFO] Step 20900/50000; xent: 2.91; lr: 0.0000007;  28 docs/s;  15145 sec\n",
            "[2021-04-30 07:11:18,633 INFO] Step 20950/50000; xent: 3.05; lr: 0.0000007;  29 docs/s;  15180 sec\n",
            "[2021-04-30 07:11:53,775 INFO] Step 21000/50000; xent: 2.98; lr: 0.0000007;  29 docs/s;  15215 sec\n",
            "[2021-04-30 07:11:53,791 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_21000.pt\n",
            "[2021-04-30 07:12:38,473 INFO] Step 21050/50000; xent: 3.06; lr: 0.0000007;  23 docs/s;  15260 sec\n",
            "[2021-04-30 07:12:59,143 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.67.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:13:15,408 INFO] Step 21100/50000; xent: 2.94; lr: 0.0000007;  28 docs/s;  15297 sec\n",
            "[2021-04-30 07:13:50,550 INFO] Step 21150/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;  15332 sec\n",
            "[2021-04-30 07:14:25,357 INFO] Step 21200/50000; xent: 2.94; lr: 0.0000007;  30 docs/s;  15367 sec\n",
            "[2021-04-30 07:15:00,336 INFO] Step 21250/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  15402 sec\n",
            "[2021-04-30 07:15:16,218 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.108.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:15:37,365 INFO] Step 21300/50000; xent: 2.97; lr: 0.0000007;  28 docs/s;  15439 sec\n",
            "[2021-04-30 07:16:12,411 INFO] Step 21350/50000; xent: 3.01; lr: 0.0000007;  30 docs/s;  15474 sec\n",
            "[2021-04-30 07:16:47,234 INFO] Step 21400/50000; xent: 2.96; lr: 0.0000007;  29 docs/s;  15509 sec\n",
            "[2021-04-30 07:17:22,245 INFO] Step 21450/50000; xent: 2.96; lr: 0.0000007;  29 docs/s;  15544 sec\n",
            "[2021-04-30 07:17:33,218 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.60.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:17:59,439 INFO] Step 21500/50000; xent: 2.93; lr: 0.0000007;  29 docs/s;  15581 sec\n",
            "[2021-04-30 07:18:34,567 INFO] Step 21550/50000; xent: 2.95; lr: 0.0000007;  29 docs/s;  15616 sec\n",
            "[2021-04-30 07:19:09,692 INFO] Step 21600/50000; xent: 2.98; lr: 0.0000007;  29 docs/s;  15651 sec\n",
            "[2021-04-30 07:19:44,888 INFO] Step 21650/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  15687 sec\n",
            "[2021-04-30 07:19:50,159 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.29.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:20:21,898 INFO] Step 21700/50000; xent: 2.96; lr: 0.0000007;  28 docs/s;  15724 sec\n",
            "[2021-04-30 07:20:56,994 INFO] Step 21750/50000; xent: 3.00; lr: 0.0000007;  30 docs/s;  15759 sec\n",
            "[2021-04-30 07:21:32,171 INFO] Step 21800/50000; xent: 2.95; lr: 0.0000007;  30 docs/s;  15794 sec\n",
            "[2021-04-30 07:22:07,858 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.90.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:22:09,396 INFO] Step 21850/50000; xent: 3.05; lr: 0.0000007;  27 docs/s;  15831 sec\n",
            "[2021-04-30 07:22:44,604 INFO] Step 21900/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;  15866 sec\n",
            "[2021-04-30 07:23:19,785 INFO] Step 21950/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  15901 sec\n",
            "[2021-04-30 07:23:54,929 INFO] Step 22000/50000; xent: 3.00; lr: 0.0000007;  30 docs/s;  15937 sec\n",
            "[2021-04-30 07:23:54,946 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_22000.pt\n",
            "[2021-04-30 07:24:34,085 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.5.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:24:41,435 INFO] Step 22050/50000; xent: 2.97; lr: 0.0000007;  22 docs/s;  15983 sec\n",
            "[2021-04-30 07:25:16,748 INFO] Step 22100/50000; xent: 3.06; lr: 0.0000007;  30 docs/s;  16018 sec\n",
            "[2021-04-30 07:25:51,966 INFO] Step 22150/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  16054 sec\n",
            "[2021-04-30 07:26:27,121 INFO] Step 22200/50000; xent: 3.01; lr: 0.0000007;  29 docs/s;  16089 sec\n",
            "[2021-04-30 07:26:52,188 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.140.bert.pt, number of examples: 1999\n",
            "[2021-04-30 07:27:04,266 INFO] Step 22250/50000; xent: 2.91; lr: 0.0000007;  28 docs/s;  16126 sec\n",
            "[2021-04-30 07:27:39,295 INFO] Step 22300/50000; xent: 2.99; lr: 0.0000007;  29 docs/s;  16161 sec\n",
            "[2021-04-30 07:28:14,541 INFO] Step 22350/50000; xent: 3.03; lr: 0.0000007;  29 docs/s;  16196 sec\n",
            "[2021-04-30 07:28:49,680 INFO] Step 22400/50000; xent: 3.08; lr: 0.0000007;  30 docs/s;  16231 sec\n",
            "[2021-04-30 07:29:09,782 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.17.bert.pt, number of examples: 1999\n",
            "[2021-04-30 07:29:26,673 INFO] Step 22450/50000; xent: 2.97; lr: 0.0000007;  28 docs/s;  16268 sec\n",
            "[2021-04-30 07:30:01,804 INFO] Step 22500/50000; xent: 2.97; lr: 0.0000007;  31 docs/s;  16303 sec\n",
            "[2021-04-30 07:30:36,997 INFO] Step 22550/50000; xent: 3.06; lr: 0.0000007;  29 docs/s;  16339 sec\n",
            "[2021-04-30 07:31:12,002 INFO] Step 22600/50000; xent: 3.11; lr: 0.0000007;  29 docs/s;  16374 sec\n",
            "[2021-04-30 07:31:26,562 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.31.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:31:49,217 INFO] Step 22650/50000; xent: 3.02; lr: 0.0000007;  28 docs/s;  16411 sec\n",
            "[2021-04-30 07:32:24,376 INFO] Step 22700/50000; xent: 2.95; lr: 0.0000007;  31 docs/s;  16446 sec\n",
            "[2021-04-30 07:32:59,603 INFO] Step 22750/50000; xent: 2.98; lr: 0.0000007;  30 docs/s;  16481 sec\n",
            "[2021-04-30 07:33:34,762 INFO] Step 22800/50000; xent: 3.01; lr: 0.0000007;  29 docs/s;  16516 sec\n",
            "[2021-04-30 07:33:43,288 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.4.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:34:11,706 INFO] Step 22850/50000; xent: 2.97; lr: 0.0000007;  28 docs/s;  16553 sec\n",
            "[2021-04-30 07:34:46,953 INFO] Step 22900/50000; xent: 3.00; lr: 0.0000007;  30 docs/s;  16589 sec\n",
            "[2021-04-30 07:35:22,231 INFO] Step 22950/50000; xent: 3.01; lr: 0.0000007;  30 docs/s;  16624 sec\n",
            "[2021-04-30 07:35:57,642 INFO] Step 23000/50000; xent: 2.92; lr: 0.0000007;  29 docs/s;  16659 sec\n",
            "[2021-04-30 07:35:57,645 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_23000.pt\n",
            "[2021-04-30 07:36:12,847 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.42.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:36:46,869 INFO] Step 23050/50000; xent: 3.02; lr: 0.0000007;  21 docs/s;  16709 sec\n",
            "[2021-04-30 07:37:22,122 INFO] Step 23100/50000; xent: 2.95; lr: 0.0000007;  30 docs/s;  16744 sec\n",
            "[2021-04-30 07:37:57,416 INFO] Step 23150/50000; xent: 2.94; lr: 0.0000007;  29 docs/s;  16779 sec\n",
            "[2021-04-30 07:38:36,302 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.75.bert.pt, number of examples: 1999\n",
            "[2021-04-30 07:38:39,239 INFO] Step 23200/50000; xent: 3.01; lr: 0.0000007;  25 docs/s;  16821 sec\n",
            "[2021-04-30 07:39:14,470 INFO] Step 23250/50000; xent: 2.93; lr: 0.0000007;  30 docs/s;  16856 sec\n",
            "[2021-04-30 07:39:49,736 INFO] Step 23300/50000; xent: 3.02; lr: 0.0000007;  29 docs/s;  16891 sec\n",
            "[2021-04-30 07:40:24,815 INFO] Step 23350/50000; xent: 2.98; lr: 0.0000007;  29 docs/s;  16927 sec\n",
            "[2021-04-30 07:40:54,173 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.129.bert.pt, number of examples: 1999\n",
            "[2021-04-30 07:41:02,011 INFO] Step 23400/50000; xent: 2.98; lr: 0.0000007;  28 docs/s;  16964 sec\n",
            "[2021-04-30 07:41:37,369 INFO] Step 23450/50000; xent: 2.97; lr: 0.0000007;  30 docs/s;  16999 sec\n",
            "[2021-04-30 07:42:12,621 INFO] Step 23500/50000; xent: 3.02; lr: 0.0000007;  30 docs/s;  17034 sec\n",
            "[2021-04-30 07:42:47,707 INFO] Step 23550/50000; xent: 3.08; lr: 0.0000007;  29 docs/s;  17069 sec\n",
            "[2021-04-30 07:43:12,065 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.117.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:43:24,892 INFO] Step 23600/50000; xent: 2.89; lr: 0.0000007;  28 docs/s;  17107 sec\n",
            "[2021-04-30 07:44:00,223 INFO] Step 23650/50000; xent: 2.95; lr: 0.0000007;  29 docs/s;  17142 sec\n",
            "[2021-04-30 07:44:35,463 INFO] Step 23700/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;  17177 sec\n",
            "[2021-04-30 07:45:10,702 INFO] Step 23750/50000; xent: 3.01; lr: 0.0000006;  29 docs/s;  17212 sec\n",
            "[2021-04-30 07:45:29,391 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.115.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:45:47,856 INFO] Step 23800/50000; xent: 2.99; lr: 0.0000006;  28 docs/s;  17250 sec\n",
            "[2021-04-30 07:46:23,068 INFO] Step 23850/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  17285 sec\n",
            "[2021-04-30 07:46:58,366 INFO] Step 23900/50000; xent: 3.01; lr: 0.0000006;  29 docs/s;  17320 sec\n",
            "[2021-04-30 07:47:33,466 INFO] Step 23950/50000; xent: 3.01; lr: 0.0000006;  29 docs/s;  17355 sec\n",
            "[2021-04-30 07:47:47,779 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.48.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:48:10,508 INFO] Step 24000/50000; xent: 3.00; lr: 0.0000006;  28 docs/s;  17392 sec\n",
            "[2021-04-30 07:48:10,525 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_24000.pt\n",
            "[2021-04-30 07:48:55,565 INFO] Step 24050/50000; xent: 2.94; lr: 0.0000006;  23 docs/s;  17437 sec\n",
            "[2021-04-30 07:49:30,847 INFO] Step 24100/50000; xent: 3.06; lr: 0.0000006;  29 docs/s;  17473 sec\n",
            "[2021-04-30 07:50:06,035 INFO] Step 24150/50000; xent: 3.03; lr: 0.0000006;  30 docs/s;  17508 sec\n",
            "[2021-04-30 07:50:16,035 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.132.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:50:43,591 INFO] Step 24200/50000; xent: 3.03; lr: 0.0000006;  27 docs/s;  17545 sec\n",
            "[2021-04-30 07:51:18,801 INFO] Step 24250/50000; xent: 2.91; lr: 0.0000006;  30 docs/s;  17580 sec\n",
            "[2021-04-30 07:51:53,990 INFO] Step 24300/50000; xent: 3.04; lr: 0.0000006;  30 docs/s;  17616 sec\n",
            "[2021-04-30 07:52:29,054 INFO] Step 24350/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;  17651 sec\n",
            "[2021-04-30 07:52:33,966 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.120.bert.pt, number of examples: 2000\n",
            "[2021-04-30 07:53:06,538 INFO] Step 24400/50000; xent: 2.96; lr: 0.0000006;  28 docs/s;  17688 sec\n",
            "[2021-04-30 07:53:41,856 INFO] Step 24450/50000; xent: 2.92; lr: 0.0000006;  29 docs/s;  17724 sec\n",
            "[2021-04-30 07:54:16,839 INFO] Step 24500/50000; xent: 2.95; lr: 0.0000006;  30 docs/s;  17759 sec\n",
            "[2021-04-30 07:54:51,409 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.30.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:54:54,353 INFO] Step 24550/50000; xent: 3.02; lr: 0.0000006;  27 docs/s;  17796 sec\n",
            "[2021-04-30 07:55:29,514 INFO] Step 24600/50000; xent: 2.99; lr: 0.0000006;  29 docs/s;  17831 sec\n",
            "[2021-04-30 07:56:04,775 INFO] Step 24650/50000; xent: 3.05; lr: 0.0000006;  30 docs/s;  17866 sec\n",
            "[2021-04-30 07:56:40,001 INFO] Step 24700/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  17902 sec\n",
            "[2021-04-30 07:57:10,169 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.124.bert.pt, number of examples: 2001\n",
            "[2021-04-30 07:57:18,056 INFO] Step 24750/50000; xent: 3.01; lr: 0.0000006;  27 docs/s;  17940 sec\n",
            "[2021-04-30 07:57:53,365 INFO] Step 24800/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  17975 sec\n",
            "[2021-04-30 07:58:28,651 INFO] Step 24850/50000; xent: 2.97; lr: 0.0000006;  30 docs/s;  18010 sec\n",
            "[2021-04-30 07:59:03,811 INFO] Step 24900/50000; xent: 3.06; lr: 0.0000006;  29 docs/s;  18046 sec\n",
            "[2021-04-30 07:59:27,998 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.101.bert.pt, number of examples: 1998\n",
            "[2021-04-30 07:59:40,817 INFO] Step 24950/50000; xent: 3.02; lr: 0.0000006;  28 docs/s;  18083 sec\n",
            "[2021-04-30 08:00:15,916 INFO] Step 25000/50000; xent: 3.06; lr: 0.0000006;  30 docs/s;  18118 sec\n",
            "[2021-04-30 08:00:15,934 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_25000.pt\n",
            "[2021-04-30 08:01:00,905 INFO] Step 25050/50000; xent: 3.02; lr: 0.0000006;  23 docs/s;  18163 sec\n",
            "[2021-04-30 08:01:36,300 INFO] Step 25100/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  18198 sec\n",
            "[2021-04-30 08:01:55,092 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.26.bert.pt, number of examples: 1998\n",
            "[2021-04-30 08:02:13,605 INFO] Step 25150/50000; xent: 2.92; lr: 0.0000006;  28 docs/s;  18235 sec\n",
            "[2021-04-30 08:02:48,985 INFO] Step 25200/50000; xent: 3.04; lr: 0.0000006;  29 docs/s;  18271 sec\n",
            "[2021-04-30 08:03:24,268 INFO] Step 25250/50000; xent: 3.02; lr: 0.0000006;  30 docs/s;  18306 sec\n",
            "[2021-04-30 08:03:59,223 INFO] Step 25300/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  18341 sec\n",
            "[2021-04-30 08:04:12,519 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.77.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:04:36,762 INFO] Step 25350/50000; xent: 3.05; lr: 0.0000006;  28 docs/s;  18378 sec\n",
            "[2021-04-30 08:05:11,896 INFO] Step 25400/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  18414 sec\n",
            "[2021-04-30 08:05:47,171 INFO] Step 25450/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  18449 sec\n",
            "[2021-04-30 08:06:22,387 INFO] Step 25500/50000; xent: 3.03; lr: 0.0000006;  29 docs/s;  18484 sec\n",
            "[2021-04-30 08:06:30,812 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.58.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:06:59,962 INFO] Step 25550/50000; xent: 2.95; lr: 0.0000006;  29 docs/s;  18522 sec\n",
            "[2021-04-30 08:07:35,308 INFO] Step 25600/50000; xent: 2.94; lr: 0.0000006;  29 docs/s;  18557 sec\n",
            "[2021-04-30 08:08:10,450 INFO] Step 25650/50000; xent: 3.01; lr: 0.0000006;  29 docs/s;  18592 sec\n",
            "[2021-04-30 08:08:45,660 INFO] Step 25700/50000; xent: 3.05; lr: 0.0000006;  29 docs/s;  18627 sec\n",
            "[2021-04-30 08:08:49,106 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.130.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:09:23,187 INFO] Step 25750/50000; xent: 3.04; lr: 0.0000006;  28 docs/s;  18665 sec\n",
            "[2021-04-30 08:09:58,511 INFO] Step 25800/50000; xent: 3.06; lr: 0.0000006;  29 docs/s;  18700 sec\n",
            "[2021-04-30 08:10:33,610 INFO] Step 25850/50000; xent: 2.94; lr: 0.0000006;  29 docs/s;  18735 sec\n",
            "[2021-04-30 08:11:07,153 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.109.bert.pt, number of examples: 1999\n",
            "[2021-04-30 08:11:10,800 INFO] Step 25900/50000; xent: 2.98; lr: 0.0000006;  28 docs/s;  18772 sec\n",
            "[2021-04-30 08:11:46,134 INFO] Step 25950/50000; xent: 2.94; lr: 0.0000006;  29 docs/s;  18808 sec\n",
            "[2021-04-30 08:12:21,510 INFO] Step 26000/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  18843 sec\n",
            "[2021-04-30 08:12:21,529 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_26000.pt\n",
            "[2021-04-30 08:13:09,951 INFO] Step 26050/50000; xent: 2.98; lr: 0.0000006;  21 docs/s;  18892 sec\n",
            "[2021-04-30 08:13:39,169 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.126.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:13:47,752 INFO] Step 26100/50000; xent: 3.02; lr: 0.0000006;  28 docs/s;  18929 sec\n",
            "[2021-04-30 08:14:23,047 INFO] Step 26150/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  18965 sec\n",
            "[2021-04-30 08:14:58,295 INFO] Step 26200/50000; xent: 2.97; lr: 0.0000006;  30 docs/s;  19000 sec\n",
            "[2021-04-30 08:15:33,672 INFO] Step 26250/50000; xent: 3.05; lr: 0.0000006;  30 docs/s;  19035 sec\n",
            "[2021-04-30 08:15:57,312 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.46.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:16:10,888 INFO] Step 26300/50000; xent: 2.97; lr: 0.0000006;  28 docs/s;  19073 sec\n",
            "[2021-04-30 08:16:46,287 INFO] Step 26350/50000; xent: 2.99; lr: 0.0000006;  29 docs/s;  19108 sec\n",
            "[2021-04-30 08:17:21,553 INFO] Step 26400/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  19143 sec\n",
            "[2021-04-30 08:17:56,806 INFO] Step 26450/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  19178 sec\n",
            "[2021-04-30 08:18:15,455 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.100.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:18:34,645 INFO] Step 26500/50000; xent: 3.00; lr: 0.0000006;  27 docs/s;  19216 sec\n",
            "[2021-04-30 08:19:09,884 INFO] Step 26550/50000; xent: 2.92; lr: 0.0000006;  30 docs/s;  19252 sec\n",
            "[2021-04-30 08:19:45,130 INFO] Step 26600/50000; xent: 3.06; lr: 0.0000006;  30 docs/s;  19287 sec\n",
            "[2021-04-30 08:20:20,373 INFO] Step 26650/50000; xent: 2.95; lr: 0.0000006;  30 docs/s;  19322 sec\n",
            "[2021-04-30 08:20:32,745 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.85.bert.pt, number of examples: 1998\n",
            "[2021-04-30 08:20:57,378 INFO] Step 26700/50000; xent: 2.97; lr: 0.0000006;  28 docs/s;  19359 sec\n",
            "[2021-04-30 08:21:32,613 INFO] Step 26750/50000; xent: 3.06; lr: 0.0000006;  29 docs/s;  19394 sec\n",
            "[2021-04-30 08:22:07,831 INFO] Step 26800/50000; xent: 3.02; lr: 0.0000006;  30 docs/s;  19430 sec\n",
            "[2021-04-30 08:22:43,115 INFO] Step 26850/50000; xent: 2.93; lr: 0.0000006;  30 docs/s;  19465 sec\n",
            "[2021-04-30 08:22:49,959 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.34.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:23:20,500 INFO] Step 26900/50000; xent: 2.94; lr: 0.0000006;  28 docs/s;  19502 sec\n",
            "[2021-04-30 08:23:55,613 INFO] Step 26950/50000; xent: 2.97; lr: 0.0000006;  31 docs/s;  19537 sec\n",
            "[2021-04-30 08:24:30,929 INFO] Step 27000/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  19573 sec\n",
            "[2021-04-30 08:24:30,947 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_27000.pt\n",
            "[2021-04-30 08:25:16,862 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.7.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:25:17,761 INFO] Step 27050/50000; xent: 3.02; lr: 0.0000006;  22 docs/s;  19619 sec\n",
            "[2021-04-30 08:25:53,219 INFO] Step 27100/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  19655 sec\n",
            "[2021-04-30 08:26:28,560 INFO] Step 27150/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  19690 sec\n",
            "[2021-04-30 08:27:03,907 INFO] Step 27200/50000; xent: 2.96; lr: 0.0000006;  30 docs/s;  19726 sec\n",
            "[2021-04-30 08:27:35,259 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.56.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:27:41,054 INFO] Step 27250/50000; xent: 2.94; lr: 0.0000006;  28 docs/s;  19763 sec\n",
            "[2021-04-30 08:28:16,379 INFO] Step 27300/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  19798 sec\n",
            "[2021-04-30 08:28:51,691 INFO] Step 27350/50000; xent: 2.95; lr: 0.0000006;  30 docs/s;  19833 sec\n",
            "[2021-04-30 08:29:27,005 INFO] Step 27400/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  19869 sec\n",
            "[2021-04-30 08:29:52,833 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.74.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:30:04,271 INFO] Step 27450/50000; xent: 2.98; lr: 0.0000006;  28 docs/s;  19906 sec\n",
            "[2021-04-30 08:30:39,602 INFO] Step 27500/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  19941 sec\n",
            "[2021-04-30 08:31:14,946 INFO] Step 27550/50000; xent: 3.04; lr: 0.0000006;  29 docs/s;  19977 sec\n",
            "[2021-04-30 08:31:50,129 INFO] Step 27600/50000; xent: 2.92; lr: 0.0000006;  30 docs/s;  20012 sec\n",
            "[2021-04-30 08:32:10,809 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.21.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:32:27,131 INFO] Step 27650/50000; xent: 3.01; lr: 0.0000006;  27 docs/s;  20049 sec\n",
            "[2021-04-30 08:33:02,406 INFO] Step 27700/50000; xent: 3.00; lr: 0.0000006;  30 docs/s;  20084 sec\n",
            "[2021-04-30 08:33:37,658 INFO] Step 27750/50000; xent: 3.02; lr: 0.0000006;  30 docs/s;  20119 sec\n",
            "[2021-04-30 08:34:12,986 INFO] Step 27800/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  20155 sec\n",
            "[2021-04-30 08:34:28,934 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.86.bert.pt, number of examples: 1999\n",
            "[2021-04-30 08:34:50,218 INFO] Step 27850/50000; xent: 2.86; lr: 0.0000006;  28 docs/s;  20192 sec\n",
            "[2021-04-30 08:35:25,524 INFO] Step 27900/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  20227 sec\n",
            "[2021-04-30 08:36:00,801 INFO] Step 27950/50000; xent: 3.04; lr: 0.0000006;  30 docs/s;  20262 sec\n",
            "[2021-04-30 08:36:36,142 INFO] Step 28000/50000; xent: 2.94; lr: 0.0000006;  29 docs/s;  20298 sec\n",
            "[2021-04-30 08:36:36,146 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_28000.pt\n",
            "[2021-04-30 08:36:57,436 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.0.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:37:24,491 INFO] Step 28050/50000; xent: 2.93; lr: 0.0000006;  21 docs/s;  20346 sec\n",
            "[2021-04-30 08:37:59,836 INFO] Step 28100/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  20382 sec\n",
            "[2021-04-30 08:38:35,211 INFO] Step 28150/50000; xent: 3.09; lr: 0.0000006;  30 docs/s;  20417 sec\n",
            "[2021-04-30 08:39:10,424 INFO] Step 28200/50000; xent: 2.99; lr: 0.0000006;  29 docs/s;  20452 sec\n",
            "[2021-04-30 08:39:15,653 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.22.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:39:47,564 INFO] Step 28250/50000; xent: 2.94; lr: 0.0000006;  28 docs/s;  20489 sec\n",
            "[2021-04-30 08:40:22,975 INFO] Step 28300/50000; xent: 3.03; lr: 0.0000006;  29 docs/s;  20525 sec\n",
            "[2021-04-30 08:40:58,082 INFO] Step 28350/50000; xent: 3.05; lr: 0.0000006;  30 docs/s;  20560 sec\n",
            "[2021-04-30 08:41:33,626 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.43.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:41:35,156 INFO] Step 28400/50000; xent: 3.01; lr: 0.0000006;  28 docs/s;  20597 sec\n",
            "[2021-04-30 08:42:10,520 INFO] Step 28450/50000; xent: 2.93; lr: 0.0000006;  30 docs/s;  20632 sec\n",
            "[2021-04-30 08:42:45,654 INFO] Step 28500/50000; xent: 3.02; lr: 0.0000006;  30 docs/s;  20667 sec\n",
            "[2021-04-30 08:43:20,978 INFO] Step 28550/50000; xent: 2.97; lr: 0.0000006;  30 docs/s;  20703 sec\n",
            "[2021-04-30 08:43:51,199 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.105.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:43:58,365 INFO] Step 28600/50000; xent: 3.08; lr: 0.0000006;  28 docs/s;  20740 sec\n",
            "[2021-04-30 08:44:33,568 INFO] Step 28650/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  20775 sec\n",
            "[2021-04-30 08:45:08,929 INFO] Step 28700/50000; xent: 3.10; lr: 0.0000006;  29 docs/s;  20811 sec\n",
            "[2021-04-30 08:45:44,234 INFO] Step 28750/50000; xent: 3.06; lr: 0.0000006;  30 docs/s;  20846 sec\n",
            "[2021-04-30 08:46:09,438 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.33.bert.pt, number of examples: 1997\n",
            "[2021-04-30 08:46:21,426 INFO] Step 28800/50000; xent: 3.11; lr: 0.0000006;  28 docs/s;  20883 sec\n",
            "[2021-04-30 08:46:56,760 INFO] Step 28850/50000; xent: 3.01; lr: 0.0000006;  30 docs/s;  20918 sec\n",
            "[2021-04-30 08:47:32,080 INFO] Step 28900/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  20954 sec\n",
            "[2021-04-30 08:48:07,446 INFO] Step 28950/50000; xent: 3.04; lr: 0.0000006;  30 docs/s;  20989 sec\n",
            "[2021-04-30 08:48:25,068 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.34.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:48:43,567 INFO] Step 29000/50000; xent: 3.07; lr: 0.0000006;  29 docs/s;  21025 sec\n",
            "[2021-04-30 08:48:43,583 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_29000.pt\n",
            "[2021-04-30 08:49:33,881 INFO] Step 29050/50000; xent: 2.97; lr: 0.0000006;  21 docs/s;  21076 sec\n",
            "[2021-04-30 08:50:09,386 INFO] Step 29100/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;  21111 sec\n",
            "[2021-04-30 08:50:44,744 INFO] Step 29150/50000; xent: 2.94; lr: 0.0000006;  30 docs/s;  21146 sec\n",
            "[2021-04-30 08:50:58,300 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.110.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:51:21,516 INFO] Step 29200/50000; xent: 2.99; lr: 0.0000006;  27 docs/s;  21183 sec\n",
            "[2021-04-30 08:51:56,887 INFO] Step 29250/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  21219 sec\n",
            "[2021-04-30 08:52:32,279 INFO] Step 29300/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  21254 sec\n",
            "[2021-04-30 08:53:07,608 INFO] Step 29350/50000; xent: 3.00; lr: 0.0000006;  30 docs/s;  21289 sec\n",
            "[2021-04-30 08:53:16,189 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.139.bert.pt, number of examples: 2000\n",
            "[2021-04-30 08:53:44,561 INFO] Step 29400/50000; xent: 2.95; lr: 0.0000006;  29 docs/s;  21326 sec\n",
            "[2021-04-30 08:54:19,785 INFO] Step 29450/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  21361 sec\n",
            "[2021-04-30 08:54:55,168 INFO] Step 29500/50000; xent: 3.01; lr: 0.0000006;  29 docs/s;  21397 sec\n",
            "[2021-04-30 08:55:30,353 INFO] Step 29550/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  21432 sec\n",
            "[2021-04-30 08:55:33,544 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.118.bert.pt, number of examples: 2001\n",
            "[2021-04-30 08:56:07,547 INFO] Step 29600/50000; xent: 2.95; lr: 0.0000006;  28 docs/s;  21469 sec\n",
            "[2021-04-30 08:56:42,987 INFO] Step 29650/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  21505 sec\n",
            "[2021-04-30 08:57:18,288 INFO] Step 29700/50000; xent: 2.93; lr: 0.0000006;  30 docs/s;  21540 sec\n",
            "[2021-04-30 08:57:50,968 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.140.bert.pt, number of examples: 1999\n",
            "[2021-04-30 08:57:55,343 INFO] Step 29750/50000; xent: 2.97; lr: 0.0000006;  28 docs/s;  21577 sec\n",
            "[2021-04-30 08:58:30,730 INFO] Step 29800/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  21612 sec\n",
            "[2021-04-30 08:59:06,063 INFO] Step 29850/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  21648 sec\n",
            "[2021-04-30 08:59:41,417 INFO] Step 29900/50000; xent: 3.04; lr: 0.0000006;  29 docs/s;  21683 sec\n",
            "[2021-04-30 09:00:10,602 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.91.bert.pt, number of examples: 1995\n",
            "[2021-04-30 09:00:19,940 INFO] Step 29950/50000; xent: 3.02; lr: 0.0000006;  27 docs/s;  21722 sec\n",
            "[2021-04-30 09:00:55,024 INFO] Step 30000/50000; xent: 2.99; lr: 0.0000006;  29 docs/s;  21757 sec\n",
            "[2021-04-30 09:00:55,027 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_30000.pt\n",
            "[2021-04-30 09:01:39,825 INFO] Step 30050/50000; xent: 2.99; lr: 0.0000006;  23 docs/s;  21802 sec\n",
            "[2021-04-30 09:02:15,251 INFO] Step 30100/50000; xent: 2.88; lr: 0.0000006;  29 docs/s;  21837 sec\n",
            "[2021-04-30 09:02:38,140 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.133.bert.pt, number of examples: 1999\n",
            "[2021-04-30 09:02:52,429 INFO] Step 30150/50000; xent: 3.00; lr: 0.0000006;  28 docs/s;  21874 sec\n",
            "[2021-04-30 09:03:27,618 INFO] Step 30200/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  21909 sec\n",
            "[2021-04-30 09:04:03,036 INFO] Step 30250/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  21945 sec\n",
            "[2021-04-30 09:04:38,482 INFO] Step 30300/50000; xent: 3.06; lr: 0.0000006;  29 docs/s;  21980 sec\n",
            "[2021-04-30 09:04:56,656 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.47.bert.pt, number of examples: 2000\n",
            "[2021-04-30 09:05:15,652 INFO] Step 30350/50000; xent: 2.95; lr: 0.0000006;  28 docs/s;  22017 sec\n",
            "[2021-04-30 09:05:51,059 INFO] Step 30400/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  22053 sec\n",
            "[2021-04-30 09:06:26,367 INFO] Step 30450/50000; xent: 3.01; lr: 0.0000006;  30 docs/s;  22088 sec\n",
            "[2021-04-30 09:07:01,819 INFO] Step 30500/50000; xent: 3.04; lr: 0.0000006;  29 docs/s;  22124 sec\n",
            "[2021-04-30 09:07:14,725 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.1.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:07:38,923 INFO] Step 30550/50000; xent: 3.04; lr: 0.0000006;  28 docs/s;  22161 sec\n",
            "[2021-04-30 09:08:14,283 INFO] Step 30600/50000; xent: 3.02; lr: 0.0000006;  30 docs/s;  22196 sec\n",
            "[2021-04-30 09:08:49,655 INFO] Step 30650/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  22231 sec\n",
            "[2021-04-30 09:09:24,690 INFO] Step 30700/50000; xent: 2.99; lr: 0.0000006;  29 docs/s;  22266 sec\n",
            "[2021-04-30 09:09:32,016 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.103.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:10:01,521 INFO] Step 30750/50000; xent: 3.03; lr: 0.0000006;  29 docs/s;  22303 sec\n",
            "[2021-04-30 09:10:36,858 INFO] Step 30800/50000; xent: 2.94; lr: 0.0000006;  30 docs/s;  22339 sec\n",
            "[2021-04-30 09:11:12,273 INFO] Step 30850/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;  22374 sec\n",
            "[2021-04-30 09:11:47,519 INFO] Step 30900/50000; xent: 2.99; lr: 0.0000006;  29 docs/s;  22409 sec\n",
            "[2021-04-30 09:11:49,869 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.27.bert.pt, number of examples: 1999\n",
            "[2021-04-30 09:12:24,693 INFO] Step 30950/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  22446 sec\n",
            "[2021-04-30 09:12:59,971 INFO] Step 31000/50000; xent: 2.99; lr: 0.0000006;  30 docs/s;  22482 sec\n",
            "[2021-04-30 09:12:59,987 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_31000.pt\n",
            "[2021-04-30 09:13:44,234 INFO] Step 31050/50000; xent: 3.00; lr: 0.0000006;  23 docs/s;  22526 sec\n",
            "[2021-04-30 09:14:16,182 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.7.bert.pt, number of examples: 2000\n",
            "[2021-04-30 09:14:21,247 INFO] Step 31100/50000; xent: 2.98; lr: 0.0000006;  27 docs/s;  22563 sec\n",
            "[2021-04-30 09:14:56,590 INFO] Step 31150/50000; xent: 2.95; lr: 0.0000006;  30 docs/s;  22598 sec\n",
            "[2021-04-30 09:15:31,941 INFO] Step 31200/50000; xent: 2.98; lr: 0.0000006;  29 docs/s;  22634 sec\n",
            "[2021-04-30 09:16:07,294 INFO] Step 31250/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  22669 sec\n",
            "[2021-04-30 09:16:34,371 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.3.bert.pt, number of examples: 2000\n",
            "[2021-04-30 09:16:44,420 INFO] Step 31300/50000; xent: 3.01; lr: 0.0000006;  28 docs/s;  22706 sec\n",
            "[2021-04-30 09:17:19,730 INFO] Step 31350/50000; xent: 2.90; lr: 0.0000006;  30 docs/s;  22741 sec\n",
            "[2021-04-30 09:17:55,075 INFO] Step 31400/50000; xent: 3.11; lr: 0.0000006;  29 docs/s;  22777 sec\n",
            "[2021-04-30 09:18:30,394 INFO] Step 31450/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  22812 sec\n",
            "[2021-04-30 09:18:52,570 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.117.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:19:07,602 INFO] Step 31500/50000; xent: 3.01; lr: 0.0000006;  28 docs/s;  22849 sec\n",
            "[2021-04-30 09:19:43,041 INFO] Step 31550/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  22885 sec\n",
            "[2021-04-30 09:20:18,300 INFO] Step 31600/50000; xent: 3.00; lr: 0.0000006;  30 docs/s;  22920 sec\n",
            "[2021-04-30 09:20:53,686 INFO] Step 31650/50000; xent: 2.89; lr: 0.0000006;  30 docs/s;  22955 sec\n",
            "[2021-04-30 09:21:10,178 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.119.bert.pt, number of examples: 1998\n",
            "[2021-04-30 09:21:30,872 INFO] Step 31700/50000; xent: 2.99; lr: 0.0000006;  28 docs/s;  22993 sec\n",
            "[2021-04-30 09:22:06,323 INFO] Step 31750/50000; xent: 3.03; lr: 0.0000006;  29 docs/s;  23028 sec\n",
            "[2021-04-30 09:22:41,667 INFO] Step 31800/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  23063 sec\n",
            "[2021-04-30 09:23:17,016 INFO] Step 31850/50000; xent: 3.08; lr: 0.0000006;  29 docs/s;  23099 sec\n",
            "[2021-04-30 09:23:28,505 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.84.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:23:54,821 INFO] Step 31900/50000; xent: 2.96; lr: 0.0000006;  27 docs/s;  23137 sec\n",
            "[2021-04-30 09:24:29,984 INFO] Step 31950/50000; xent: 3.03; lr: 0.0000006;  29 docs/s;  23172 sec\n",
            "[2021-04-30 09:25:05,364 INFO] Step 32000/50000; xent: 2.95; lr: 0.0000006;  29 docs/s;  23207 sec\n",
            "[2021-04-30 09:25:05,381 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_32000.pt\n",
            "[2021-04-30 09:25:50,954 INFO] Step 32050/50000; xent: 2.90; lr: 0.0000006;  23 docs/s;  23253 sec\n",
            "[2021-04-30 09:25:57,287 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.71.bert.pt, number of examples: 1999\n",
            "[2021-04-30 09:26:28,574 INFO] Step 32100/50000; xent: 3.02; lr: 0.0000006;  28 docs/s;  23290 sec\n",
            "[2021-04-30 09:27:03,935 INFO] Step 32150/50000; xent: 2.92; lr: 0.0000006;  30 docs/s;  23326 sec\n",
            "[2021-04-30 09:27:39,275 INFO] Step 32200/50000; xent: 2.97; lr: 0.0000006;  29 docs/s;  23361 sec\n",
            "[2021-04-30 09:28:15,339 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.124.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:28:16,163 INFO] Step 32250/50000; xent: 2.99; lr: 0.0000006;  28 docs/s;  23398 sec\n",
            "[2021-04-30 09:28:51,517 INFO] Step 32300/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  23433 sec\n",
            "[2021-04-30 09:29:26,716 INFO] Step 32350/50000; xent: 3.02; lr: 0.0000006;  29 docs/s;  23468 sec\n",
            "[2021-04-30 09:30:02,048 INFO] Step 32400/50000; xent: 3.04; lr: 0.0000006;  30 docs/s;  23504 sec\n",
            "[2021-04-30 09:30:32,857 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.35.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:30:39,321 INFO] Step 32450/50000; xent: 3.00; lr: 0.0000006;  28 docs/s;  23541 sec\n",
            "[2021-04-30 09:31:14,623 INFO] Step 32500/50000; xent: 3.09; lr: 0.0000006;  29 docs/s;  23576 sec\n",
            "[2021-04-30 09:31:49,946 INFO] Step 32550/50000; xent: 2.97; lr: 0.0000006;  30 docs/s;  23612 sec\n",
            "[2021-04-30 09:32:25,152 INFO] Step 32600/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  23647 sec\n",
            "[2021-04-30 09:32:50,586 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.31.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:33:01,983 INFO] Step 32650/50000; xent: 2.95; lr: 0.0000006;  28 docs/s;  23684 sec\n",
            "[2021-04-30 09:33:37,137 INFO] Step 32700/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;  23719 sec\n",
            "[2021-04-30 09:34:12,507 INFO] Step 32750/50000; xent: 2.96; lr: 0.0000006;  30 docs/s;  23754 sec\n",
            "[2021-04-30 09:34:47,782 INFO] Step 32800/50000; xent: 2.96; lr: 0.0000006;  30 docs/s;  23789 sec\n",
            "[2021-04-30 09:35:07,771 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.62.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:35:24,862 INFO] Step 32850/50000; xent: 3.02; lr: 0.0000006;  28 docs/s;  23827 sec\n",
            "[2021-04-30 09:36:00,045 INFO] Step 32900/50000; xent: 3.00; lr: 0.0000006;  29 docs/s;  23862 sec\n",
            "[2021-04-30 09:36:35,371 INFO] Step 32950/50000; xent: 2.96; lr: 0.0000006;  29 docs/s;  23897 sec\n",
            "[2021-04-30 09:37:10,712 INFO] Step 33000/50000; xent: 2.98; lr: 0.0000006;  30 docs/s;  23932 sec\n",
            "[2021-04-30 09:37:10,715 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_33000.pt\n",
            "[2021-04-30 09:37:34,492 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.52.bert.pt, number of examples: 1999\n",
            "[2021-04-30 09:37:57,947 INFO] Step 33050/50000; xent: 2.99; lr: 0.0000006;  22 docs/s;  23980 sec\n",
            "[2021-04-30 09:38:33,182 INFO] Step 33100/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  24015 sec\n",
            "[2021-04-30 09:39:08,537 INFO] Step 33150/50000; xent: 2.90; lr: 0.0000005;  29 docs/s;  24050 sec\n",
            "[2021-04-30 09:39:43,866 INFO] Step 33200/50000; xent: 2.93; lr: 0.0000005;  30 docs/s;  24086 sec\n",
            "[2021-04-30 09:39:53,156 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.25.bert.pt, number of examples: 1999\n",
            "[2021-04-30 09:40:20,799 INFO] Step 33250/50000; xent: 3.03; lr: 0.0000005;  28 docs/s;  24122 sec\n",
            "[2021-04-30 09:40:56,096 INFO] Step 33300/50000; xent: 3.03; lr: 0.0000005;  30 docs/s;  24158 sec\n",
            "[2021-04-30 09:41:31,440 INFO] Step 33350/50000; xent: 2.95; lr: 0.0000005;  29 docs/s;  24193 sec\n",
            "[2021-04-30 09:42:06,579 INFO] Step 33400/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  24228 sec\n",
            "[2021-04-30 09:42:11,582 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.136.bert.pt, number of examples: 1998\n",
            "[2021-04-30 09:42:44,739 INFO] Step 33450/50000; xent: 3.03; lr: 0.0000005;  27 docs/s;  24266 sec\n",
            "[2021-04-30 09:43:19,964 INFO] Step 33500/50000; xent: 2.97; lr: 0.0000005;  31 docs/s;  24302 sec\n",
            "[2021-04-30 09:43:55,314 INFO] Step 33550/50000; xent: 2.91; lr: 0.0000005;  29 docs/s;  24337 sec\n",
            "[2021-04-30 09:44:28,733 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.82.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:44:32,392 INFO] Step 33600/50000; xent: 2.96; lr: 0.0000005;  28 docs/s;  24374 sec\n",
            "[2021-04-30 09:45:07,740 INFO] Step 33650/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  24409 sec\n",
            "[2021-04-30 09:45:43,035 INFO] Step 33700/50000; xent: 2.96; lr: 0.0000005;  30 docs/s;  24445 sec\n",
            "[2021-04-30 09:46:18,199 INFO] Step 33750/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  24480 sec\n",
            "[2021-04-30 09:46:46,616 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.55.bert.pt, number of examples: 1999\n",
            "[2021-04-30 09:46:55,223 INFO] Step 33800/50000; xent: 3.07; lr: 0.0000005;  28 docs/s;  24517 sec\n",
            "[2021-04-30 09:47:30,471 INFO] Step 33850/50000; xent: 3.00; lr: 0.0000005;  31 docs/s;  24552 sec\n",
            "[2021-04-30 09:48:05,662 INFO] Step 33900/50000; xent: 3.03; lr: 0.0000005;  29 docs/s;  24587 sec\n",
            "[2021-04-30 09:48:40,931 INFO] Step 33950/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  24623 sec\n",
            "[2021-04-30 09:49:04,352 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.116.bert.pt, number of examples: 1997\n",
            "[2021-04-30 09:49:18,598 INFO] Step 34000/50000; xent: 2.94; lr: 0.0000005;  27 docs/s;  24660 sec\n",
            "[2021-04-30 09:49:18,615 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_34000.pt\n",
            "[2021-04-30 09:50:10,119 INFO] Step 34050/50000; xent: 3.02; lr: 0.0000005;  20 docs/s;  24712 sec\n",
            "[2021-04-30 09:50:45,552 INFO] Step 34100/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  24747 sec\n",
            "[2021-04-30 09:51:20,843 INFO] Step 34150/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  24783 sec\n",
            "[2021-04-30 09:51:38,382 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.101.bert.pt, number of examples: 1998\n",
            "[2021-04-30 09:51:57,539 INFO] Step 34200/50000; xent: 3.04; lr: 0.0000005;  28 docs/s;  24819 sec\n",
            "[2021-04-30 09:52:32,803 INFO] Step 34250/50000; xent: 2.95; lr: 0.0000005;  29 docs/s;  24854 sec\n",
            "[2021-04-30 09:53:08,093 INFO] Step 34300/50000; xent: 3.01; lr: 0.0000005;  30 docs/s;  24890 sec\n",
            "[2021-04-30 09:53:43,370 INFO] Step 34350/50000; xent: 3.03; lr: 0.0000005;  29 docs/s;  24925 sec\n",
            "[2021-04-30 09:53:55,737 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.95.bert.pt, number of examples: 2000\n",
            "[2021-04-30 09:54:20,604 INFO] Step 34400/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  24962 sec\n",
            "[2021-04-30 09:54:55,893 INFO] Step 34450/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  24998 sec\n",
            "[2021-04-30 09:55:31,146 INFO] Step 34500/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  25033 sec\n",
            "[2021-04-30 09:56:06,290 INFO] Step 34550/50000; xent: 3.03; lr: 0.0000005;  30 docs/s;  25068 sec\n",
            "[2021-04-30 09:56:13,066 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.81.bert.pt, number of examples: 2001\n",
            "[2021-04-30 09:56:43,638 INFO] Step 34600/50000; xent: 3.01; lr: 0.0000005;  27 docs/s;  25105 sec\n",
            "[2021-04-30 09:57:18,918 INFO] Step 34650/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  25141 sec\n",
            "[2021-04-30 09:57:54,246 INFO] Step 34700/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  25176 sec\n",
            "[2021-04-30 09:58:30,522 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.12.bert.pt, number of examples: 1998\n",
            "[2021-04-30 09:58:31,349 INFO] Step 34750/50000; xent: 2.99; lr: 0.0000005;  28 docs/s;  25213 sec\n",
            "[2021-04-30 09:59:06,749 INFO] Step 34800/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  25248 sec\n",
            "[2021-04-30 09:59:42,059 INFO] Step 34850/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  25284 sec\n",
            "[2021-04-30 10:00:17,362 INFO] Step 34900/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  25319 sec\n",
            "[2021-04-30 10:00:48,357 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.131.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:00:54,837 INFO] Step 34950/50000; xent: 2.99; lr: 0.0000005;  28 docs/s;  25357 sec\n",
            "[2021-04-30 10:01:30,148 INFO] Step 35000/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  25392 sec\n",
            "[2021-04-30 10:01:30,165 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_35000.pt\n",
            "[2021-04-30 10:02:15,174 INFO] Step 35050/50000; xent: 3.06; lr: 0.0000005;  23 docs/s;  25437 sec\n",
            "[2021-04-30 10:02:50,386 INFO] Step 35100/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  25472 sec\n",
            "[2021-04-30 10:03:16,033 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.97.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:03:28,190 INFO] Step 35150/50000; xent: 3.00; lr: 0.0000005;  27 docs/s;  25510 sec\n",
            "[2021-04-30 10:04:03,552 INFO] Step 35200/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  25545 sec\n",
            "[2021-04-30 10:04:38,805 INFO] Step 35250/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  25580 sec\n",
            "[2021-04-30 10:05:14,135 INFO] Step 35300/50000; xent: 2.95; lr: 0.0000005;  29 docs/s;  25616 sec\n",
            "[2021-04-30 10:05:35,001 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.83.bert.pt, number of examples: 1999\n",
            "[2021-04-30 10:05:51,143 INFO] Step 35350/50000; xent: 3.00; lr: 0.0000005;  28 docs/s;  25653 sec\n",
            "[2021-04-30 10:06:26,538 INFO] Step 35400/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  25688 sec\n",
            "[2021-04-30 10:07:01,794 INFO] Step 35450/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  25723 sec\n",
            "[2021-04-30 10:07:36,873 INFO] Step 35500/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  25759 sec\n",
            "[2021-04-30 10:07:51,974 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.100.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:08:13,961 INFO] Step 35550/50000; xent: 3.02; lr: 0.0000005;  28 docs/s;  25796 sec\n",
            "[2021-04-30 10:08:49,249 INFO] Step 35600/50000; xent: 2.90; lr: 0.0000005;  30 docs/s;  25831 sec\n",
            "[2021-04-30 10:09:24,435 INFO] Step 35650/50000; xent: 3.05; lr: 0.0000005;  30 docs/s;  25866 sec\n",
            "[2021-04-30 10:09:59,823 INFO] Step 35700/50000; xent: 2.94; lr: 0.0000005;  29 docs/s;  25902 sec\n",
            "[2021-04-30 10:10:09,592 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.15.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:10:37,923 INFO] Step 35750/50000; xent: 2.95; lr: 0.0000005;  27 docs/s;  25940 sec\n",
            "[2021-04-30 10:11:12,982 INFO] Step 35800/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  25975 sec\n",
            "[2021-04-30 10:11:48,286 INFO] Step 35850/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  26010 sec\n",
            "[2021-04-30 10:12:23,360 INFO] Step 35900/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  26045 sec\n",
            "[2021-04-30 10:12:27,218 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.137.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:13:00,464 INFO] Step 35950/50000; xent: 2.97; lr: 0.0000005;  28 docs/s;  26082 sec\n",
            "[2021-04-30 10:13:35,700 INFO] Step 36000/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  26117 sec\n",
            "[2021-04-30 10:13:35,722 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_36000.pt\n",
            "[2021-04-30 10:14:20,915 INFO] Step 36050/50000; xent: 3.02; lr: 0.0000005;  23 docs/s;  26163 sec\n",
            "[2021-04-30 10:14:56,979 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.106.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:14:59,238 INFO] Step 36100/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  26201 sec\n",
            "[2021-04-30 10:15:34,514 INFO] Step 36150/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  26236 sec\n",
            "[2021-04-30 10:16:09,804 INFO] Step 36200/50000; xent: 3.01; lr: 0.0000005;  30 docs/s;  26271 sec\n",
            "[2021-04-30 10:16:45,082 INFO] Step 36250/50000; xent: 3.01; lr: 0.0000005;  29 docs/s;  26307 sec\n",
            "[2021-04-30 10:17:14,444 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.94.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:17:22,311 INFO] Step 36300/50000; xent: 2.96; lr: 0.0000005;  27 docs/s;  26344 sec\n",
            "[2021-04-30 10:17:57,546 INFO] Step 36350/50000; xent: 2.96; lr: 0.0000005;  30 docs/s;  26379 sec\n",
            "[2021-04-30 10:18:32,864 INFO] Step 36400/50000; xent: 2.92; lr: 0.0000005;  30 docs/s;  26415 sec\n",
            "[2021-04-30 10:19:08,134 INFO] Step 36450/50000; xent: 3.03; lr: 0.0000005;  29 docs/s;  26450 sec\n",
            "[2021-04-30 10:19:32,470 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.112.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:19:46,033 INFO] Step 36500/50000; xent: 2.98; lr: 0.0000005;  27 docs/s;  26488 sec\n",
            "[2021-04-30 10:20:21,209 INFO] Step 36550/50000; xent: 3.01; lr: 0.0000005;  29 docs/s;  26523 sec\n",
            "[2021-04-30 10:20:56,487 INFO] Step 36600/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  26558 sec\n",
            "[2021-04-30 10:21:31,815 INFO] Step 36650/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  26594 sec\n",
            "[2021-04-30 10:21:50,486 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.96.bert.pt, number of examples: 1999\n",
            "[2021-04-30 10:22:08,949 INFO] Step 36700/50000; xent: 2.97; lr: 0.0000005;  28 docs/s;  26631 sec\n",
            "[2021-04-30 10:22:44,299 INFO] Step 36750/50000; xent: 3.00; lr: 0.0000005;  30 docs/s;  26666 sec\n",
            "[2021-04-30 10:23:19,510 INFO] Step 36800/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  26701 sec\n",
            "[2021-04-30 10:23:54,844 INFO] Step 36850/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  26737 sec\n",
            "[2021-04-30 10:24:07,244 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.20.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:24:32,053 INFO] Step 36900/50000; xent: 3.02; lr: 0.0000005;  28 docs/s;  26774 sec\n",
            "[2021-04-30 10:25:07,317 INFO] Step 36950/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  26809 sec\n",
            "[2021-04-30 10:25:42,526 INFO] Step 37000/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  26844 sec\n",
            "[2021-04-30 10:25:42,545 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_37000.pt\n",
            "[2021-04-30 10:26:27,080 INFO] Step 37050/50000; xent: 2.99; lr: 0.0000005;  23 docs/s;  26889 sec\n",
            "[2021-04-30 10:26:33,088 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.74.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:27:03,624 INFO] Step 37100/50000; xent: 2.97; lr: 0.0000005;  28 docs/s;  26925 sec\n",
            "[2021-04-30 10:27:38,993 INFO] Step 37150/50000; xent: 2.95; lr: 0.0000005;  29 docs/s;  26961 sec\n",
            "[2021-04-30 10:28:14,096 INFO] Step 37200/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  26996 sec\n",
            "[2021-04-30 10:28:49,277 INFO] Step 37250/50000; xent: 3.04; lr: 0.0000005;  30 docs/s;  27031 sec\n",
            "[2021-04-30 10:28:50,969 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.129.bert.pt, number of examples: 1999\n",
            "[2021-04-30 10:29:26,319 INFO] Step 37300/50000; xent: 2.88; lr: 0.0000005;  29 docs/s;  27068 sec\n",
            "[2021-04-30 10:30:01,616 INFO] Step 37350/50000; xent: 3.06; lr: 0.0000005;  29 docs/s;  27103 sec\n",
            "[2021-04-30 10:30:36,917 INFO] Step 37400/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  27139 sec\n",
            "[2021-04-30 10:31:08,420 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.113.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:31:14,181 INFO] Step 37450/50000; xent: 3.00; lr: 0.0000005;  28 docs/s;  27176 sec\n",
            "[2021-04-30 10:31:49,484 INFO] Step 37500/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  27211 sec\n",
            "[2021-04-30 10:32:24,514 INFO] Step 37550/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;  27246 sec\n",
            "[2021-04-30 10:32:59,786 INFO] Step 37600/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  27281 sec\n",
            "[2021-04-30 10:33:26,313 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.23.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:33:37,029 INFO] Step 37650/50000; xent: 3.02; lr: 0.0000005;  28 docs/s;  27319 sec\n",
            "[2021-04-30 10:34:12,305 INFO] Step 37700/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  27354 sec\n",
            "[2021-04-30 10:34:47,571 INFO] Step 37750/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  27389 sec\n",
            "[2021-04-30 10:35:22,796 INFO] Step 37800/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  27424 sec\n",
            "[2021-04-30 10:35:43,424 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.46.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:35:59,797 INFO] Step 37850/50000; xent: 3.03; lr: 0.0000005;  27 docs/s;  27461 sec\n",
            "[2021-04-30 10:36:35,051 INFO] Step 37900/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  27497 sec\n",
            "[2021-04-30 10:37:10,343 INFO] Step 37950/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  27532 sec\n",
            "[2021-04-30 10:37:45,639 INFO] Step 38000/50000; xent: 2.94; lr: 0.0000005;  29 docs/s;  27567 sec\n",
            "[2021-04-30 10:37:45,643 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_38000.pt\n",
            "[2021-04-30 10:38:14,830 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.104.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:38:36,761 INFO] Step 38050/50000; xent: 2.93; lr: 0.0000005;  20 docs/s;  27618 sec\n",
            "[2021-04-30 10:39:12,031 INFO] Step 38100/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  27654 sec\n",
            "[2021-04-30 10:39:47,271 INFO] Step 38150/50000; xent: 2.88; lr: 0.0000005;  30 docs/s;  27689 sec\n",
            "[2021-04-30 10:40:22,378 INFO] Step 38200/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  27724 sec\n",
            "[2021-04-30 10:40:31,684 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.126.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:40:59,272 INFO] Step 38250/50000; xent: 3.03; lr: 0.0000005;  28 docs/s;  27761 sec\n",
            "[2021-04-30 10:41:34,563 INFO] Step 38300/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  27796 sec\n",
            "[2021-04-30 10:42:09,561 INFO] Step 38350/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  27831 sec\n",
            "[2021-04-30 10:42:44,604 INFO] Step 38400/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  27866 sec\n",
            "[2021-04-30 10:42:50,094 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.114.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:43:21,930 INFO] Step 38450/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  27904 sec\n",
            "[2021-04-30 10:43:57,209 INFO] Step 38500/50000; xent: 3.04; lr: 0.0000005;  30 docs/s;  27939 sec\n",
            "[2021-04-30 10:44:32,245 INFO] Step 38550/50000; xent: 3.00; lr: 0.0000005;  30 docs/s;  27974 sec\n",
            "[2021-04-30 10:45:07,992 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.87.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:45:09,523 INFO] Step 38600/50000; xent: 2.97; lr: 0.0000005;  28 docs/s;  28011 sec\n",
            "[2021-04-30 10:45:44,753 INFO] Step 38650/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  28046 sec\n",
            "[2021-04-30 10:46:19,719 INFO] Step 38700/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  28081 sec\n",
            "[2021-04-30 10:46:54,991 INFO] Step 38750/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  28117 sec\n",
            "[2021-04-30 10:47:25,428 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.49.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:47:31,711 INFO] Step 38800/50000; xent: 2.95; lr: 0.0000005;  28 docs/s;  28153 sec\n",
            "[2021-04-30 10:48:06,923 INFO] Step 38850/50000; xent: 2.96; lr: 0.0000005;  30 docs/s;  28189 sec\n",
            "[2021-04-30 10:48:42,145 INFO] Step 38900/50000; xent: 2.96; lr: 0.0000005;  29 docs/s;  28224 sec\n",
            "[2021-04-30 10:49:17,425 INFO] Step 38950/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  28259 sec\n",
            "[2021-04-30 10:49:42,161 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.141.bert.pt, number of examples: 1998\n",
            "[2021-04-30 10:49:54,301 INFO] Step 39000/50000; xent: 2.96; lr: 0.0000005;  28 docs/s;  28296 sec\n",
            "[2021-04-30 10:49:54,317 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_39000.pt\n",
            "[2021-04-30 10:50:39,410 INFO] Step 39050/50000; xent: 3.01; lr: 0.0000005;  23 docs/s;  28341 sec\n",
            "[2021-04-30 10:51:14,730 INFO] Step 39100/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  28376 sec\n",
            "[2021-04-30 10:51:49,838 INFO] Step 39150/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  28412 sec\n",
            "[2021-04-30 10:52:09,817 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.50.bert.pt, number of examples: 1999\n",
            "[2021-04-30 10:52:26,866 INFO] Step 39200/50000; xent: 3.00; lr: 0.0000005;  28 docs/s;  28449 sec\n",
            "[2021-04-30 10:53:02,092 INFO] Step 39250/50000; xent: 3.03; lr: 0.0000005;  29 docs/s;  28484 sec\n",
            "[2021-04-30 10:53:37,441 INFO] Step 39300/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  28519 sec\n",
            "[2021-04-30 10:54:12,569 INFO] Step 39350/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  28554 sec\n",
            "[2021-04-30 10:54:27,546 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.48.bert.pt, number of examples: 2000\n",
            "[2021-04-30 10:54:49,431 INFO] Step 39400/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  28591 sec\n",
            "[2021-04-30 10:55:24,668 INFO] Step 39450/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  28626 sec\n",
            "[2021-04-30 10:55:59,820 INFO] Step 39500/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;  28662 sec\n",
            "[2021-04-30 10:56:35,168 INFO] Step 39550/50000; xent: 2.96; lr: 0.0000005;  29 docs/s;  28697 sec\n",
            "[2021-04-30 10:56:45,102 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.76.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:57:12,103 INFO] Step 39600/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  28734 sec\n",
            "[2021-04-30 10:57:47,346 INFO] Step 39650/50000; xent: 2.92; lr: 0.0000005;  29 docs/s;  28769 sec\n",
            "[2021-04-30 10:58:22,671 INFO] Step 39700/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  28804 sec\n",
            "[2021-04-30 10:58:57,724 INFO] Step 39750/50000; xent: 2.94; lr: 0.0000005;  29 docs/s;  28839 sec\n",
            "[2021-04-30 10:59:02,948 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.132.bert.pt, number of examples: 2001\n",
            "[2021-04-30 10:59:34,813 INFO] Step 39800/50000; xent: 2.96; lr: 0.0000005;  28 docs/s;  28877 sec\n",
            "[2021-04-30 11:00:09,910 INFO] Step 39850/50000; xent: 3.05; lr: 0.0000005;  29 docs/s;  28912 sec\n",
            "[2021-04-30 11:00:45,142 INFO] Step 39900/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  28947 sec\n",
            "[2021-04-30 11:01:20,469 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.58.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:01:21,995 INFO] Step 39950/50000; xent: 3.02; lr: 0.0000005;  28 docs/s;  28984 sec\n",
            "[2021-04-30 11:01:57,038 INFO] Step 40000/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  29019 sec\n",
            "[2021-04-30 11:01:57,056 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_40000.pt\n",
            "[2021-04-30 11:02:45,759 INFO] Step 40050/50000; xent: 2.98; lr: 0.0000005;  22 docs/s;  29067 sec\n",
            "[2021-04-30 11:03:21,174 INFO] Step 40100/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  29103 sec\n",
            "[2021-04-30 11:03:51,776 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.41.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:03:58,251 INFO] Step 40150/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  29140 sec\n",
            "[2021-04-30 11:04:33,364 INFO] Step 40200/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;  29175 sec\n",
            "[2021-04-30 11:05:08,544 INFO] Step 40250/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  29210 sec\n",
            "[2021-04-30 11:05:43,785 INFO] Step 40300/50000; xent: 2.92; lr: 0.0000005;  30 docs/s;  29245 sec\n",
            "[2021-04-30 11:06:09,683 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.108.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:06:21,805 INFO] Step 40350/50000; xent: 2.93; lr: 0.0000005;  27 docs/s;  29283 sec\n",
            "[2021-04-30 11:06:57,056 INFO] Step 40400/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  29319 sec\n",
            "[2021-04-30 11:07:32,314 INFO] Step 40450/50000; xent: 2.93; lr: 0.0000005;  30 docs/s;  29354 sec\n",
            "[2021-04-30 11:08:07,530 INFO] Step 40500/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  29389 sec\n",
            "[2021-04-30 11:08:26,859 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.45.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:08:44,570 INFO] Step 40550/50000; xent: 2.94; lr: 0.0000005;  28 docs/s;  29426 sec\n",
            "[2021-04-30 11:09:19,807 INFO] Step 40600/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  29462 sec\n",
            "[2021-04-30 11:09:55,083 INFO] Step 40650/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  29497 sec\n",
            "[2021-04-30 11:10:30,222 INFO] Step 40700/50000; xent: 3.01; lr: 0.0000005;  29 docs/s;  29532 sec\n",
            "[2021-04-30 11:10:44,504 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.107.bert.pt, number of examples: 1999\n",
            "[2021-04-30 11:11:07,189 INFO] Step 40750/50000; xent: 2.99; lr: 0.0000005;  28 docs/s;  29569 sec\n",
            "[2021-04-30 11:11:42,344 INFO] Step 40800/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  29604 sec\n",
            "[2021-04-30 11:12:17,597 INFO] Step 40850/50000; xent: 2.95; lr: 0.0000005;  29 docs/s;  29639 sec\n",
            "[2021-04-30 11:12:52,847 INFO] Step 40900/50000; xent: 3.01; lr: 0.0000005;  30 docs/s;  29675 sec\n",
            "[2021-04-30 11:13:03,033 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.67.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:13:30,402 INFO] Step 40950/50000; xent: 2.92; lr: 0.0000005;  28 docs/s;  29712 sec\n",
            "[2021-04-30 11:14:05,591 INFO] Step 41000/50000; xent: 2.96; lr: 0.0000005;  30 docs/s;  29747 sec\n",
            "[2021-04-30 11:14:05,608 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_41000.pt\n",
            "[2021-04-30 11:14:50,478 INFO] Step 41050/50000; xent: 3.01; lr: 0.0000005;  23 docs/s;  29792 sec\n",
            "[2021-04-30 11:15:25,595 INFO] Step 41100/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  29827 sec\n",
            "[2021-04-30 11:15:30,305 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.138.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:16:02,795 INFO] Step 41150/50000; xent: 3.04; lr: 0.0000005;  28 docs/s;  29864 sec\n",
            "[2021-04-30 11:16:38,090 INFO] Step 41200/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  29900 sec\n",
            "[2021-04-30 11:17:13,363 INFO] Step 41250/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  29935 sec\n",
            "[2021-04-30 11:17:46,843 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.4.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:17:50,485 INFO] Step 41300/50000; xent: 2.90; lr: 0.0000005;  28 docs/s;  29972 sec\n",
            "[2021-04-30 11:18:25,769 INFO] Step 41350/50000; xent: 2.92; lr: 0.0000005;  30 docs/s;  30007 sec\n",
            "[2021-04-30 11:19:00,991 INFO] Step 41400/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  30043 sec\n",
            "[2021-04-30 11:19:36,247 INFO] Step 41450/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  30078 sec\n",
            "[2021-04-30 11:20:05,721 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.93.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:20:14,314 INFO] Step 41500/50000; xent: 3.02; lr: 0.0000005;  27 docs/s;  30116 sec\n",
            "[2021-04-30 11:20:49,439 INFO] Step 41550/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  30151 sec\n",
            "[2021-04-30 11:21:24,710 INFO] Step 41600/50000; xent: 2.94; lr: 0.0000005;  29 docs/s;  30186 sec\n",
            "[2021-04-30 11:21:59,825 INFO] Step 41650/50000; xent: 2.93; lr: 0.0000005;  31 docs/s;  30222 sec\n",
            "[2021-04-30 11:22:23,327 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.26.bert.pt, number of examples: 1998\n",
            "[2021-04-30 11:22:36,848 INFO] Step 41700/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  30259 sec\n",
            "[2021-04-30 11:23:12,031 INFO] Step 41750/50000; xent: 3.00; lr: 0.0000005;  30 docs/s;  30294 sec\n",
            "[2021-04-30 11:23:47,210 INFO] Step 41800/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  30329 sec\n",
            "[2021-04-30 11:24:22,463 INFO] Step 41850/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  30364 sec\n",
            "[2021-04-30 11:24:40,350 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.99.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:24:59,437 INFO] Step 41900/50000; xent: 3.01; lr: 0.0000005;  28 docs/s;  30401 sec\n",
            "[2021-04-30 11:25:34,594 INFO] Step 41950/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  30436 sec\n",
            "[2021-04-30 11:26:09,729 INFO] Step 42000/50000; xent: 2.93; lr: 0.0000005;  29 docs/s;  30471 sec\n",
            "[2021-04-30 11:26:09,746 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_42000.pt\n",
            "[2021-04-30 11:26:56,438 INFO] Step 42050/50000; xent: 2.99; lr: 0.0000005;  22 docs/s;  30518 sec\n",
            "[2021-04-30 11:27:08,783 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.77.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:27:33,613 INFO] Step 42100/50000; xent: 3.00; lr: 0.0000005;  28 docs/s;  30555 sec\n",
            "[2021-04-30 11:28:08,835 INFO] Step 42150/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  30591 sec\n",
            "[2021-04-30 11:28:44,012 INFO] Step 42200/50000; xent: 3.03; lr: 0.0000005;  30 docs/s;  30626 sec\n",
            "[2021-04-30 11:29:19,317 INFO] Step 42250/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;  30661 sec\n",
            "[2021-04-30 11:29:25,686 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.73.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:29:56,171 INFO] Step 42300/50000; xent: 3.01; lr: 0.0000005;  28 docs/s;  30698 sec\n",
            "[2021-04-30 11:30:31,375 INFO] Step 42350/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;  30733 sec\n",
            "[2021-04-30 11:31:06,440 INFO] Step 42400/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  30768 sec\n",
            "[2021-04-30 11:31:41,695 INFO] Step 42450/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  30803 sec\n",
            "[2021-04-30 11:31:43,441 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.127.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:32:18,792 INFO] Step 42500/50000; xent: 2.93; lr: 0.0000005;  29 docs/s;  30840 sec\n",
            "[2021-04-30 11:32:54,010 INFO] Step 42550/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  30876 sec\n",
            "[2021-04-30 11:33:29,003 INFO] Step 42600/50000; xent: 2.92; lr: 0.0000005;  29 docs/s;  30911 sec\n",
            "[2021-04-30 11:34:01,019 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.143.bert.pt, number of examples: 1084\n",
            "[2021-04-30 11:34:06,522 INFO] Step 42650/50000; xent: 3.01; lr: 0.0000005;  27 docs/s;  30948 sec\n",
            "[2021-04-30 11:34:41,750 INFO] Step 42700/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  30983 sec\n",
            "[2021-04-30 11:35:16,544 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.6.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:35:18,752 INFO] Step 42750/50000; xent: 2.94; lr: 0.0000005;  28 docs/s;  31020 sec\n",
            "[2021-04-30 11:35:53,839 INFO] Step 42800/50000; xent: 3.02; lr: 0.0000005;  31 docs/s;  31056 sec\n",
            "[2021-04-30 11:36:29,060 INFO] Step 42850/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  31091 sec\n",
            "[2021-04-30 11:37:04,313 INFO] Step 42900/50000; xent: 3.06; lr: 0.0000005;  29 docs/s;  31126 sec\n",
            "[2021-04-30 11:37:34,001 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.90.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:37:41,212 INFO] Step 42950/50000; xent: 3.08; lr: 0.0000005;  28 docs/s;  31163 sec\n",
            "[2021-04-30 11:38:16,482 INFO] Step 43000/50000; xent: 3.07; lr: 0.0000005;  30 docs/s;  31198 sec\n",
            "[2021-04-30 11:38:16,501 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_43000.pt\n",
            "[2021-04-30 11:39:01,009 INFO] Step 43050/50000; xent: 2.94; lr: 0.0000005;  23 docs/s;  31243 sec\n",
            "[2021-04-30 11:39:36,203 INFO] Step 43100/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  31278 sec\n",
            "[2021-04-30 11:40:01,465 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.88.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:40:13,599 INFO] Step 43150/50000; xent: 3.01; lr: 0.0000005;  27 docs/s;  31315 sec\n",
            "[2021-04-30 11:40:48,652 INFO] Step 43200/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  31350 sec\n",
            "[2021-04-30 11:41:23,806 INFO] Step 43250/50000; xent: 2.96; lr: 0.0000005;  30 docs/s;  31386 sec\n",
            "[2021-04-30 11:41:59,048 INFO] Step 43300/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  31421 sec\n",
            "[2021-04-30 11:42:19,973 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.69.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:42:36,973 INFO] Step 43350/50000; xent: 2.92; lr: 0.0000005;  27 docs/s;  31459 sec\n",
            "[2021-04-30 11:43:12,233 INFO] Step 43400/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  31494 sec\n",
            "[2021-04-30 11:43:47,217 INFO] Step 43450/50000; xent: 3.05; lr: 0.0000005;  30 docs/s;  31529 sec\n",
            "[2021-04-30 11:44:22,456 INFO] Step 43500/50000; xent: 3.03; lr: 0.0000005;  29 docs/s;  31564 sec\n",
            "[2021-04-30 11:44:36,921 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.102.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:44:59,341 INFO] Step 43550/50000; xent: 2.97; lr: 0.0000005;  28 docs/s;  31601 sec\n",
            "[2021-04-30 11:45:34,499 INFO] Step 43600/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  31636 sec\n",
            "[2021-04-30 11:46:09,723 INFO] Step 43650/50000; xent: 3.01; lr: 0.0000005;  30 docs/s;  31671 sec\n",
            "[2021-04-30 11:46:44,922 INFO] Step 43700/50000; xent: 2.92; lr: 0.0000005;  30 docs/s;  31707 sec\n",
            "[2021-04-30 11:46:53,776 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.98.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:47:21,972 INFO] Step 43750/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  31744 sec\n",
            "[2021-04-30 11:47:57,076 INFO] Step 43800/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  31779 sec\n",
            "[2021-04-30 11:48:32,058 INFO] Step 43850/50000; xent: 2.93; lr: 0.0000005;  30 docs/s;  31814 sec\n",
            "[2021-04-30 11:49:07,028 INFO] Step 43900/50000; xent: 3.00; lr: 0.0000005;  29 docs/s;  31849 sec\n",
            "[2021-04-30 11:49:10,278 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.80.bert.pt, number of examples: 2000\n",
            "[2021-04-30 11:49:43,998 INFO] Step 43950/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  31886 sec\n",
            "[2021-04-30 11:50:19,212 INFO] Step 44000/50000; xent: 3.04; lr: 0.0000005;  29 docs/s;  31921 sec\n",
            "[2021-04-30 11:50:19,232 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_44000.pt\n",
            "[2021-04-30 11:51:03,864 INFO] Step 44050/50000; xent: 2.95; lr: 0.0000005;  24 docs/s;  31966 sec\n",
            "[2021-04-30 11:51:36,981 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.122.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:51:40,607 INFO] Step 44100/50000; xent: 2.95; lr: 0.0000005;  28 docs/s;  32002 sec\n",
            "[2021-04-30 11:52:15,426 INFO] Step 44150/50000; xent: 2.94; lr: 0.0000005;  29 docs/s;  32037 sec\n",
            "[2021-04-30 11:52:50,508 INFO] Step 44200/50000; xent: 2.98; lr: 0.0000005;  30 docs/s;  32072 sec\n",
            "[2021-04-30 11:53:25,658 INFO] Step 44250/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  32107 sec\n",
            "[2021-04-30 11:53:54,614 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.18.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:54:03,184 INFO] Step 44300/50000; xent: 2.95; lr: 0.0000005;  28 docs/s;  32145 sec\n",
            "[2021-04-30 11:54:38,295 INFO] Step 44350/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  32180 sec\n",
            "[2021-04-30 11:55:13,311 INFO] Step 44400/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  32215 sec\n",
            "[2021-04-30 11:55:48,241 INFO] Step 44450/50000; xent: 2.93; lr: 0.0000005;  30 docs/s;  32250 sec\n",
            "[2021-04-30 11:56:12,296 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.36.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:56:26,480 INFO] Step 44500/50000; xent: 2.99; lr: 0.0000005;  27 docs/s;  32288 sec\n",
            "[2021-04-30 11:57:01,437 INFO] Step 44550/50000; xent: 2.92; lr: 0.0000005;  30 docs/s;  32323 sec\n",
            "[2021-04-30 11:57:36,544 INFO] Step 44600/50000; xent: 3.00; lr: 0.0000005;  30 docs/s;  32358 sec\n",
            "[2021-04-30 11:58:11,699 INFO] Step 44650/50000; xent: 2.97; lr: 0.0000005;  30 docs/s;  32393 sec\n",
            "[2021-04-30 11:58:29,749 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.8.bert.pt, number of examples: 2001\n",
            "[2021-04-30 11:58:48,808 INFO] Step 44700/50000; xent: 2.89; lr: 0.0000005;  28 docs/s;  32431 sec\n",
            "[2021-04-30 11:59:23,900 INFO] Step 44750/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  32466 sec\n",
            "[2021-04-30 11:59:59,044 INFO] Step 44800/50000; xent: 3.02; lr: 0.0000005;  29 docs/s;  32501 sec\n",
            "[2021-04-30 12:00:34,070 INFO] Step 44850/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  32536 sec\n",
            "[2021-04-30 12:00:47,000 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.38.bert.pt, number of examples: 2001\n",
            "[2021-04-30 12:01:11,674 INFO] Step 44900/50000; xent: 2.95; lr: 0.0000005;  28 docs/s;  32573 sec\n",
            "[2021-04-30 12:01:46,726 INFO] Step 44950/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  32608 sec\n",
            "[2021-04-30 12:02:21,882 INFO] Step 45000/50000; xent: 3.01; lr: 0.0000005;  30 docs/s;  32644 sec\n",
            "[2021-04-30 12:02:21,899 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_45000.pt\n",
            "[2021-04-30 12:03:06,735 INFO] Step 45050/50000; xent: 3.01; lr: 0.0000005;  23 docs/s;  32688 sec\n",
            "[2021-04-30 12:03:13,228 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.125.bert.pt, number of examples: 2000\n",
            "[2021-04-30 12:03:43,574 INFO] Step 45100/50000; xent: 2.98; lr: 0.0000005;  28 docs/s;  32725 sec\n",
            "[2021-04-30 12:04:18,701 INFO] Step 45150/50000; xent: 2.99; lr: 0.0000005;  29 docs/s;  32760 sec\n",
            "[2021-04-30 12:04:53,547 INFO] Step 45200/50000; xent: 2.99; lr: 0.0000005;  30 docs/s;  32795 sec\n",
            "[2021-04-30 12:05:29,822 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.85.bert.pt, number of examples: 1998\n",
            "[2021-04-30 12:05:30,630 INFO] Step 45250/50000; xent: 2.93; lr: 0.0000005;  29 docs/s;  32832 sec\n",
            "[2021-04-30 12:06:05,716 INFO] Step 45300/50000; xent: 2.93; lr: 0.0000005;  30 docs/s;  32867 sec\n",
            "[2021-04-30 12:06:40,806 INFO] Step 45350/50000; xent: 3.01; lr: 0.0000005;  30 docs/s;  32903 sec\n",
            "[2021-04-30 12:07:15,806 INFO] Step 45400/50000; xent: 3.01; lr: 0.0000005;  29 docs/s;  32938 sec\n",
            "[2021-04-30 12:07:46,375 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.111.bert.pt, number of examples: 2001\n",
            "[2021-04-30 12:07:52,796 INFO] Step 45450/50000; xent: 3.01; lr: 0.0000005;  28 docs/s;  32974 sec\n",
            "[2021-04-30 12:08:27,946 INFO] Step 45500/50000; xent: 2.95; lr: 0.0000005;  29 docs/s;  33010 sec\n",
            "[2021-04-30 12:09:02,953 INFO] Step 45550/50000; xent: 3.00; lr: 0.0000005;  30 docs/s;  33045 sec\n",
            "[2021-04-30 12:09:38,068 INFO] Step 45600/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  33080 sec\n",
            "[2021-04-30 12:10:02,948 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.37.bert.pt, number of examples: 2001\n",
            "[2021-04-30 12:10:14,993 INFO] Step 45650/50000; xent: 2.88; lr: 0.0000005;  29 docs/s;  33117 sec\n",
            "[2021-04-30 12:10:50,051 INFO] Step 45700/50000; xent: 2.95; lr: 0.0000005;  30 docs/s;  33152 sec\n",
            "[2021-04-30 12:11:25,233 INFO] Step 45750/50000; xent: 2.91; lr: 0.0000005;  30 docs/s;  33187 sec\n",
            "[2021-04-30 12:12:00,359 INFO] Step 45800/50000; xent: 2.97; lr: 0.0000005;  29 docs/s;  33222 sec\n",
            "[2021-04-30 12:12:19,616 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.44.bert.pt, number of examples: 2000\n",
            "[2021-04-30 12:12:37,320 INFO] Step 45850/50000; xent: 3.00; lr: 0.0000005;  28 docs/s;  33259 sec\n",
            "[2021-04-30 12:13:12,155 INFO] Step 45900/50000; xent: 2.90; lr: 0.0000005;  30 docs/s;  33294 sec\n",
            "[2021-04-30 12:13:47,255 INFO] Step 45950/50000; xent: 2.98; lr: 0.0000005;  29 docs/s;  33329 sec\n",
            "[2021-04-30 12:14:22,352 INFO] Step 46000/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  33364 sec\n",
            "[2021-04-30 12:14:22,355 INFO] Saving checkpoint ../data/trained_models/Tushar/model_step_46000.pt\n",
            "[2021-04-30 12:14:51,569 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.9.bert.pt, number of examples: 2000\n",
            "[2021-04-30 12:15:15,472 INFO] Step 46050/50000; xent: 2.89; lr: 0.0000005;  20 docs/s;  33417 sec\n",
            "[2021-04-30 12:15:50,411 INFO] Step 46100/50000; xent: 2.92; lr: 0.0000005;  29 docs/s;  33452 sec\n",
            "[2021-04-30 12:16:25,521 INFO] Step 46150/50000; xent: 2.94; lr: 0.0000005;  30 docs/s;  33487 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVVAuNqNocrw"
      },
      "source": [
        "##lr=2e-2 layer=1 (not working)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxr9s7XXlglS",
        "outputId": "49887652-ebb9-4a0d-8a80-ecc0d9f4281a"
      },
      "source": [
        "!python3 train.py -task ext -encoder roberta -mode train -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -ext_dropout 0.1 -model_path ../data/trained_models/reberta_e2 -lr 2e-2 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 5000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_roberta_cnndm -use_interval true -warmup_steps 10000 -max_pos 512 -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "----TRAIN EXTRACTIVE----\n",
            "[2021-05-01 06:21:59,901 INFO] Device ID 0\n",
            "[2021-05-01 06:22:01,758 INFO] Device cuda\n",
            "[2021-05-01 06:22:01,924 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-01 06:22:02,188 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-01 06:22:02,235 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-01 06:22:25,695 INFO] ExtSummarizer(\n",
            "  (bert): Roberta(\n",
            "    (model): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(514, 768)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ext_layer): ExtTransformerEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-05-01 06:22:25,789 INFO] * number of parameters: 130161921\n",
            "[2021-05-01 06:22:25,790 INFO] Start training...\n",
            "[2021-05-01 06:22:30,422 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.123.bert.pt, number of examples: 2000\n",
            "[2021-05-01 06:23:23,716 INFO] Step 50/50000; xent: 3.71; lr: 0.0000010;  19 docs/s;     53 sec\n",
            "[2021-05-01 06:24:21,517 INFO] Step 100/50000; xent: 3.25; lr: 0.0000020;  18 docs/s;    111 sec\n",
            "[2021-05-01 06:25:19,388 INFO] Step 150/50000; xent: 3.17; lr: 0.0000030;  19 docs/s;    169 sec\n",
            "[2021-05-01 06:26:07,949 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.91.bert.pt, number of examples: 1995\n",
            "[2021-05-01 06:26:18,362 INFO] Step 200/50000; xent: 3.08; lr: 0.0000040;  17 docs/s;    228 sec\n",
            "[2021-05-01 06:27:16,237 INFO] Step 250/50000; xent: 2.98; lr: 0.0000050;  18 docs/s;    286 sec\n",
            "[2021-05-01 06:28:14,093 INFO] Step 300/50000; xent: 2.97; lr: 0.0000060;  18 docs/s;    344 sec\n",
            "[2021-05-01 06:29:12,130 INFO] Step 350/50000; xent: 2.94; lr: 0.0000070;  18 docs/s;    402 sec\n",
            "[2021-05-01 06:29:52,136 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.39.bert.pt, number of examples: 2001\n",
            "[2021-05-01 06:30:10,719 INFO] Step 400/50000; xent: 2.92; lr: 0.0000080;  18 docs/s;    460 sec\n",
            "[2021-05-01 06:31:08,365 INFO] Step 450/50000; xent: 2.95; lr: 0.0000090;  18 docs/s;    518 sec\n",
            "[2021-05-01 06:32:06,808 INFO] Step 500/50000; xent: 2.89; lr: 0.0000100;  18 docs/s;    576 sec\n",
            "[2021-05-01 06:33:04,523 INFO] Step 550/50000; xent: 2.86; lr: 0.0000110;  18 docs/s;    634 sec\n",
            "[2021-05-01 06:33:35,710 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.6.bert.pt, number of examples: 2001\n",
            "[2021-05-01 06:34:03,388 INFO] Step 600/50000; xent: 2.92; lr: 0.0000120;  17 docs/s;    693 sec\n",
            "[2021-05-01 06:35:01,477 INFO] Step 650/50000; xent: 2.96; lr: 0.0000130;  18 docs/s;    751 sec\n",
            "[2021-05-01 06:35:59,493 INFO] Step 700/50000; xent: 2.89; lr: 0.0000140;  18 docs/s;    809 sec\n",
            "[2021-05-01 06:36:57,169 INFO] Step 750/50000; xent: 2.83; lr: 0.0000150;  18 docs/s;    867 sec\n",
            "[2021-05-01 06:37:19,995 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.81.bert.pt, number of examples: 2001\n",
            "[2021-05-01 06:37:55,826 INFO] Step 800/50000; xent: 2.79; lr: 0.0000160;  18 docs/s;    925 sec\n",
            "[2021-05-01 06:38:53,852 INFO] Step 850/50000; xent: 2.80; lr: 0.0000170;  18 docs/s;    983 sec\n",
            "[2021-05-01 06:39:51,613 INFO] Step 900/50000; xent: 2.81; lr: 0.0000180;  18 docs/s;   1041 sec\n",
            "[2021-05-01 06:40:49,618 INFO] Step 950/50000; xent: 2.87; lr: 0.0000190;  18 docs/s;   1099 sec\n",
            "[2021-05-01 06:41:03,268 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.98.bert.pt, number of examples: 2001\n",
            "[2021-05-01 06:41:48,376 INFO] Step 1000/50000; xent: 2.73; lr: 0.0000200;  18 docs/s;   1158 sec\n",
            "[2021-05-01 06:42:46,061 INFO] Step 1050/50000; xent: 2.72; lr: 0.0000210;  18 docs/s;   1216 sec\n",
            "[2021-05-01 06:43:44,156 INFO] Step 1100/50000; xent: 2.80; lr: 0.0000220;  18 docs/s;   1274 sec\n",
            "[2021-05-01 06:44:41,529 INFO] Step 1150/50000; xent: 2.83; lr: 0.0000230;  18 docs/s;   1331 sec\n",
            "[2021-05-01 06:44:46,758 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.93.bert.pt, number of examples: 2001\n",
            "[2021-05-01 06:45:41,172 INFO] Step 1200/50000; xent: 2.72; lr: 0.0000240;  18 docs/s;   1391 sec\n",
            "[2021-05-01 06:46:38,921 INFO] Step 1250/50000; xent: 2.74; lr: 0.0000250;  18 docs/s;   1448 sec\n",
            "[2021-05-01 06:47:36,643 INFO] Step 1300/50000; xent: 2.70; lr: 0.0000260;  18 docs/s;   1506 sec\n",
            "[2021-05-01 06:48:32,501 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.63.bert.pt, number of examples: 2001\n",
            "[2021-05-01 06:48:36,049 INFO] Step 1350/50000; xent: 2.79; lr: 0.0000270;  17 docs/s;   1566 sec\n",
            "[2021-05-01 06:49:33,656 INFO] Step 1400/50000; xent: 2.70; lr: 0.0000280;  18 docs/s;   1623 sec\n",
            "[2021-05-01 06:50:31,707 INFO] Step 1450/50000; xent: 2.82; lr: 0.0000290;  18 docs/s;   1681 sec\n",
            "[2021-05-01 06:51:29,600 INFO] Step 1500/50000; xent: 2.79; lr: 0.0000300;  18 docs/s;   1739 sec\n",
            "[2021-05-01 06:52:15,340 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.15.bert.pt, number of examples: 2000\n",
            "[2021-05-01 06:52:28,226 INFO] Step 1550/50000; xent: 2.74; lr: 0.0000310;  18 docs/s;   1798 sec\n",
            "[2021-05-01 06:53:26,020 INFO] Step 1600/50000; xent: 2.79; lr: 0.0000320;  18 docs/s;   1856 sec\n",
            "[2021-05-01 06:54:23,941 INFO] Step 1650/50000; xent: 2.66; lr: 0.0000330;  18 docs/s;   1914 sec\n",
            "[2021-05-01 06:55:21,789 INFO] Step 1700/50000; xent: 2.72; lr: 0.0000340;  18 docs/s;   1971 sec\n",
            "[2021-05-01 06:55:59,560 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.64.bert.pt, number of examples: 1998\n",
            "[2021-05-01 06:56:20,382 INFO] Step 1750/50000; xent: 2.74; lr: 0.0000350;  17 docs/s;   2030 sec\n",
            "[2021-05-01 06:57:18,369 INFO] Step 1800/50000; xent: 2.83; lr: 0.0000360;  18 docs/s;   2088 sec\n",
            "[2021-05-01 06:58:16,480 INFO] Step 1850/50000; xent: 2.64; lr: 0.0000370;  19 docs/s;   2146 sec\n",
            "[2021-05-01 06:59:14,177 INFO] Step 1900/50000; xent: 2.83; lr: 0.0000380;  18 docs/s;   2204 sec\n",
            "[2021-05-01 06:59:42,652 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.28.bert.pt, number of examples: 1999\n",
            "[2021-05-01 07:00:12,307 INFO] Step 1950/50000; xent: 2.71; lr: 0.0000390;  17 docs/s;   2262 sec\n",
            "[2021-05-01 07:01:10,636 INFO] Step 2000/50000; xent: 2.79; lr: 0.0000400;  18 docs/s;   2320 sec\n",
            "[2021-05-01 07:02:08,398 INFO] Step 2050/50000; xent: 2.75; lr: 0.0000410;  18 docs/s;   2378 sec\n",
            "[2021-05-01 07:03:06,181 INFO] Step 2100/50000; xent: 2.73; lr: 0.0000420;  18 docs/s;   2436 sec\n",
            "[2021-05-01 07:03:27,723 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.32.bert.pt, number of examples: 2001\n",
            "[2021-05-01 07:04:04,840 INFO] Step 2150/50000; xent: 2.70; lr: 0.0000430;  17 docs/s;   2494 sec\n",
            "[2021-05-01 07:05:02,838 INFO] Step 2200/50000; xent: 2.69; lr: 0.0000440;  18 docs/s;   2552 sec\n",
            "[2021-05-01 07:06:00,872 INFO] Step 2250/50000; xent: 2.74; lr: 0.0000450;  19 docs/s;   2610 sec\n",
            "[2021-05-01 07:06:58,453 INFO] Step 2300/50000; xent: 2.68; lr: 0.0000460;  18 docs/s;   2668 sec\n",
            "[2021-05-01 07:07:11,111 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.111.bert.pt, number of examples: 2001\n",
            "[2021-05-01 07:07:57,883 INFO] Step 2350/50000; xent: 2.72; lr: 0.0000470;  18 docs/s;   2727 sec\n",
            "[2021-05-01 07:08:55,864 INFO] Step 2400/50000; xent: 2.71; lr: 0.0000480;  18 docs/s;   2785 sec\n",
            "[2021-05-01 07:09:53,901 INFO] Step 2450/50000; xent: 2.71; lr: 0.0000490;  18 docs/s;   2843 sec\n",
            "[2021-05-01 07:10:52,044 INFO] Step 2500/50000; xent: 2.78; lr: 0.0000500;  18 docs/s;   2902 sec\n",
            "[2021-05-01 07:10:54,237 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.49.bert.pt, number of examples: 2000\n",
            "[2021-05-01 07:11:50,792 INFO] Step 2550/50000; xent: 2.67; lr: 0.0000510;  18 docs/s;   2960 sec\n",
            "[2021-05-01 07:12:48,674 INFO] Step 2600/50000; xent: 2.71; lr: 0.0000520;  18 docs/s;   3018 sec\n",
            "[2021-05-01 07:13:46,574 INFO] Step 2650/50000; xent: 2.71; lr: 0.0000530;  18 docs/s;   3076 sec\n",
            "[2021-05-01 07:14:38,331 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.59.bert.pt, number of examples: 2000\n",
            "[2021-05-01 07:14:45,358 INFO] Step 2700/50000; xent: 2.71; lr: 0.0000540;  18 docs/s;   3135 sec\n",
            "[2021-05-01 07:15:43,366 INFO] Step 2750/50000; xent: 2.71; lr: 0.0000550;  18 docs/s;   3193 sec\n",
            "[2021-05-01 07:16:41,317 INFO] Step 2800/50000; xent: 2.74; lr: 0.0000560;  18 docs/s;   3251 sec\n",
            "[2021-05-01 07:17:39,148 INFO] Step 2850/50000; xent: 2.69; lr: 0.0000570;  18 docs/s;   3309 sec\n",
            "[2021-05-01 07:18:22,521 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.84.bert.pt, number of examples: 2001\n",
            "[2021-05-01 07:18:38,801 INFO] Step 2900/50000; xent: 2.71; lr: 0.0000580;  17 docs/s;   3368 sec\n",
            "[2021-05-01 07:19:37,071 INFO] Step 2950/50000; xent: 2.76; lr: 0.0000590;  18 docs/s;   3427 sec\n",
            "[2021-05-01 07:20:35,248 INFO] Step 3000/50000; xent: 2.66; lr: 0.0000600;  18 docs/s;   3485 sec\n",
            "[2021-05-01 07:21:32,887 INFO] Step 3050/50000; xent: 2.68; lr: 0.0000610;  18 docs/s;   3542 sec\n",
            "[2021-05-01 07:22:07,487 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.137.bert.pt, number of examples: 2001\n",
            "[2021-05-01 07:22:31,861 INFO] Step 3100/50000; xent: 2.75; lr: 0.0000620;  18 docs/s;   3601 sec\n",
            "[2021-05-01 07:23:29,919 INFO] Step 3150/50000; xent: 2.77; lr: 0.0000630;  18 docs/s;   3659 sec\n",
            "[2021-05-01 07:24:27,782 INFO] Step 3200/50000; xent: 2.80; lr: 0.0000640;  18 docs/s;   3717 sec\n",
            "[2021-05-01 07:25:25,575 INFO] Step 3250/50000; xent: 2.70; lr: 0.0000650;  18 docs/s;   3775 sec\n",
            "[2021-05-01 07:25:51,810 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.13.bert.pt, number of examples: 2000\n",
            "[2021-05-01 07:26:24,248 INFO] Step 3300/50000; xent: 2.68; lr: 0.0000660;  18 docs/s;   3834 sec\n",
            "[2021-05-01 07:27:21,978 INFO] Step 3350/50000; xent: 2.72; lr: 0.0000670;  18 docs/s;   3892 sec\n",
            "[2021-05-01 07:28:20,281 INFO] Step 3400/50000; xent: 2.72; lr: 0.0000680;  18 docs/s;   3950 sec\n",
            "[2021-05-01 07:29:18,159 INFO] Step 3450/50000; xent: 2.69; lr: 0.0000690;  18 docs/s;   4008 sec\n",
            "[2021-05-01 07:29:34,836 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.135.bert.pt, number of examples: 2000\n",
            "[2021-05-01 07:30:16,491 INFO] Step 3500/50000; xent: 2.75; lr: 0.0000700;  17 docs/s;   4066 sec\n",
            "[2021-05-01 07:31:14,605 INFO] Step 3550/50000; xent: 2.77; lr: 0.0000710;  18 docs/s;   4124 sec\n",
            "[2021-05-01 07:32:12,601 INFO] Step 3600/50000; xent: 2.72; lr: 0.0000720;  18 docs/s;   4182 sec\n",
            "[2021-05-01 07:33:10,554 INFO] Step 3650/50000; xent: 2.72; lr: 0.0000730;  18 docs/s;   4240 sec\n",
            "[2021-05-01 07:33:19,582 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.71.bert.pt, number of examples: 1999\n",
            "[2021-05-01 07:34:09,465 INFO] Step 3700/50000; xent: 2.71; lr: 0.0000740;  17 docs/s;   4299 sec\n",
            "[2021-05-01 07:35:07,490 INFO] Step 3750/50000; xent: 2.71; lr: 0.0000750;  18 docs/s;   4357 sec\n",
            "[2021-05-01 07:36:05,709 INFO] Step 3800/50000; xent: 2.84; lr: 0.0000760;  18 docs/s;   4415 sec\n",
            "[2021-05-01 07:37:02,983 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.9.bert.pt, number of examples: 2000\n",
            "[2021-05-01 07:37:04,205 INFO] Step 3850/50000; xent: 2.73; lr: 0.0000770;  18 docs/s;   4474 sec\n",
            "[2021-05-01 07:38:02,031 INFO] Step 3900/50000; xent: 2.68; lr: 0.0000780;  18 docs/s;   4532 sec\n",
            "[2021-05-01 07:38:59,895 INFO] Step 3950/50000; xent: 2.83; lr: 0.0000790;  18 docs/s;   4589 sec\n",
            "[2021-05-01 07:39:57,771 INFO] Step 4000/50000; xent: 2.71; lr: 0.0000800;  18 docs/s;   4647 sec\n",
            "[2021-05-01 07:40:47,218 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.105.bert.pt, number of examples: 2000\n",
            "[2021-05-01 07:40:56,427 INFO] Step 4050/50000; xent: 2.70; lr: 0.0000810;  18 docs/s;   4706 sec\n",
            "[2021-05-01 07:41:54,482 INFO] Step 4100/50000; xent: 2.78; lr: 0.0000820;  18 docs/s;   4764 sec\n",
            "[2021-05-01 07:42:52,844 INFO] Step 4150/50000; xent: 2.88; lr: 0.0000830;  18 docs/s;   4822 sec\n",
            "[2021-05-01 07:43:50,710 INFO] Step 4200/50000; xent: 2.86; lr: 0.0000840;  18 docs/s;   4880 sec\n",
            "[2021-05-01 07:44:32,341 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.50.bert.pt, number of examples: 1999\n",
            "[2021-05-01 07:44:49,757 INFO] Step 4250/50000; xent: 2.78; lr: 0.0000850;  17 docs/s;   4939 sec\n",
            "[2021-05-01 07:45:47,868 INFO] Step 4300/50000; xent: 2.79; lr: 0.0000860;  18 docs/s;   4997 sec\n",
            "[2021-05-01 07:46:46,273 INFO] Step 4350/50000; xent: 2.82; lr: 0.0000870;  18 docs/s;   5056 sec\n",
            "[2021-05-01 07:47:44,262 INFO] Step 4400/50000; xent: 2.72; lr: 0.0000880;  18 docs/s;   5114 sec\n",
            "[2021-05-01 07:48:17,638 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.106.bert.pt, number of examples: 2001\n",
            "[2021-05-01 07:48:43,324 INFO] Step 4450/50000; xent: 2.75; lr: 0.0000890;  18 docs/s;   5173 sec\n",
            "[2021-05-01 07:49:42,036 INFO] Step 4500/50000; xent: 2.75; lr: 0.0000900;  18 docs/s;   5232 sec\n",
            "[2021-05-01 07:50:39,829 INFO] Step 4550/50000; xent: 2.80; lr: 0.0000910;  18 docs/s;   5289 sec\n",
            "[2021-05-01 07:51:37,249 INFO] Step 4600/50000; xent: 2.75; lr: 0.0000920;  18 docs/s;   5347 sec\n",
            "[2021-05-01 07:52:01,410 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.38.bert.pt, number of examples: 2001\n",
            "[2021-05-01 07:52:36,456 INFO] Step 4650/50000; xent: 2.67; lr: 0.0000930;  18 docs/s;   5406 sec\n",
            "[2021-05-01 07:53:34,521 INFO] Step 4700/50000; xent: 2.74; lr: 0.0000940;  18 docs/s;   5464 sec\n",
            "[2021-05-01 07:54:32,897 INFO] Step 4750/50000; xent: 2.74; lr: 0.0000950;  18 docs/s;   5522 sec\n",
            "[2021-05-01 07:55:31,222 INFO] Step 4800/50000; xent: 2.80; lr: 0.0000960;  18 docs/s;   5581 sec\n",
            "[2021-05-01 07:55:47,441 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.24.bert.pt, number of examples: 1999\n",
            "[2021-05-01 07:56:30,692 INFO] Step 4850/50000; xent: 2.70; lr: 0.0000970;  17 docs/s;   5640 sec\n",
            "[2021-05-01 07:57:28,888 INFO] Step 4900/50000; xent: 2.80; lr: 0.0000980;  18 docs/s;   5698 sec\n",
            "[2021-05-01 07:58:27,319 INFO] Step 4950/50000; xent: 2.76; lr: 0.0000990;  18 docs/s;   5757 sec\n",
            "[2021-05-01 07:59:25,368 INFO] Step 5000/50000; xent: 2.75; lr: 0.0001000;  18 docs/s;   5815 sec\n",
            "[2021-05-01 07:59:25,371 INFO] Saving checkpoint ../data/trained_models/reberta_e2/model_step_5000.pt\n",
            "[2021-05-01 07:59:39,396 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.143.bert.pt, number of examples: 1084\n",
            "[2021-05-01 08:00:32,604 INFO] Step 5050/50000; xent: 2.75; lr: 0.0001010;  16 docs/s;   5882 sec\n",
            "[2021-05-01 08:01:30,257 INFO] Step 5100/50000; xent: 2.76; lr: 0.0001020;  18 docs/s;   5940 sec\n",
            "[2021-05-01 08:01:41,776 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.94.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:02:29,742 INFO] Step 5150/50000; xent: 2.74; lr: 0.0001030;  17 docs/s;   5999 sec\n",
            "[2021-05-01 08:03:27,760 INFO] Step 5200/50000; xent: 2.77; lr: 0.0001040;  18 docs/s;   6057 sec\n",
            "[2021-05-01 08:04:25,820 INFO] Step 5250/50000; xent: 2.81; lr: 0.0001050;  18 docs/s;   6115 sec\n",
            "[2021-05-01 08:05:23,610 INFO] Step 5300/50000; xent: 2.66; lr: 0.0001060;  18 docs/s;   6173 sec\n",
            "[2021-05-01 08:05:26,900 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.62.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:06:22,894 INFO] Step 5350/50000; xent: 2.80; lr: 0.0001070;  18 docs/s;   6232 sec\n",
            "[2021-05-01 08:07:21,136 INFO] Step 5400/50000; xent: 2.84; lr: 0.0001080;  18 docs/s;   6291 sec\n",
            "[2021-05-01 08:08:19,438 INFO] Step 5450/50000; xent: 2.72; lr: 0.0001090;  18 docs/s;   6349 sec\n",
            "[2021-05-01 08:09:11,438 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.139.bert.pt, number of examples: 2000\n",
            "[2021-05-01 08:09:18,438 INFO] Step 5500/50000; xent: 2.77; lr: 0.0001100;  17 docs/s;   6408 sec\n",
            "[2021-05-01 08:10:16,690 INFO] Step 5550/50000; xent: 2.76; lr: 0.0001110;  18 docs/s;   6466 sec\n",
            "[2021-05-01 08:11:14,576 INFO] Step 5600/50000; xent: 2.73; lr: 0.0001120;  18 docs/s;   6524 sec\n",
            "[2021-05-01 08:12:13,133 INFO] Step 5650/50000; xent: 2.76; lr: 0.0001130;  18 docs/s;   6583 sec\n",
            "[2021-05-01 08:12:57,394 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.88.bert.pt, number of examples: 2000\n",
            "[2021-05-01 08:13:12,618 INFO] Step 5700/50000; xent: 2.81; lr: 0.0001140;  18 docs/s;   6642 sec\n",
            "[2021-05-01 08:14:10,590 INFO] Step 5750/50000; xent: 2.80; lr: 0.0001150;  17 docs/s;   6700 sec\n",
            "[2021-05-01 08:15:08,933 INFO] Step 5800/50000; xent: 2.77; lr: 0.0001160;  18 docs/s;   6759 sec\n",
            "[2021-05-01 08:16:07,250 INFO] Step 5850/50000; xent: 3.06; lr: 0.0001170;  18 docs/s;   6817 sec\n",
            "[2021-05-01 08:16:42,620 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.92.bert.pt, number of examples: 1998\n",
            "[2021-05-01 08:17:05,883 INFO] Step 5900/50000; xent: 3.13; lr: 0.0001180;  17 docs/s;   6875 sec\n",
            "[2021-05-01 08:18:04,184 INFO] Step 5950/50000; xent: 3.00; lr: 0.0001190;  18 docs/s;   6934 sec\n",
            "[2021-05-01 08:19:01,932 INFO] Step 6000/50000; xent: 2.98; lr: 0.0001200;  18 docs/s;   6992 sec\n",
            "[2021-05-01 08:20:00,170 INFO] Step 6050/50000; xent: 2.93; lr: 0.0001210;  19 docs/s;   7050 sec\n",
            "[2021-05-01 08:20:26,590 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.68.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:20:58,985 INFO] Step 6100/50000; xent: 2.97; lr: 0.0001220;  17 docs/s;   7109 sec\n",
            "[2021-05-01 08:21:57,097 INFO] Step 6150/50000; xent: 2.94; lr: 0.0001230;  18 docs/s;   7167 sec\n",
            "[2021-05-01 08:22:55,049 INFO] Step 6200/50000; xent: 3.13; lr: 0.0001240;  18 docs/s;   7225 sec\n",
            "[2021-05-01 08:23:53,056 INFO] Step 6250/50000; xent: 2.93; lr: 0.0001250;  18 docs/s;   7283 sec\n",
            "[2021-05-01 08:24:11,165 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.0.bert.pt, number of examples: 2000\n",
            "[2021-05-01 08:24:51,579 INFO] Step 6300/50000; xent: 2.95; lr: 0.0001260;  18 docs/s;   7341 sec\n",
            "[2021-05-01 08:25:49,754 INFO] Step 6350/50000; xent: 2.96; lr: 0.0001270;  18 docs/s;   7399 sec\n",
            "[2021-05-01 08:26:47,876 INFO] Step 6400/50000; xent: 2.99; lr: 0.0001280;  18 docs/s;   7457 sec\n",
            "[2021-05-01 08:27:45,505 INFO] Step 6450/50000; xent: 3.02; lr: 0.0001290;  18 docs/s;   7515 sec\n",
            "[2021-05-01 08:27:55,518 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.83.bert.pt, number of examples: 1999\n",
            "[2021-05-01 08:28:44,516 INFO] Step 6500/50000; xent: 3.00; lr: 0.0001300;  18 docs/s;   7574 sec\n",
            "[2021-05-01 08:29:42,139 INFO] Step 6550/50000; xent: 2.97; lr: 0.0001310;  18 docs/s;   7632 sec\n",
            "[2021-05-01 08:30:40,626 INFO] Step 6600/50000; xent: 2.99; lr: 0.0001320;  18 docs/s;   7690 sec\n",
            "[2021-05-01 08:31:38,141 INFO] Step 6650/50000; xent: 2.95; lr: 0.0001330;  18 docs/s;   7748 sec\n",
            "[2021-05-01 08:31:39,272 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.138.bert.pt, number of examples: 2000\n",
            "[2021-05-01 08:32:37,411 INFO] Step 6700/50000; xent: 3.00; lr: 0.0001340;  17 docs/s;   7807 sec\n",
            "[2021-05-01 08:33:35,931 INFO] Step 6750/50000; xent: 2.97; lr: 0.0001350;  19 docs/s;   7866 sec\n",
            "[2021-05-01 08:34:33,862 INFO] Step 6800/50000; xent: 3.01; lr: 0.0001360;  18 docs/s;   7923 sec\n",
            "[2021-05-01 08:35:23,188 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.72.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:35:32,489 INFO] Step 6850/50000; xent: 2.96; lr: 0.0001370;  18 docs/s;   7982 sec\n",
            "[2021-05-01 08:36:30,194 INFO] Step 6900/50000; xent: 3.01; lr: 0.0001380;  18 docs/s;   8040 sec\n",
            "[2021-05-01 08:37:28,202 INFO] Step 6950/50000; xent: 3.07; lr: 0.0001390;  18 docs/s;   8098 sec\n",
            "[2021-05-01 08:38:26,014 INFO] Step 7000/50000; xent: 2.93; lr: 0.0001400;  18 docs/s;   8156 sec\n",
            "[2021-05-01 08:39:07,253 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.89.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:39:24,533 INFO] Step 7050/50000; xent: 2.91; lr: 0.0001410;  17 docs/s;   8214 sec\n",
            "[2021-05-01 08:40:22,308 INFO] Step 7100/50000; xent: 2.95; lr: 0.0001420;  18 docs/s;   8272 sec\n",
            "[2021-05-01 08:41:20,080 INFO] Step 7150/50000; xent: 3.00; lr: 0.0001430;  18 docs/s;   8330 sec\n",
            "[2021-05-01 08:42:17,876 INFO] Step 7200/50000; xent: 2.99; lr: 0.0001440;  18 docs/s;   8387 sec\n",
            "[2021-05-01 08:42:52,154 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.51.bert.pt, number of examples: 1999\n",
            "[2021-05-01 08:43:17,580 INFO] Step 7250/50000; xent: 2.97; lr: 0.0001450;  18 docs/s;   8447 sec\n",
            "[2021-05-01 08:44:15,604 INFO] Step 7300/50000; xent: 3.04; lr: 0.0001460;  18 docs/s;   8505 sec\n",
            "[2021-05-01 08:45:12,732 INFO] Step 7350/50000; xent: 2.98; lr: 0.0001470;  18 docs/s;   8562 sec\n",
            "[2021-05-01 08:46:10,554 INFO] Step 7400/50000; xent: 2.89; lr: 0.0001480;  18 docs/s;   8620 sec\n",
            "[2021-05-01 08:46:37,633 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.35.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:47:10,014 INFO] Step 7450/50000; xent: 2.92; lr: 0.0001490;  17 docs/s;   8680 sec\n",
            "[2021-05-01 08:48:07,934 INFO] Step 7500/50000; xent: 3.01; lr: 0.0001500;  18 docs/s;   8738 sec\n",
            "[2021-05-01 08:49:05,434 INFO] Step 7550/50000; xent: 2.99; lr: 0.0001510;  18 docs/s;   8795 sec\n",
            "[2021-05-01 08:50:03,540 INFO] Step 7600/50000; xent: 3.02; lr: 0.0001520;  18 docs/s;   8853 sec\n",
            "[2021-05-01 08:50:20,770 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.37.bert.pt, number of examples: 2001\n",
            "[2021-05-01 08:51:02,053 INFO] Step 7650/50000; xent: 3.02; lr: 0.0001530;  18 docs/s;   8912 sec\n",
            "[2021-05-01 08:51:59,770 INFO] Step 7700/50000; xent: 2.92; lr: 0.0001540;  18 docs/s;   8969 sec\n",
            "[2021-05-01 08:52:57,661 INFO] Step 7750/50000; xent: 2.93; lr: 0.0001550;  18 docs/s;   9027 sec\n",
            "[2021-05-01 08:53:55,414 INFO] Step 7800/50000; xent: 2.96; lr: 0.0001560;  18 docs/s;   9085 sec\n",
            "[2021-05-01 08:54:03,366 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.80.bert.pt, number of examples: 2000\n",
            "[2021-05-01 08:54:54,280 INFO] Step 7850/50000; xent: 3.00; lr: 0.0001570;  18 docs/s;   9144 sec\n",
            "[2021-05-01 08:55:52,066 INFO] Step 7900/50000; xent: 2.98; lr: 0.0001580;  18 docs/s;   9202 sec\n",
            "[2021-05-01 08:56:50,025 INFO] Step 7950/50000; xent: 3.04; lr: 0.0001590;  18 docs/s;   9260 sec\n",
            "[2021-05-01 08:57:46,846 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.52.bert.pt, number of examples: 1999\n",
            "[2021-05-01 08:57:48,079 INFO] Step 8000/50000; xent: 2.94; lr: 0.0001600;  18 docs/s;   9318 sec\n",
            "[2021-05-01 08:58:45,567 INFO] Step 8050/50000; xent: 2.92; lr: 0.0001610;  18 docs/s;   9375 sec\n",
            "[2021-05-01 08:59:43,068 INFO] Step 8100/50000; xent: 2.92; lr: 0.0001620;  18 docs/s;   9433 sec\n",
            "[2021-05-01 09:00:40,904 INFO] Step 8150/50000; xent: 2.96; lr: 0.0001630;  18 docs/s;   9490 sec\n",
            "[2021-05-01 09:01:29,214 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.12.bert.pt, number of examples: 1998\n",
            "[2021-05-01 09:01:39,657 INFO] Step 8200/50000; xent: 3.05; lr: 0.0001640;  18 docs/s;   9549 sec\n",
            "[2021-05-01 09:02:37,274 INFO] Step 8250/50000; xent: 2.93; lr: 0.0001650;  18 docs/s;   9607 sec\n",
            "[2021-05-01 09:03:35,310 INFO] Step 8300/50000; xent: 3.00; lr: 0.0001660;  18 docs/s;   9665 sec\n",
            "[2021-05-01 09:04:33,137 INFO] Step 8350/50000; xent: 2.97; lr: 0.0001670;  18 docs/s;   9723 sec\n",
            "[2021-05-01 09:05:11,886 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.114.bert.pt, number of examples: 2001\n",
            "[2021-05-01 09:05:31,570 INFO] Step 8400/50000; xent: 2.96; lr: 0.0001680;  18 docs/s;   9781 sec\n",
            "[2021-05-01 09:06:29,200 INFO] Step 8450/50000; xent: 3.02; lr: 0.0001690;  18 docs/s;   9839 sec\n",
            "[2021-05-01 09:07:26,712 INFO] Step 8500/50000; xent: 2.97; lr: 0.0001700;  18 docs/s;   9896 sec\n",
            "[2021-05-01 09:08:24,637 INFO] Step 8550/50000; xent: 3.04; lr: 0.0001710;  18 docs/s;   9954 sec\n",
            "[2021-05-01 09:08:55,152 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.86.bert.pt, number of examples: 1999\n",
            "[2021-05-01 09:09:22,826 INFO] Step 8600/50000; xent: 2.99; lr: 0.0001720;  18 docs/s;  10012 sec\n",
            "[2021-05-01 09:10:20,553 INFO] Step 8650/50000; xent: 3.03; lr: 0.0001730;  18 docs/s;  10070 sec\n",
            "[2021-05-01 09:11:18,098 INFO] Step 8700/50000; xent: 3.02; lr: 0.0001740;  18 docs/s;  10128 sec\n",
            "[2021-05-01 09:12:16,013 INFO] Step 8750/50000; xent: 2.89; lr: 0.0001750;  18 docs/s;  10186 sec\n",
            "[2021-05-01 09:12:37,577 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.113.bert.pt, number of examples: 2000\n",
            "[2021-05-01 09:13:14,362 INFO] Step 8800/50000; xent: 3.00; lr: 0.0001760;  18 docs/s;  10244 sec\n",
            "[2021-05-01 09:14:12,140 INFO] Step 8850/50000; xent: 2.93; lr: 0.0001770;  18 docs/s;  10302 sec\n",
            "[2021-05-01 09:15:10,200 INFO] Step 8900/50000; xent: 3.03; lr: 0.0001780;  18 docs/s;  10360 sec\n",
            "[2021-05-01 09:16:07,699 INFO] Step 8950/50000; xent: 3.00; lr: 0.0001790;  18 docs/s;  10417 sec\n",
            "[2021-05-01 09:16:21,430 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.96.bert.pt, number of examples: 1999\n",
            "[2021-05-01 09:17:06,831 INFO] Step 9000/50000; xent: 2.98; lr: 0.0001800;  18 docs/s;  10476 sec\n",
            "[2021-05-01 09:18:04,288 INFO] Step 9050/50000; xent: 2.99; lr: 0.0001810;  18 docs/s;  10534 sec\n",
            "[2021-05-01 09:19:01,876 INFO] Step 9100/50000; xent: 3.01; lr: 0.0001820;  18 docs/s;  10591 sec\n",
            "[2021-05-01 09:19:59,666 INFO] Step 9150/50000; xent: 3.01; lr: 0.0001830;  18 docs/s;  10649 sec\n",
            "[2021-05-01 09:20:04,213 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.131.bert.pt, number of examples: 2001\n",
            "[2021-05-01 09:20:58,438 INFO] Step 9200/50000; xent: 3.04; lr: 0.0001840;  18 docs/s;  10708 sec\n",
            "[2021-05-01 09:21:56,383 INFO] Step 9250/50000; xent: 3.04; lr: 0.0001850;  18 docs/s;  10766 sec\n",
            "[2021-05-01 09:22:54,056 INFO] Step 9300/50000; xent: 2.97; lr: 0.0001860;  18 docs/s;  10824 sec\n",
            "[2021-05-01 09:23:46,901 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.97.bert.pt, number of examples: 2000\n",
            "[2021-05-01 09:23:52,700 INFO] Step 9350/50000; xent: 3.05; lr: 0.0001870;  18 docs/s;  10882 sec\n",
            "[2021-05-01 09:24:50,448 INFO] Step 9400/50000; xent: 2.97; lr: 0.0001880;  18 docs/s;  10940 sec\n",
            "[2021-05-01 09:25:48,495 INFO] Step 9450/50000; xent: 2.94; lr: 0.0001890;  18 docs/s;  10998 sec\n",
            "[2021-05-01 09:26:46,217 INFO] Step 9500/50000; xent: 3.04; lr: 0.0001900;  18 docs/s;  11056 sec\n",
            "[2021-05-01 09:27:32,029 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.55.bert.pt, number of examples: 1999\n",
            "[2021-05-01 09:27:44,734 INFO] Step 9550/50000; xent: 3.03; lr: 0.0001910;  17 docs/s;  11114 sec\n",
            "[2021-05-01 09:28:42,763 INFO] Step 9600/50000; xent: 3.00; lr: 0.0001920;  18 docs/s;  11172 sec\n",
            "[2021-05-01 09:29:40,562 INFO] Step 9650/50000; xent: 2.97; lr: 0.0001930;  18 docs/s;  11230 sec\n",
            "[2021-05-01 09:30:38,242 INFO] Step 9700/50000; xent: 2.98; lr: 0.0001940;  18 docs/s;  11288 sec\n",
            "[2021-05-01 09:31:13,933 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.128.bert.pt, number of examples: 2000\n",
            "[2021-05-01 09:31:37,064 INFO] Step 9750/50000; xent: 3.01; lr: 0.0001950;  17 docs/s;  11347 sec\n",
            "[2021-05-01 09:32:35,273 INFO] Step 9800/50000; xent: 2.94; lr: 0.0001960;  18 docs/s;  11405 sec\n",
            "[2021-05-01 09:33:32,813 INFO] Step 9850/50000; xent: 3.01; lr: 0.0001970;  18 docs/s;  11462 sec\n",
            "[2021-05-01 09:34:30,481 INFO] Step 9900/50000; xent: 2.94; lr: 0.0001980;  18 docs/s;  11520 sec\n",
            "[2021-05-01 09:34:56,350 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.104.bert.pt, number of examples: 2000\n",
            "[2021-05-01 09:35:28,504 INFO] Step 9950/50000; xent: 2.93; lr: 0.0001990;  18 docs/s;  11578 sec\n",
            "[2021-05-01 09:36:26,316 INFO] Step 10000/50000; xent: 3.00; lr: 0.0002000;  18 docs/s;  11636 sec\n",
            "[2021-05-01 09:36:26,331 INFO] Saving checkpoint ../data/trained_models/reberta_e2/model_step_10000.pt\n",
            "[2021-05-01 09:37:31,217 INFO] Step 10050/50000; xent: 2.93; lr: 0.0001995;  17 docs/s;  11701 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 147, in <module>\n",
            "    train_ext(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 204, in train_ext\n",
            "    train_single_ext(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 247, in train_single_ext\n",
            "    trainer.train(train_iter_fct, args.train_steps)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 152, in train\n",
            "    report_stats)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 312, in _gradient_accumulation\n",
            "    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/model_builder.py\", line 280, in forward\n",
            "    sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqD8xUvkeBE4"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## lr=2e-3 layer=1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOklnVvtSz_s",
        "outputId": "9cdf2883-8299-4c5a-a423-71fb7bb26a24"
      },
      "source": [
        "!python3 train.py -task ext -encoder roberta -mode train -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -ext_dropout 0.1 -model_path ../data/trained_models/reberta_smaller -lr 2e-3 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_roberta_cnndm -use_interval true -warmup_steps 10000 -max_pos 512 -ext_layers 1 -train_from ../data/trained_models/reberta_smaller/model_step_35000.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "----TRAIN EXTRACTIVE----\n",
            "[2021-05-01 09:41:13,162 INFO] Device ID 0\n",
            "[2021-05-01 09:41:13,162 INFO] Device cuda\n",
            "[2021-05-01 09:41:13,163 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_35000.pt\n",
            "---TRAINING FROM:  ../data/trained_models/reberta_smaller/model_step_35000.pt\n",
            "[2021-05-01 09:41:57,210 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-01 09:41:57,211 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-01 09:41:57,249 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-01 09:42:19,472 INFO] ExtSummarizer(\n",
            "  (bert): Roberta(\n",
            "    (model): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(514, 768)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ext_layer): ExtTransformerEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-05-01 09:42:19,552 INFO] * number of parameters: 130161921\n",
            "[2021-05-01 09:42:19,552 INFO] Start training...\n",
            "[2021-05-01 09:42:20,076 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.123.bert.pt, number of examples: 2000\n",
            "[2021-05-01 09:43:13,942 INFO] Step 35050/50000; xent: 2.66; lr: 0.0000107;  19 docs/s;     54 sec\n",
            "[2021-05-01 09:44:12,274 INFO] Step 35100/50000; xent: 2.78; lr: 0.0000107;  18 docs/s;    112 sec\n",
            "[2021-05-01 09:45:09,929 INFO] Step 35150/50000; xent: 2.85; lr: 0.0000107;  19 docs/s;    170 sec\n",
            "[2021-05-01 09:45:58,229 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.91.bert.pt, number of examples: 1995\n",
            "[2021-05-01 09:46:08,643 INFO] Step 35200/50000; xent: 2.86; lr: 0.0000107;  17 docs/s;    229 sec\n",
            "[2021-05-01 09:47:06,509 INFO] Step 35250/50000; xent: 2.76; lr: 0.0000107;  18 docs/s;    286 sec\n",
            "[2021-05-01 09:48:04,257 INFO] Step 35300/50000; xent: 2.74; lr: 0.0000106;  18 docs/s;    344 sec\n",
            "[2021-05-01 09:49:02,399 INFO] Step 35350/50000; xent: 2.71; lr: 0.0000106;  18 docs/s;    402 sec\n",
            "[2021-05-01 09:49:42,183 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.39.bert.pt, number of examples: 2001\n",
            "[2021-05-01 09:50:00,872 INFO] Step 35400/50000; xent: 2.72; lr: 0.0000106;  18 docs/s;    461 sec\n",
            "[2021-05-01 09:50:58,651 INFO] Step 35450/50000; xent: 2.69; lr: 0.0000106;  18 docs/s;    519 sec\n",
            "[2021-05-01 09:51:57,043 INFO] Step 35500/50000; xent: 2.56; lr: 0.0000106;  18 docs/s;    577 sec\n",
            "[2021-05-01 09:52:54,738 INFO] Step 35550/50000; xent: 2.64; lr: 0.0000106;  18 docs/s;    635 sec\n",
            "[2021-05-01 09:53:25,450 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.6.bert.pt, number of examples: 2001\n",
            "[2021-05-01 09:53:53,180 INFO] Step 35600/50000; xent: 2.61; lr: 0.0000106;  18 docs/s;    693 sec\n",
            "[2021-05-01 09:54:51,271 INFO] Step 35650/50000; xent: 2.69; lr: 0.0000106;  18 docs/s;    751 sec\n",
            "[2021-05-01 09:55:49,227 INFO] Step 35700/50000; xent: 2.68; lr: 0.0000106;  18 docs/s;    809 sec\n",
            "[2021-05-01 09:56:46,903 INFO] Step 35750/50000; xent: 2.60; lr: 0.0000106;  18 docs/s;    867 sec\n",
            "[2021-05-01 09:57:09,187 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.81.bert.pt, number of examples: 2001\n",
            "[2021-05-01 09:57:45,141 INFO] Step 35800/50000; xent: 2.54; lr: 0.0000106;  18 docs/s;    925 sec\n",
            "[2021-05-01 09:58:43,192 INFO] Step 35850/50000; xent: 2.62; lr: 0.0000106;  18 docs/s;    983 sec\n",
            "[2021-05-01 09:59:40,934 INFO] Step 35900/50000; xent: 2.59; lr: 0.0000106;  18 docs/s;   1041 sec\n",
            "[2021-05-01 10:00:38,966 INFO] Step 35950/50000; xent: 2.67; lr: 0.0000105;  18 docs/s;   1099 sec\n",
            "[2021-05-01 10:00:52,369 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.98.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:01:37,521 INFO] Step 36000/50000; xent: 2.50; lr: 0.0000105;  18 docs/s;   1157 sec\n",
            "[2021-05-01 10:01:37,536 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_36000.pt\n",
            "[2021-05-01 10:02:42,917 INFO] Step 36050/50000; xent: 2.53; lr: 0.0000105;  16 docs/s;   1223 sec\n",
            "[2021-05-01 10:03:40,849 INFO] Step 36100/50000; xent: 2.60; lr: 0.0000105;  19 docs/s;   1281 sec\n",
            "[2021-05-01 10:04:38,331 INFO] Step 36150/50000; xent: 2.63; lr: 0.0000105;  18 docs/s;   1338 sec\n",
            "[2021-05-01 10:04:42,684 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.93.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:05:37,064 INFO] Step 36200/50000; xent: 2.55; lr: 0.0000105;  18 docs/s;   1397 sec\n",
            "[2021-05-01 10:06:35,006 INFO] Step 36250/50000; xent: 2.54; lr: 0.0000105;  18 docs/s;   1455 sec\n",
            "[2021-05-01 10:07:32,933 INFO] Step 36300/50000; xent: 2.52; lr: 0.0000105;  18 docs/s;   1513 sec\n",
            "[2021-05-01 10:08:28,354 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.63.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:08:31,882 INFO] Step 36350/50000; xent: 2.65; lr: 0.0000105;  17 docs/s;   1572 sec\n",
            "[2021-05-01 10:09:29,456 INFO] Step 36400/50000; xent: 2.53; lr: 0.0000105;  18 docs/s;   1629 sec\n",
            "[2021-05-01 10:10:27,536 INFO] Step 36450/50000; xent: 2.63; lr: 0.0000105;  18 docs/s;   1687 sec\n",
            "[2021-05-01 10:11:25,580 INFO] Step 36500/50000; xent: 2.59; lr: 0.0000105;  18 docs/s;   1746 sec\n",
            "[2021-05-01 10:12:11,465 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.15.bert.pt, number of examples: 2000\n",
            "[2021-05-01 10:12:24,362 INFO] Step 36550/50000; xent: 2.61; lr: 0.0000105;  18 docs/s;   1804 sec\n",
            "[2021-05-01 10:13:22,334 INFO] Step 36600/50000; xent: 2.61; lr: 0.0000105;  18 docs/s;   1862 sec\n",
            "[2021-05-01 10:14:20,140 INFO] Step 36650/50000; xent: 2.48; lr: 0.0000104;  18 docs/s;   1920 sec\n",
            "[2021-05-01 10:15:18,072 INFO] Step 36700/50000; xent: 2.57; lr: 0.0000104;  18 docs/s;   1978 sec\n",
            "[2021-05-01 10:15:55,915 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.64.bert.pt, number of examples: 1998\n",
            "[2021-05-01 10:16:16,789 INFO] Step 36750/50000; xent: 2.59; lr: 0.0000104;  17 docs/s;   2037 sec\n",
            "[2021-05-01 10:17:14,718 INFO] Step 36800/50000; xent: 2.67; lr: 0.0000104;  18 docs/s;   2095 sec\n",
            "[2021-05-01 10:18:12,908 INFO] Step 36850/50000; xent: 2.47; lr: 0.0000104;  19 docs/s;   2153 sec\n",
            "[2021-05-01 10:19:10,829 INFO] Step 36900/50000; xent: 2.67; lr: 0.0000104;  18 docs/s;   2211 sec\n",
            "[2021-05-01 10:19:39,333 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.28.bert.pt, number of examples: 1999\n",
            "[2021-05-01 10:20:08,984 INFO] Step 36950/50000; xent: 2.54; lr: 0.0000104;  17 docs/s;   2269 sec\n",
            "[2021-05-01 10:21:07,457 INFO] Step 37000/50000; xent: 2.58; lr: 0.0000104;  18 docs/s;   2327 sec\n",
            "[2021-05-01 10:21:07,472 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_37000.pt\n",
            "[2021-05-01 10:22:12,611 INFO] Step 37050/50000; xent: 2.49; lr: 0.0000104;  16 docs/s;   2393 sec\n",
            "[2021-05-01 10:23:10,366 INFO] Step 37100/50000; xent: 2.47; lr: 0.0000104;  18 docs/s;   2450 sec\n",
            "[2021-05-01 10:23:32,114 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.32.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:24:09,426 INFO] Step 37150/50000; xent: 2.53; lr: 0.0000104;  17 docs/s;   2509 sec\n",
            "[2021-05-01 10:25:07,517 INFO] Step 37200/50000; xent: 2.48; lr: 0.0000104;  18 docs/s;   2567 sec\n",
            "[2021-05-01 10:26:05,480 INFO] Step 37250/50000; xent: 2.56; lr: 0.0000104;  19 docs/s;   2625 sec\n",
            "[2021-05-01 10:27:02,974 INFO] Step 37300/50000; xent: 2.53; lr: 0.0000104;  18 docs/s;   2683 sec\n",
            "[2021-05-01 10:27:15,558 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.111.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:28:02,286 INFO] Step 37350/50000; xent: 2.57; lr: 0.0000103;  18 docs/s;   2742 sec\n",
            "[2021-05-01 10:29:00,352 INFO] Step 37400/50000; xent: 2.57; lr: 0.0000103;  18 docs/s;   2800 sec\n",
            "[2021-05-01 10:29:58,384 INFO] Step 37450/50000; xent: 2.57; lr: 0.0000103;  18 docs/s;   2858 sec\n",
            "[2021-05-01 10:30:56,739 INFO] Step 37500/50000; xent: 2.63; lr: 0.0000103;  18 docs/s;   2917 sec\n",
            "[2021-05-01 10:30:58,874 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.49.bert.pt, number of examples: 2000\n",
            "[2021-05-01 10:31:55,339 INFO] Step 37550/50000; xent: 2.55; lr: 0.0000103;  18 docs/s;   2975 sec\n",
            "[2021-05-01 10:32:53,241 INFO] Step 37600/50000; xent: 2.54; lr: 0.0000103;  18 docs/s;   3033 sec\n",
            "[2021-05-01 10:33:51,244 INFO] Step 37650/50000; xent: 2.52; lr: 0.0000103;  18 docs/s;   3091 sec\n",
            "[2021-05-01 10:34:42,898 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.59.bert.pt, number of examples: 2000\n",
            "[2021-05-01 10:34:49,946 INFO] Step 37700/50000; xent: 2.56; lr: 0.0000103;  18 docs/s;   3150 sec\n",
            "[2021-05-01 10:35:47,784 INFO] Step 37750/50000; xent: 2.54; lr: 0.0000103;  18 docs/s;   3208 sec\n",
            "[2021-05-01 10:36:45,845 INFO] Step 37800/50000; xent: 2.58; lr: 0.0000103;  18 docs/s;   3266 sec\n",
            "[2021-05-01 10:37:43,766 INFO] Step 37850/50000; xent: 2.55; lr: 0.0000103;  18 docs/s;   3324 sec\n",
            "[2021-05-01 10:38:26,372 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.84.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:38:42,595 INFO] Step 37900/50000; xent: 2.57; lr: 0.0000103;  17 docs/s;   3383 sec\n",
            "[2021-05-01 10:39:40,601 INFO] Step 37950/50000; xent: 2.62; lr: 0.0000103;  18 docs/s;   3441 sec\n",
            "[2021-05-01 10:40:38,593 INFO] Step 38000/50000; xent: 2.54; lr: 0.0000103;  18 docs/s;   3499 sec\n",
            "[2021-05-01 10:40:38,609 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_38000.pt\n",
            "[2021-05-01 10:41:43,774 INFO] Step 38050/50000; xent: 2.53; lr: 0.0000103;  16 docs/s;   3564 sec\n",
            "[2021-05-01 10:42:18,054 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.137.bert.pt, number of examples: 2001\n",
            "[2021-05-01 10:42:42,451 INFO] Step 38100/50000; xent: 2.57; lr: 0.0000102;  18 docs/s;   3622 sec\n",
            "[2021-05-01 10:43:40,480 INFO] Step 38150/50000; xent: 2.57; lr: 0.0000102;  18 docs/s;   3680 sec\n",
            "[2021-05-01 10:44:38,336 INFO] Step 38200/50000; xent: 2.65; lr: 0.0000102;  18 docs/s;   3738 sec\n",
            "[2021-05-01 10:45:36,112 INFO] Step 38250/50000; xent: 2.54; lr: 0.0000102;  18 docs/s;   3796 sec\n",
            "[2021-05-01 10:46:02,295 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.13.bert.pt, number of examples: 2000\n",
            "[2021-05-01 10:46:34,709 INFO] Step 38300/50000; xent: 2.46; lr: 0.0000102;  18 docs/s;   3855 sec\n",
            "[2021-05-01 10:47:32,425 INFO] Step 38350/50000; xent: 2.50; lr: 0.0000102;  18 docs/s;   3912 sec\n",
            "[2021-05-01 10:48:30,778 INFO] Step 38400/50000; xent: 2.52; lr: 0.0000102;  18 docs/s;   3971 sec\n",
            "[2021-05-01 10:49:28,932 INFO] Step 38450/50000; xent: 2.48; lr: 0.0000102;  18 docs/s;   4029 sec\n",
            "[2021-05-01 10:49:45,740 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.135.bert.pt, number of examples: 2000\n",
            "[2021-05-01 10:50:27,542 INFO] Step 38500/50000; xent: 2.58; lr: 0.0000102;  17 docs/s;   4087 sec\n",
            "[2021-05-01 10:51:25,571 INFO] Step 38550/50000; xent: 2.60; lr: 0.0000102;  18 docs/s;   4145 sec\n",
            "[2021-05-01 10:52:23,655 INFO] Step 38600/50000; xent: 2.56; lr: 0.0000102;  18 docs/s;   4204 sec\n",
            "[2021-05-01 10:53:21,535 INFO] Step 38650/50000; xent: 2.59; lr: 0.0000102;  18 docs/s;   4261 sec\n",
            "[2021-05-01 10:53:30,608 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.71.bert.pt, number of examples: 1999\n",
            "[2021-05-01 10:54:20,699 INFO] Step 38700/50000; xent: 2.54; lr: 0.0000102;  17 docs/s;   4321 sec\n",
            "[2021-05-01 10:55:18,416 INFO] Step 38750/50000; xent: 2.54; lr: 0.0000102;  18 docs/s;   4378 sec\n",
            "[2021-05-01 10:56:16,521 INFO] Step 38800/50000; xent: 2.68; lr: 0.0000102;  18 docs/s;   4436 sec\n",
            "[2021-05-01 10:57:14,068 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.9.bert.pt, number of examples: 2000\n",
            "[2021-05-01 10:57:15,309 INFO] Step 38850/50000; xent: 2.52; lr: 0.0000101;  17 docs/s;   4495 sec\n",
            "[2021-05-01 10:58:13,179 INFO] Step 38900/50000; xent: 2.43; lr: 0.0000101;  18 docs/s;   4553 sec\n",
            "[2021-05-01 10:59:11,074 INFO] Step 38950/50000; xent: 2.50; lr: 0.0000101;  18 docs/s;   4611 sec\n",
            "[2021-05-01 11:00:08,919 INFO] Step 39000/50000; xent: 2.40; lr: 0.0000101;  18 docs/s;   4669 sec\n",
            "[2021-05-01 11:00:08,934 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_39000.pt\n",
            "[2021-05-01 11:01:06,314 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.105.bert.pt, number of examples: 2000\n",
            "[2021-05-01 11:01:15,580 INFO] Step 39050/50000; xent: 2.42; lr: 0.0000101;  15 docs/s;   4736 sec\n",
            "[2021-05-01 11:02:13,477 INFO] Step 39100/50000; xent: 2.51; lr: 0.0000101;  18 docs/s;   4793 sec\n",
            "[2021-05-01 11:03:11,785 INFO] Step 39150/50000; xent: 2.63; lr: 0.0000101;  18 docs/s;   4852 sec\n",
            "[2021-05-01 11:04:09,747 INFO] Step 39200/50000; xent: 2.57; lr: 0.0000101;  18 docs/s;   4910 sec\n",
            "[2021-05-01 11:04:51,372 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.50.bert.pt, number of examples: 1999\n",
            "[2021-05-01 11:05:08,672 INFO] Step 39250/50000; xent: 2.49; lr: 0.0000101;  17 docs/s;   4969 sec\n",
            "[2021-05-01 11:06:06,855 INFO] Step 39300/50000; xent: 2.57; lr: 0.0000101;  18 docs/s;   5027 sec\n",
            "[2021-05-01 11:07:04,866 INFO] Step 39350/50000; xent: 2.62; lr: 0.0000101;  18 docs/s;   5085 sec\n",
            "[2021-05-01 11:08:02,780 INFO] Step 39400/50000; xent: 2.56; lr: 0.0000101;  18 docs/s;   5143 sec\n",
            "[2021-05-01 11:08:36,086 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.106.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:09:01,766 INFO] Step 39450/50000; xent: 2.54; lr: 0.0000101;  18 docs/s;   5202 sec\n",
            "[2021-05-01 11:10:00,184 INFO] Step 39500/50000; xent: 2.55; lr: 0.0000101;  18 docs/s;   5260 sec\n",
            "[2021-05-01 11:10:57,962 INFO] Step 39550/50000; xent: 2.58; lr: 0.0000101;  18 docs/s;   5318 sec\n",
            "[2021-05-01 11:11:55,272 INFO] Step 39600/50000; xent: 2.53; lr: 0.0000101;  18 docs/s;   5375 sec\n",
            "[2021-05-01 11:12:19,518 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.38.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:12:54,558 INFO] Step 39650/50000; xent: 2.49; lr: 0.0000100;  18 docs/s;   5434 sec\n",
            "[2021-05-01 11:13:52,484 INFO] Step 39700/50000; xent: 2.54; lr: 0.0000100;  18 docs/s;   5492 sec\n",
            "[2021-05-01 11:14:50,490 INFO] Step 39750/50000; xent: 2.56; lr: 0.0000100;  18 docs/s;   5550 sec\n",
            "[2021-05-01 11:15:48,419 INFO] Step 39800/50000; xent: 2.58; lr: 0.0000100;  18 docs/s;   5608 sec\n",
            "[2021-05-01 11:16:04,505 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.24.bert.pt, number of examples: 1999\n",
            "[2021-05-01 11:16:47,590 INFO] Step 39850/50000; xent: 2.46; lr: 0.0000100;  18 docs/s;   5668 sec\n",
            "[2021-05-01 11:17:45,589 INFO] Step 39900/50000; xent: 2.53; lr: 0.0000100;  18 docs/s;   5726 sec\n",
            "[2021-05-01 11:18:43,856 INFO] Step 39950/50000; xent: 2.49; lr: 0.0000100;  18 docs/s;   5784 sec\n",
            "[2021-05-01 11:19:41,842 INFO] Step 40000/50000; xent: 2.45; lr: 0.0000100;  18 docs/s;   5842 sec\n",
            "[2021-05-01 11:19:41,845 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_40000.pt\n",
            "[2021-05-01 11:19:56,024 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.143.bert.pt, number of examples: 1084\n",
            "[2021-05-01 11:20:49,114 INFO] Step 40050/50000; xent: 2.55; lr: 0.0000100;  16 docs/s;   5909 sec\n",
            "[2021-05-01 11:21:46,544 INFO] Step 40100/50000; xent: 2.58; lr: 0.0000100;  18 docs/s;   5966 sec\n",
            "[2021-05-01 11:21:57,977 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.94.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:22:45,915 INFO] Step 40150/50000; xent: 2.53; lr: 0.0000100;  17 docs/s;   6026 sec\n",
            "[2021-05-01 11:23:43,856 INFO] Step 40200/50000; xent: 2.53; lr: 0.0000100;  18 docs/s;   6084 sec\n",
            "[2021-05-01 11:24:41,683 INFO] Step 40250/50000; xent: 2.61; lr: 0.0000100;  18 docs/s;   6142 sec\n",
            "[2021-05-01 11:25:39,139 INFO] Step 40300/50000; xent: 2.45; lr: 0.0000100;  18 docs/s;   6199 sec\n",
            "[2021-05-01 11:25:42,595 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.62.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:26:38,460 INFO] Step 40350/50000; xent: 2.53; lr: 0.0000100;  18 docs/s;   6258 sec\n",
            "[2021-05-01 11:27:36,694 INFO] Step 40400/50000; xent: 2.51; lr: 0.0000100;  18 docs/s;   6317 sec\n",
            "[2021-05-01 11:28:34,940 INFO] Step 40450/50000; xent: 2.44; lr: 0.0000099;  18 docs/s;   6375 sec\n",
            "[2021-05-01 11:29:26,830 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.139.bert.pt, number of examples: 2000\n",
            "[2021-05-01 11:29:33,822 INFO] Step 40500/50000; xent: 2.50; lr: 0.0000099;  17 docs/s;   6434 sec\n",
            "[2021-05-01 11:30:31,784 INFO] Step 40550/50000; xent: 2.53; lr: 0.0000099;  18 docs/s;   6492 sec\n",
            "[2021-05-01 11:31:29,433 INFO] Step 40600/50000; xent: 2.49; lr: 0.0000099;  18 docs/s;   6549 sec\n",
            "[2021-05-01 11:32:27,644 INFO] Step 40650/50000; xent: 2.55; lr: 0.0000099;  18 docs/s;   6608 sec\n",
            "[2021-05-01 11:33:11,516 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.88.bert.pt, number of examples: 2000\n",
            "[2021-05-01 11:33:26,651 INFO] Step 40700/50000; xent: 2.57; lr: 0.0000099;  18 docs/s;   6667 sec\n",
            "[2021-05-01 11:34:24,435 INFO] Step 40750/50000; xent: 2.51; lr: 0.0000099;  17 docs/s;   6724 sec\n",
            "[2021-05-01 11:35:22,612 INFO] Step 40800/50000; xent: 2.53; lr: 0.0000099;  18 docs/s;   6783 sec\n",
            "[2021-05-01 11:36:20,846 INFO] Step 40850/50000; xent: 2.60; lr: 0.0000099;  18 docs/s;   6841 sec\n",
            "[2021-05-01 11:36:56,290 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.92.bert.pt, number of examples: 1998\n",
            "[2021-05-01 11:37:19,564 INFO] Step 40900/50000; xent: 2.58; lr: 0.0000099;  17 docs/s;   6899 sec\n",
            "[2021-05-01 11:38:17,970 INFO] Step 40950/50000; xent: 2.47; lr: 0.0000099;  18 docs/s;   6958 sec\n",
            "[2021-05-01 11:39:15,822 INFO] Step 41000/50000; xent: 2.43; lr: 0.0000099;  18 docs/s;   7016 sec\n",
            "[2021-05-01 11:39:15,837 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_41000.pt\n",
            "[2021-05-01 11:40:21,722 INFO] Step 41050/50000; xent: 2.35; lr: 0.0000099;  16 docs/s;   7082 sec\n",
            "[2021-05-01 11:40:47,940 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.68.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:41:20,388 INFO] Step 41100/50000; xent: 2.44; lr: 0.0000099;  17 docs/s;   7140 sec\n",
            "[2021-05-01 11:42:18,408 INFO] Step 41150/50000; xent: 2.52; lr: 0.0000099;  18 docs/s;   7198 sec\n",
            "[2021-05-01 11:43:16,064 INFO] Step 41200/50000; xent: 2.63; lr: 0.0000099;  18 docs/s;   7256 sec\n",
            "[2021-05-01 11:44:14,066 INFO] Step 41250/50000; xent: 2.52; lr: 0.0000098;  18 docs/s;   7314 sec\n",
            "[2021-05-01 11:44:32,372 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.0.bert.pt, number of examples: 2000\n",
            "[2021-05-01 11:45:12,959 INFO] Step 41300/50000; xent: 2.51; lr: 0.0000098;  17 docs/s;   7373 sec\n",
            "[2021-05-01 11:46:11,368 INFO] Step 41350/50000; xent: 2.50; lr: 0.0000098;  18 docs/s;   7431 sec\n",
            "[2021-05-01 11:47:09,595 INFO] Step 41400/50000; xent: 2.57; lr: 0.0000098;  18 docs/s;   7490 sec\n",
            "[2021-05-01 11:48:07,149 INFO] Step 41450/50000; xent: 2.53; lr: 0.0000098;  18 docs/s;   7547 sec\n",
            "[2021-05-01 11:48:17,079 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.83.bert.pt, number of examples: 1999\n",
            "[2021-05-01 11:49:05,995 INFO] Step 41500/50000; xent: 2.57; lr: 0.0000098;  18 docs/s;   7606 sec\n",
            "[2021-05-01 11:50:03,693 INFO] Step 41550/50000; xent: 2.52; lr: 0.0000098;  18 docs/s;   7664 sec\n",
            "[2021-05-01 11:51:02,109 INFO] Step 41600/50000; xent: 2.50; lr: 0.0000098;  18 docs/s;   7722 sec\n",
            "[2021-05-01 11:51:59,448 INFO] Step 41650/50000; xent: 2.47; lr: 0.0000098;  18 docs/s;   7779 sec\n",
            "[2021-05-01 11:52:00,458 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.138.bert.pt, number of examples: 2000\n",
            "[2021-05-01 11:52:58,723 INFO] Step 41700/50000; xent: 2.52; lr: 0.0000098;  17 docs/s;   7839 sec\n",
            "[2021-05-01 11:53:57,274 INFO] Step 41750/50000; xent: 2.56; lr: 0.0000098;  19 docs/s;   7897 sec\n",
            "[2021-05-01 11:54:55,008 INFO] Step 41800/50000; xent: 2.61; lr: 0.0000098;  18 docs/s;   7955 sec\n",
            "[2021-05-01 11:55:44,256 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.72.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:55:53,545 INFO] Step 41850/50000; xent: 2.49; lr: 0.0000098;  18 docs/s;   8013 sec\n",
            "[2021-05-01 11:56:51,321 INFO] Step 41900/50000; xent: 2.52; lr: 0.0000098;  18 docs/s;   8071 sec\n",
            "[2021-05-01 11:57:49,545 INFO] Step 41950/50000; xent: 2.60; lr: 0.0000098;  18 docs/s;   8129 sec\n",
            "[2021-05-01 11:58:47,693 INFO] Step 42000/50000; xent: 2.47; lr: 0.0000098;  18 docs/s;   8188 sec\n",
            "[2021-05-01 11:58:47,708 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_42000.pt\n",
            "[2021-05-01 11:59:40,854 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.89.bert.pt, number of examples: 2001\n",
            "[2021-05-01 11:59:58,302 INFO] Step 42050/50000; xent: 2.40; lr: 0.0000098;  14 docs/s;   8258 sec\n",
            "[2021-05-01 12:00:55,963 INFO] Step 42100/50000; xent: 2.38; lr: 0.0000097;  18 docs/s;   8316 sec\n",
            "[2021-05-01 12:01:53,945 INFO] Step 42150/50000; xent: 2.49; lr: 0.0000097;  18 docs/s;   8374 sec\n",
            "[2021-05-01 12:02:51,723 INFO] Step 42200/50000; xent: 2.48; lr: 0.0000097;  18 docs/s;   8432 sec\n",
            "[2021-05-01 12:03:24,956 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.51.bert.pt, number of examples: 1999\n",
            "[2021-05-01 12:03:50,410 INFO] Step 42250/50000; xent: 2.46; lr: 0.0000097;  18 docs/s;   8490 sec\n",
            "[2021-05-01 12:04:48,675 INFO] Step 42300/50000; xent: 2.54; lr: 0.0000097;  18 docs/s;   8549 sec\n",
            "[2021-05-01 12:05:46,216 INFO] Step 42350/50000; xent: 2.50; lr: 0.0000097;  18 docs/s;   8606 sec\n",
            "[2021-05-01 12:06:44,211 INFO] Step 42400/50000; xent: 2.42; lr: 0.0000097;  18 docs/s;   8664 sec\n",
            "[2021-05-01 12:07:10,362 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.35.bert.pt, number of examples: 2001\n",
            "[2021-05-01 12:07:42,773 INFO] Step 42450/50000; xent: 2.47; lr: 0.0000097;  17 docs/s;   8723 sec\n",
            "[2021-05-01 12:08:41,123 INFO] Step 42500/50000; xent: 2.60; lr: 0.0000097;  18 docs/s;   8781 sec\n",
            "[2021-05-01 12:09:39,278 INFO] Step 42550/50000; xent: 2.52; lr: 0.0000097;  18 docs/s;   8839 sec\n",
            "[2021-05-01 12:10:37,816 INFO] Step 42600/50000; xent: 2.56; lr: 0.0000097;  18 docs/s;   8898 sec\n",
            "[2021-05-01 12:10:55,104 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.37.bert.pt, number of examples: 2001\n",
            "[2021-05-01 12:11:36,717 INFO] Step 42650/50000; xent: 2.54; lr: 0.0000097;  17 docs/s;   8957 sec\n",
            "[2021-05-01 12:12:35,019 INFO] Step 42700/50000; xent: 2.47; lr: 0.0000097;  18 docs/s;   9015 sec\n",
            "[2021-05-01 12:13:33,641 INFO] Step 42750/50000; xent: 2.54; lr: 0.0000097;  18 docs/s;   9074 sec\n",
            "[2021-05-01 12:14:32,087 INFO] Step 42800/50000; xent: 2.49; lr: 0.0000097;  18 docs/s;   9132 sec\n",
            "[2021-05-01 12:14:39,963 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.80.bert.pt, number of examples: 2000\n",
            "[2021-05-01 12:15:31,276 INFO] Step 42850/50000; xent: 2.49; lr: 0.0000097;  18 docs/s;   9191 sec\n",
            "[2021-05-01 12:16:29,458 INFO] Step 42900/50000; xent: 2.53; lr: 0.0000097;  18 docs/s;   9249 sec\n",
            "[2021-05-01 12:17:27,975 INFO] Step 42950/50000; xent: 2.58; lr: 0.0000097;  18 docs/s;   9308 sec\n",
            "[2021-05-01 12:18:26,970 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.52.bert.pt, number of examples: 1999\n",
            "[2021-05-01 12:18:28,228 INFO] Step 43000/50000; xent: 2.48; lr: 0.0000096;  17 docs/s;   9368 sec\n",
            "[2021-05-01 12:18:28,243 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_43000.pt\n",
            "[2021-05-01 12:19:37,363 INFO] Step 43050/50000; xent: 2.31; lr: 0.0000096;  15 docs/s;   9437 sec\n",
            "[2021-05-01 12:20:34,572 INFO] Step 43100/50000; xent: 2.39; lr: 0.0000096;  18 docs/s;   9494 sec\n",
            "[2021-05-01 12:21:32,498 INFO] Step 43150/50000; xent: 2.40; lr: 0.0000096;  18 docs/s;   9552 sec\n",
            "[2021-05-01 12:22:21,045 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.12.bert.pt, number of examples: 1998\n",
            "[2021-05-01 12:22:31,545 INFO] Step 43200/50000; xent: 2.49; lr: 0.0000096;  17 docs/s;   9611 sec\n",
            "[2021-05-01 12:23:29,332 INFO] Step 43250/50000; xent: 2.52; lr: 0.0000096;  18 docs/s;   9669 sec\n",
            "[2021-05-01 12:24:27,525 INFO] Step 43300/50000; xent: 2.54; lr: 0.0000096;  18 docs/s;   9727 sec\n",
            "[2021-05-01 12:25:25,623 INFO] Step 43350/50000; xent: 2.52; lr: 0.0000096;  18 docs/s;   9786 sec\n",
            "[2021-05-01 12:26:04,583 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.114.bert.pt, number of examples: 2001\n",
            "[2021-05-01 12:26:24,423 INFO] Step 43400/50000; xent: 2.46; lr: 0.0000096;  18 docs/s;   9844 sec\n",
            "[2021-05-01 12:27:22,667 INFO] Step 43450/50000; xent: 2.42; lr: 0.0000096;  18 docs/s;   9903 sec\n",
            "[2021-05-01 12:28:20,531 INFO] Step 43500/50000; xent: 2.44; lr: 0.0000096;  18 docs/s;   9960 sec\n",
            "[2021-05-01 12:29:18,840 INFO] Step 43550/50000; xent: 2.51; lr: 0.0000096;  18 docs/s;  10019 sec\n",
            "[2021-05-01 12:29:49,585 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.86.bert.pt, number of examples: 1999\n",
            "[2021-05-01 12:30:17,441 INFO] Step 43600/50000; xent: 2.48; lr: 0.0000096;  17 docs/s;  10077 sec\n",
            "[2021-05-01 12:31:15,305 INFO] Step 43650/50000; xent: 2.46; lr: 0.0000096;  18 docs/s;  10135 sec\n",
            "[2021-05-01 12:32:13,078 INFO] Step 43700/50000; xent: 2.42; lr: 0.0000096;  18 docs/s;  10193 sec\n",
            "[2021-05-01 12:33:11,335 INFO] Step 43750/50000; xent: 2.29; lr: 0.0000096;  18 docs/s;  10251 sec\n",
            "[2021-05-01 12:33:33,199 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.113.bert.pt, number of examples: 2000\n",
            "[2021-05-01 12:34:10,127 INFO] Step 43800/50000; xent: 2.51; lr: 0.0000096;  17 docs/s;  10310 sec\n",
            "[2021-05-01 12:35:08,419 INFO] Step 43850/50000; xent: 2.50; lr: 0.0000096;  18 docs/s;  10368 sec\n",
            "[2021-05-01 12:36:06,737 INFO] Step 43900/50000; xent: 2.59; lr: 0.0000095;  18 docs/s;  10427 sec\n",
            "[2021-05-01 12:37:04,472 INFO] Step 43950/50000; xent: 2.52; lr: 0.0000095;  18 docs/s;  10484 sec\n",
            "[2021-05-01 12:37:18,223 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.96.bert.pt, number of examples: 1999\n",
            "[2021-05-01 12:38:03,701 INFO] Step 44000/50000; xent: 2.52; lr: 0.0000095;  18 docs/s;  10544 sec\n",
            "[2021-05-01 12:38:03,717 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_44000.pt\n",
            "[2021-05-01 12:39:14,999 INFO] Step 44050/50000; xent: 2.51; lr: 0.0000095;  15 docs/s;  10615 sec\n",
            "[2021-05-01 12:40:12,598 INFO] Step 44100/50000; xent: 2.49; lr: 0.0000095;  18 docs/s;  10673 sec\n",
            "[2021-05-01 12:41:10,548 INFO] Step 44150/50000; xent: 2.50; lr: 0.0000095;  18 docs/s;  10730 sec\n",
            "[2021-05-01 12:41:14,968 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.131.bert.pt, number of examples: 2001\n",
            "[2021-05-01 12:42:09,351 INFO] Step 44200/50000; xent: 2.50; lr: 0.0000095;  18 docs/s;  10789 sec\n",
            "[2021-05-01 12:43:07,456 INFO] Step 44250/50000; xent: 2.56; lr: 0.0000095;  18 docs/s;  10847 sec\n",
            "[2021-05-01 12:44:05,475 INFO] Step 44300/50000; xent: 2.53; lr: 0.0000095;  18 docs/s;  10905 sec\n",
            "[2021-05-01 12:44:58,694 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.97.bert.pt, number of examples: 2000\n",
            "[2021-05-01 12:45:04,515 INFO] Step 44350/50000; xent: 2.55; lr: 0.0000095;  18 docs/s;  10964 sec\n",
            "[2021-05-01 12:46:02,409 INFO] Step 44400/50000; xent: 2.49; lr: 0.0000095;  18 docs/s;  11022 sec\n",
            "[2021-05-01 12:47:00,568 INFO] Step 44450/50000; xent: 2.48; lr: 0.0000095;  18 docs/s;  11080 sec\n",
            "[2021-05-01 12:47:58,434 INFO] Step 44500/50000; xent: 2.55; lr: 0.0000095;  18 docs/s;  11138 sec\n",
            "[2021-05-01 12:48:44,362 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.55.bert.pt, number of examples: 1999\n",
            "[2021-05-01 12:48:57,134 INFO] Step 44550/50000; xent: 2.55; lr: 0.0000095;  17 docs/s;  11197 sec\n",
            "[2021-05-01 12:49:55,212 INFO] Step 44600/50000; xent: 2.46; lr: 0.0000095;  18 docs/s;  11255 sec\n",
            "[2021-05-01 12:50:53,318 INFO] Step 44650/50000; xent: 2.53; lr: 0.0000095;  18 docs/s;  11313 sec\n",
            "[2021-05-01 12:51:51,427 INFO] Step 44700/50000; xent: 2.47; lr: 0.0000095;  18 docs/s;  11371 sec\n",
            "[2021-05-01 12:52:27,190 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.128.bert.pt, number of examples: 2000\n",
            "[2021-05-01 12:52:50,370 INFO] Step 44750/50000; xent: 2.47; lr: 0.0000095;  17 docs/s;  11430 sec\n",
            "[2021-05-01 12:53:48,666 INFO] Step 44800/50000; xent: 2.47; lr: 0.0000094;  18 docs/s;  11489 sec\n",
            "[2021-05-01 12:54:46,481 INFO] Step 44850/50000; xent: 2.55; lr: 0.0000094;  18 docs/s;  11546 sec\n",
            "[2021-05-01 12:55:44,594 INFO] Step 44900/50000; xent: 2.41; lr: 0.0000094;  18 docs/s;  11605 sec\n",
            "[2021-05-01 12:56:10,912 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.104.bert.pt, number of examples: 2000\n",
            "[2021-05-01 12:56:43,207 INFO] Step 44950/50000; xent: 2.42; lr: 0.0000094;  18 docs/s;  11663 sec\n",
            "[2021-05-01 12:57:41,226 INFO] Step 45000/50000; xent: 2.50; lr: 0.0000094;  18 docs/s;  11721 sec\n",
            "[2021-05-01 12:57:41,242 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_45000.pt\n",
            "[2021-05-01 12:58:47,138 INFO] Step 45050/50000; xent: 2.47; lr: 0.0000094;  16 docs/s;  11787 sec\n",
            "[2021-05-01 12:59:44,699 INFO] Step 45100/50000; xent: 2.47; lr: 0.0000094;  18 docs/s;  11845 sec\n",
            "[2021-05-01 13:00:01,882 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.118.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:00:44,040 INFO] Step 45150/50000; xent: 2.50; lr: 0.0000094;  17 docs/s;  11904 sec\n",
            "[2021-05-01 13:01:41,713 INFO] Step 45200/50000; xent: 2.39; lr: 0.0000094;  18 docs/s;  11962 sec\n",
            "[2021-05-01 13:02:39,458 INFO] Step 45250/50000; xent: 2.53; lr: 0.0000094;  18 docs/s;  12019 sec\n",
            "[2021-05-01 13:03:37,577 INFO] Step 45300/50000; xent: 2.30; lr: 0.0000094;  18 docs/s;  12078 sec\n",
            "[2021-05-01 13:03:45,324 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.47.bert.pt, number of examples: 2000\n",
            "[2021-05-01 13:04:36,291 INFO] Step 45350/50000; xent: 2.42; lr: 0.0000094;  18 docs/s;  12136 sec\n",
            "[2021-05-01 13:05:34,090 INFO] Step 45400/50000; xent: 2.38; lr: 0.0000094;  18 docs/s;  12194 sec\n",
            "[2021-05-01 13:06:32,125 INFO] Step 45450/50000; xent: 2.40; lr: 0.0000094;  18 docs/s;  12252 sec\n",
            "[2021-05-01 13:07:28,936 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.8.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:07:31,337 INFO] Step 45500/50000; xent: 2.41; lr: 0.0000094;  18 docs/s;  12311 sec\n",
            "[2021-05-01 13:08:29,520 INFO] Step 45550/50000; xent: 2.48; lr: 0.0000094;  18 docs/s;  12369 sec\n",
            "[2021-05-01 13:09:27,116 INFO] Step 45600/50000; xent: 2.45; lr: 0.0000094;  18 docs/s;  12427 sec\n",
            "[2021-05-01 13:10:25,258 INFO] Step 45650/50000; xent: 2.44; lr: 0.0000094;  18 docs/s;  12485 sec\n",
            "[2021-05-01 13:11:12,544 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.11.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:11:24,281 INFO] Step 45700/50000; xent: 2.42; lr: 0.0000094;  18 docs/s;  12544 sec\n",
            "[2021-05-01 13:12:22,147 INFO] Step 45750/50000; xent: 2.48; lr: 0.0000094;  18 docs/s;  12602 sec\n",
            "[2021-05-01 13:13:20,274 INFO] Step 45800/50000; xent: 2.41; lr: 0.0000093;  18 docs/s;  12660 sec\n",
            "[2021-05-01 13:14:18,168 INFO] Step 45850/50000; xent: 2.40; lr: 0.0000093;  18 docs/s;  12718 sec\n",
            "[2021-05-01 13:14:56,709 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.66.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:15:16,550 INFO] Step 45900/50000; xent: 2.52; lr: 0.0000093;  18 docs/s;  12776 sec\n",
            "[2021-05-01 13:16:14,696 INFO] Step 45950/50000; xent: 2.50; lr: 0.0000093;  18 docs/s;  12835 sec\n",
            "[2021-05-01 13:17:12,713 INFO] Step 46000/50000; xent: 2.49; lr: 0.0000093;  18 docs/s;  12893 sec\n",
            "[2021-05-01 13:17:12,729 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_46000.pt\n",
            "[2021-05-01 13:18:18,841 INFO] Step 46050/50000; xent: 2.59; lr: 0.0000093;  16 docs/s;  12959 sec\n",
            "[2021-05-01 13:18:48,126 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.36.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:19:17,102 INFO] Step 46100/50000; xent: 2.39; lr: 0.0000093;  18 docs/s;  13017 sec\n",
            "[2021-05-01 13:20:15,184 INFO] Step 46150/50000; xent: 2.44; lr: 0.0000093;  18 docs/s;  13075 sec\n",
            "[2021-05-01 13:21:13,222 INFO] Step 46200/50000; xent: 2.45; lr: 0.0000093;  18 docs/s;  13133 sec\n",
            "[2021-05-01 13:22:11,025 INFO] Step 46250/50000; xent: 2.50; lr: 0.0000093;  18 docs/s;  13191 sec\n",
            "[2021-05-01 13:22:32,931 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.73.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:23:10,340 INFO] Step 46300/50000; xent: 2.55; lr: 0.0000093;  17 docs/s;  13250 sec\n",
            "[2021-05-01 13:24:08,459 INFO] Step 46350/50000; xent: 2.45; lr: 0.0000093;  18 docs/s;  13308 sec\n",
            "[2021-05-01 13:25:06,172 INFO] Step 46400/50000; xent: 2.52; lr: 0.0000093;  18 docs/s;  13366 sec\n",
            "[2021-05-01 13:26:04,159 INFO] Step 46450/50000; xent: 2.47; lr: 0.0000093;  18 docs/s;  13424 sec\n",
            "[2021-05-01 13:26:17,665 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.33.bert.pt, number of examples: 1997\n",
            "[2021-05-01 13:27:02,872 INFO] Step 46500/50000; xent: 2.46; lr: 0.0000093;  18 docs/s;  13483 sec\n",
            "[2021-05-01 13:28:01,057 INFO] Step 46550/50000; xent: 2.56; lr: 0.0000093;  18 docs/s;  13541 sec\n",
            "[2021-05-01 13:28:59,408 INFO] Step 46600/50000; xent: 2.47; lr: 0.0000093;  18 docs/s;  13599 sec\n",
            "[2021-05-01 13:29:57,142 INFO] Step 46650/50000; xent: 2.46; lr: 0.0000093;  18 docs/s;  13657 sec\n",
            "[2021-05-01 13:30:01,790 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.121.bert.pt, number of examples: 1999\n",
            "[2021-05-01 13:30:56,602 INFO] Step 46700/50000; xent: 2.47; lr: 0.0000093;  18 docs/s;  13717 sec\n",
            "[2021-05-01 13:31:54,616 INFO] Step 46750/50000; xent: 2.35; lr: 0.0000092;  18 docs/s;  13775 sec\n",
            "[2021-05-01 13:32:52,605 INFO] Step 46800/50000; xent: 2.33; lr: 0.0000092;  18 docs/s;  13833 sec\n",
            "[2021-05-01 13:33:47,322 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.136.bert.pt, number of examples: 1998\n",
            "[2021-05-01 13:33:53,182 INFO] Step 46850/50000; xent: 2.35; lr: 0.0000092;  17 docs/s;  13893 sec\n",
            "[2021-05-01 13:34:51,603 INFO] Step 46900/50000; xent: 2.47; lr: 0.0000092;  18 docs/s;  13952 sec\n",
            "[2021-05-01 13:35:49,580 INFO] Step 46950/50000; xent: 2.49; lr: 0.0000092;  18 docs/s;  14010 sec\n",
            "[2021-05-01 13:36:47,574 INFO] Step 47000/50000; xent: 2.48; lr: 0.0000092;  18 docs/s;  14067 sec\n",
            "[2021-05-01 13:36:47,589 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_47000.pt\n",
            "[2021-05-01 13:37:39,350 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.20.bert.pt, number of examples: 2000\n",
            "[2021-05-01 13:37:54,440 INFO] Step 47050/50000; xent: 2.43; lr: 0.0000092;  15 docs/s;  14134 sec\n",
            "[2021-05-01 13:38:52,685 INFO] Step 47100/50000; xent: 2.63; lr: 0.0000092;  18 docs/s;  14193 sec\n",
            "[2021-05-01 13:39:50,583 INFO] Step 47150/50000; xent: 2.43; lr: 0.0000092;  18 docs/s;  14251 sec\n",
            "[2021-05-01 13:40:48,378 INFO] Step 47200/50000; xent: 2.46; lr: 0.0000092;  18 docs/s;  14308 sec\n",
            "[2021-05-01 13:41:23,187 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.61.bert.pt, number of examples: 1999\n",
            "[2021-05-01 13:41:47,591 INFO] Step 47250/50000; xent: 2.51; lr: 0.0000092;  17 docs/s;  14368 sec\n",
            "[2021-05-01 13:42:45,795 INFO] Step 47300/50000; xent: 2.51; lr: 0.0000092;  18 docs/s;  14426 sec\n",
            "[2021-05-01 13:43:43,797 INFO] Step 47350/50000; xent: 2.53; lr: 0.0000092;  18 docs/s;  14484 sec\n",
            "[2021-05-01 13:44:42,150 INFO] Step 47400/50000; xent: 2.47; lr: 0.0000092;  18 docs/s;  14542 sec\n",
            "[2021-05-01 13:45:08,552 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.45.bert.pt, number of examples: 2000\n",
            "[2021-05-01 13:45:41,238 INFO] Step 47450/50000; xent: 2.41; lr: 0.0000092;  18 docs/s;  14601 sec\n",
            "[2021-05-01 13:46:39,561 INFO] Step 47500/50000; xent: 2.40; lr: 0.0000092;  18 docs/s;  14659 sec\n",
            "[2021-05-01 13:47:37,801 INFO] Step 47550/50000; xent: 2.44; lr: 0.0000092;  18 docs/s;  14718 sec\n",
            "[2021-05-01 13:48:35,958 INFO] Step 47600/50000; xent: 2.50; lr: 0.0000092;  18 docs/s;  14776 sec\n",
            "[2021-05-01 13:48:54,328 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.134.bert.pt, number of examples: 2000\n",
            "[2021-05-01 13:49:35,072 INFO] Step 47650/50000; xent: 2.44; lr: 0.0000092;  17 docs/s;  14835 sec\n",
            "[2021-05-01 13:50:33,229 INFO] Step 47700/50000; xent: 2.52; lr: 0.0000092;  17 docs/s;  14893 sec\n",
            "[2021-05-01 13:51:31,766 INFO] Step 47750/50000; xent: 2.45; lr: 0.0000092;  18 docs/s;  14952 sec\n",
            "[2021-05-01 13:52:30,322 INFO] Step 47800/50000; xent: 2.51; lr: 0.0000091;  18 docs/s;  15010 sec\n",
            "[2021-05-01 13:52:39,781 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.21.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:53:29,817 INFO] Step 47850/50000; xent: 2.61; lr: 0.0000091;  17 docs/s;  15070 sec\n",
            "[2021-05-01 13:54:27,654 INFO] Step 47900/50000; xent: 2.49; lr: 0.0000091;  18 docs/s;  15128 sec\n",
            "[2021-05-01 13:55:26,079 INFO] Step 47950/50000; xent: 2.47; lr: 0.0000091;  18 docs/s;  15186 sec\n",
            "[2021-05-01 13:56:24,076 INFO] Step 48000/50000; xent: 2.44; lr: 0.0000091;  18 docs/s;  15244 sec\n",
            "[2021-05-01 13:56:24,079 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_48000.pt\n",
            "[2021-05-01 13:56:38,135 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.43.bert.pt, number of examples: 2001\n",
            "[2021-05-01 13:57:37,142 INFO] Step 48050/50000; xent: 2.55; lr: 0.0000091;  15 docs/s;  15317 sec\n",
            "[2021-05-01 13:58:35,003 INFO] Step 48100/50000; xent: 2.51; lr: 0.0000091;  18 docs/s;  15375 sec\n",
            "[2021-05-01 13:59:33,091 INFO] Step 48150/50000; xent: 2.48; lr: 0.0000091;  18 docs/s;  15433 sec\n",
            "[2021-05-01 14:00:23,103 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.141.bert.pt, number of examples: 1998\n",
            "[2021-05-01 14:00:32,419 INFO] Step 48200/50000; xent: 2.44; lr: 0.0000091;  17 docs/s;  15492 sec\n",
            "[2021-05-01 14:01:30,663 INFO] Step 48250/50000; xent: 2.51; lr: 0.0000091;  18 docs/s;  15551 sec\n",
            "[2021-05-01 14:02:28,909 INFO] Step 48300/50000; xent: 2.53; lr: 0.0000091;  18 docs/s;  15609 sec\n",
            "[2021-05-01 14:03:27,456 INFO] Step 48350/50000; xent: 2.48; lr: 0.0000091;  18 docs/s;  15667 sec\n",
            "[2021-05-01 14:04:07,991 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.16.bert.pt, number of examples: 2001\n",
            "[2021-05-01 14:04:26,598 INFO] Step 48400/50000; xent: 2.46; lr: 0.0000091;  17 docs/s;  15727 sec\n",
            "[2021-05-01 14:05:24,934 INFO] Step 48450/50000; xent: 2.46; lr: 0.0000091;  18 docs/s;  15785 sec\n",
            "[2021-05-01 14:06:23,088 INFO] Step 48500/50000; xent: 2.50; lr: 0.0000091;  18 docs/s;  15843 sec\n",
            "[2021-05-01 14:07:21,455 INFO] Step 48550/50000; xent: 2.53; lr: 0.0000091;  18 docs/s;  15901 sec\n",
            "[2021-05-01 14:07:53,817 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.116.bert.pt, number of examples: 1997\n",
            "[2021-05-01 14:08:20,543 INFO] Step 48600/50000; xent: 2.61; lr: 0.0000091;  17 docs/s;  15960 sec\n",
            "[2021-05-01 14:09:18,793 INFO] Step 48650/50000; xent: 2.40; lr: 0.0000091;  18 docs/s;  16019 sec\n",
            "[2021-05-01 14:10:17,229 INFO] Step 48700/50000; xent: 2.50; lr: 0.0000091;  18 docs/s;  16077 sec\n",
            "[2021-05-01 14:11:15,424 INFO] Step 48750/50000; xent: 2.51; lr: 0.0000091;  18 docs/s;  16135 sec\n",
            "[2021-05-01 14:11:38,736 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.76.bert.pt, number of examples: 2001\n",
            "[2021-05-01 14:12:14,759 INFO] Step 48800/50000; xent: 2.43; lr: 0.0000091;  17 docs/s;  16195 sec\n",
            "[2021-05-01 14:13:13,192 INFO] Step 48850/50000; xent: 2.30; lr: 0.0000090;  17 docs/s;  16253 sec\n",
            "[2021-05-01 14:14:11,232 INFO] Step 48900/50000; xent: 2.31; lr: 0.0000090;  18 docs/s;  16311 sec\n",
            "[2021-05-01 14:15:09,436 INFO] Step 48950/50000; xent: 2.32; lr: 0.0000090;  18 docs/s;  16369 sec\n",
            "[2021-05-01 14:15:25,705 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.19.bert.pt, number of examples: 1999\n",
            "[2021-05-01 14:16:08,736 INFO] Step 49000/50000; xent: 2.50; lr: 0.0000090;  18 docs/s;  16429 sec\n",
            "[2021-05-01 14:16:08,753 INFO] Saving checkpoint ../data/trained_models/reberta_smaller/model_step_49000.pt\n",
            "[2021-05-01 14:17:14,518 INFO] Step 49050/50000; xent: 2.52; lr: 0.0000090;  16 docs/s;  16494 sec\n",
            "[2021-05-01 14:18:12,663 INFO] Step 49100/50000; xent: 2.62; lr: 0.0000090;  18 docs/s;  16553 sec\n",
            "[2021-05-01 14:19:10,599 INFO] Step 49150/50000; xent: 2.66; lr: 0.0000090;  18 docs/s;  16611 sec\n",
            "[2021-05-01 14:19:17,556 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.107.bert.pt, number of examples: 1999\n",
            "[2021-05-01 14:20:10,143 INFO] Step 49200/50000; xent: 2.60; lr: 0.0000090;  17 docs/s;  16670 sec\n",
            "[2021-05-01 14:21:08,693 INFO] Step 49250/50000; xent: 2.57; lr: 0.0000090;  18 docs/s;  16729 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPKnD74mvbCB"
      },
      "source": [
        "## lr=2e-3, layer=2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQFq4XLuvHBD",
        "outputId": "f058dcb0-cf4b-41ef-9f5d-60f64c0939ad"
      },
      "source": [
        "!python3 train.py -task ext -encoder roberta -mode train -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -ext_dropout 0.1 -model_path ../data/trained_models/reberta_2layers -lr 2e-3 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_roberta2_cnndm -use_interval true -warmup_steps 10000 -max_pos 512 -ext_layers 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "----TRAIN EXTRACTIVE----\n",
            "[2021-05-02 06:21:00,415 INFO] Device ID 0\n",
            "[2021-05-02 06:21:00,415 INFO] Device cuda\n",
            "[2021-05-02 06:21:00,449 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 06:21:00,450 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 06:21:00,480 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 06:21:08,376 INFO] ExtSummarizer(\n",
            "  (bert): Roberta(\n",
            "    (model): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(514, 768)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ext_layer): ExtTransformerEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-05-02 06:21:08,438 INFO] * number of parameters: 135675905\n",
            "[2021-05-02 06:21:08,439 INFO] Start training...\n",
            "[2021-05-02 06:21:09,384 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.123.bert.pt, number of examples: 2000\n",
            "[2021-05-02 06:22:02,107 INFO] Step 50/50000; xent: 3.20; lr: 0.0000001;  19 docs/s;     53 sec\n",
            "[2021-05-02 06:22:58,276 INFO] Step 100/50000; xent: 3.21; lr: 0.0000002;  19 docs/s;    109 sec\n",
            "[2021-05-02 06:23:56,444 INFO] Step 150/50000; xent: 3.20; lr: 0.0000003;  18 docs/s;    167 sec\n",
            "[2021-05-02 06:24:44,735 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.91.bert.pt, number of examples: 1995\n",
            "[2021-05-02 06:24:55,141 INFO] Step 200/50000; xent: 3.23; lr: 0.0000004;  17 docs/s;    226 sec\n",
            "[2021-05-02 06:25:52,959 INFO] Step 250/50000; xent: 3.11; lr: 0.0000005;  18 docs/s;    284 sec\n",
            "[2021-05-02 06:26:50,537 INFO] Step 300/50000; xent: 3.09; lr: 0.0000006;  18 docs/s;    341 sec\n",
            "[2021-05-02 06:27:48,282 INFO] Step 350/50000; xent: 3.10; lr: 0.0000007;  18 docs/s;    399 sec\n",
            "[2021-05-02 06:28:28,248 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.39.bert.pt, number of examples: 2001\n",
            "[2021-05-02 06:28:46,787 INFO] Step 400/50000; xent: 3.03; lr: 0.0000008;  18 docs/s;    457 sec\n",
            "[2021-05-02 06:29:44,139 INFO] Step 450/50000; xent: 3.09; lr: 0.0000009;  18 docs/s;    515 sec\n",
            "[2021-05-02 06:30:42,316 INFO] Step 500/50000; xent: 2.99; lr: 0.0000010;  19 docs/s;    573 sec\n",
            "[2021-05-02 06:31:39,948 INFO] Step 550/50000; xent: 2.99; lr: 0.0000011;  18 docs/s;    631 sec\n",
            "[2021-05-02 06:32:10,918 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.6.bert.pt, number of examples: 2001\n",
            "[2021-05-02 06:32:38,583 INFO] Step 600/50000; xent: 3.06; lr: 0.0000012;  18 docs/s;    689 sec\n",
            "[2021-05-02 06:33:36,420 INFO] Step 650/50000; xent: 3.08; lr: 0.0000013;  18 docs/s;    747 sec\n",
            "[2021-05-02 06:34:34,160 INFO] Step 700/50000; xent: 3.04; lr: 0.0000014;  18 docs/s;    805 sec\n",
            "[2021-05-02 06:35:31,672 INFO] Step 750/50000; xent: 2.99; lr: 0.0000015;  18 docs/s;    862 sec\n",
            "[2021-05-02 06:35:54,309 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.81.bert.pt, number of examples: 2001\n",
            "[2021-05-02 06:36:30,061 INFO] Step 800/50000; xent: 2.91; lr: 0.0000016;  18 docs/s;    921 sec\n",
            "[2021-05-02 06:37:27,855 INFO] Step 850/50000; xent: 2.97; lr: 0.0000017;  18 docs/s;    978 sec\n",
            "[2021-05-02 06:38:25,479 INFO] Step 900/50000; xent: 2.96; lr: 0.0000018;  18 docs/s;   1036 sec\n",
            "[2021-05-02 06:39:23,435 INFO] Step 950/50000; xent: 3.01; lr: 0.0000019;  18 docs/s;   1094 sec\n",
            "[2021-05-02 06:39:37,271 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.98.bert.pt, number of examples: 2001\n",
            "[2021-05-02 06:40:22,333 INFO] Step 1000/50000; xent: 2.87; lr: 0.0000020;  18 docs/s;   1153 sec\n",
            "[2021-05-02 06:40:22,351 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_1000.pt\n",
            "[2021-05-02 06:41:34,076 INFO] Step 1050/50000; xent: 2.85; lr: 0.0000021;  14 docs/s;   1225 sec\n",
            "[2021-05-02 06:42:31,817 INFO] Step 1100/50000; xent: 2.92; lr: 0.0000022;  19 docs/s;   1282 sec\n",
            "[2021-05-02 06:43:29,017 INFO] Step 1150/50000; xent: 2.93; lr: 0.0000023;  18 docs/s;   1340 sec\n",
            "[2021-05-02 06:43:34,678 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.93.bert.pt, number of examples: 2001\n",
            "[2021-05-02 06:44:29,118 INFO] Step 1200/50000; xent: 2.83; lr: 0.0000024;  18 docs/s;   1400 sec\n",
            "[2021-05-02 06:45:26,677 INFO] Step 1250/50000; xent: 2.86; lr: 0.0000025;  18 docs/s;   1457 sec\n",
            "[2021-05-02 06:46:24,323 INFO] Step 1300/50000; xent: 2.83; lr: 0.0000026;  18 docs/s;   1515 sec\n",
            "[2021-05-02 06:47:20,245 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.63.bert.pt, number of examples: 2001\n",
            "[2021-05-02 06:47:23,797 INFO] Step 1350/50000; xent: 2.89; lr: 0.0000027;  17 docs/s;   1574 sec\n",
            "[2021-05-02 06:48:21,217 INFO] Step 1400/50000; xent: 2.81; lr: 0.0000028;  18 docs/s;   1632 sec\n",
            "[2021-05-02 06:49:18,909 INFO] Step 1450/50000; xent: 2.90; lr: 0.0000029;  18 docs/s;   1690 sec\n",
            "[2021-05-02 06:50:16,726 INFO] Step 1500/50000; xent: 2.85; lr: 0.0000030;  18 docs/s;   1747 sec\n",
            "[2021-05-02 06:51:02,557 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.15.bert.pt, number of examples: 2000\n",
            "[2021-05-02 06:51:15,462 INFO] Step 1550/50000; xent: 2.82; lr: 0.0000031;  18 docs/s;   1806 sec\n",
            "[2021-05-02 06:52:13,213 INFO] Step 1600/50000; xent: 2.91; lr: 0.0000032;  18 docs/s;   1864 sec\n",
            "[2021-05-02 06:53:11,002 INFO] Step 1650/50000; xent: 2.73; lr: 0.0000033;  18 docs/s;   1922 sec\n",
            "[2021-05-02 06:54:08,723 INFO] Step 1700/50000; xent: 2.80; lr: 0.0000034;  18 docs/s;   1979 sec\n",
            "[2021-05-02 06:54:46,481 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.64.bert.pt, number of examples: 1998\n",
            "[2021-05-02 06:55:07,297 INFO] Step 1750/50000; xent: 2.82; lr: 0.0000035;  17 docs/s;   2038 sec\n",
            "[2021-05-02 06:56:05,018 INFO] Step 1800/50000; xent: 2.89; lr: 0.0000036;  18 docs/s;   2096 sec\n",
            "[2021-05-02 06:57:03,085 INFO] Step 1850/50000; xent: 2.70; lr: 0.0000037;  19 docs/s;   2154 sec\n",
            "[2021-05-02 06:58:00,906 INFO] Step 1900/50000; xent: 2.87; lr: 0.0000038;  18 docs/s;   2212 sec\n",
            "[2021-05-02 06:58:29,266 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.28.bert.pt, number of examples: 1999\n",
            "[2021-05-02 06:58:58,936 INFO] Step 1950/50000; xent: 2.75; lr: 0.0000039;  18 docs/s;   2270 sec\n",
            "[2021-05-02 06:59:56,793 INFO] Step 2000/50000; xent: 2.80; lr: 0.0000040;  18 docs/s;   2327 sec\n",
            "[2021-05-02 06:59:56,807 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_2000.pt\n",
            "[2021-05-02 07:01:04,064 INFO] Step 2050/50000; xent: 2.78; lr: 0.0000041;  16 docs/s;   2395 sec\n",
            "[2021-05-02 07:02:01,477 INFO] Step 2100/50000; xent: 2.74; lr: 0.0000042;  18 docs/s;   2452 sec\n",
            "[2021-05-02 07:02:23,042 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.32.bert.pt, number of examples: 2001\n",
            "[2021-05-02 07:03:00,007 INFO] Step 2150/50000; xent: 2.76; lr: 0.0000043;  18 docs/s;   2511 sec\n",
            "[2021-05-02 07:03:57,626 INFO] Step 2200/50000; xent: 2.70; lr: 0.0000044;  18 docs/s;   2568 sec\n",
            "[2021-05-02 07:04:55,337 INFO] Step 2250/50000; xent: 2.79; lr: 0.0000045;  19 docs/s;   2626 sec\n",
            "[2021-05-02 07:05:52,593 INFO] Step 2300/50000; xent: 2.71; lr: 0.0000046;  18 docs/s;   2683 sec\n",
            "[2021-05-02 07:06:05,167 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.111.bert.pt, number of examples: 2001\n",
            "[2021-05-02 07:06:51,686 INFO] Step 2350/50000; xent: 2.74; lr: 0.0000047;  18 docs/s;   2742 sec\n",
            "[2021-05-02 07:07:49,380 INFO] Step 2400/50000; xent: 2.75; lr: 0.0000048;  18 docs/s;   2800 sec\n",
            "[2021-05-02 07:08:47,042 INFO] Step 2450/50000; xent: 2.75; lr: 0.0000049;  18 docs/s;   2858 sec\n",
            "[2021-05-02 07:09:44,741 INFO] Step 2500/50000; xent: 2.81; lr: 0.0000050;  18 docs/s;   2915 sec\n",
            "[2021-05-02 07:09:46,932 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.49.bert.pt, number of examples: 2000\n",
            "[2021-05-02 07:10:43,215 INFO] Step 2550/50000; xent: 2.70; lr: 0.0000051;  18 docs/s;   2974 sec\n",
            "[2021-05-02 07:11:40,690 INFO] Step 2600/50000; xent: 2.73; lr: 0.0000052;  18 docs/s;   3031 sec\n",
            "[2021-05-02 07:12:38,280 INFO] Step 2650/50000; xent: 2.72; lr: 0.0000053;  18 docs/s;   3089 sec\n",
            "[2021-05-02 07:13:59,397 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.59.bert.pt, number of examples: 2000\n",
            "[2021-05-02 07:14:06,379 INFO] Step 2700/50000; xent: 2.74; lr: 0.0000054;  12 docs/s;   3177 sec\n",
            "[2021-05-02 07:15:04,625 INFO] Step 2750/50000; xent: 2.73; lr: 0.0000055;  18 docs/s;   3235 sec\n",
            "[2021-05-02 07:16:02,287 INFO] Step 2800/50000; xent: 2.75; lr: 0.0000056;  18 docs/s;   3293 sec\n",
            "[2021-05-02 07:16:59,825 INFO] Step 2850/50000; xent: 2.71; lr: 0.0000057;  18 docs/s;   3350 sec\n",
            "[2021-05-02 07:17:42,125 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.84.bert.pt, number of examples: 2001\n",
            "[2021-05-02 07:17:58,353 INFO] Step 2900/50000; xent: 2.69; lr: 0.0000058;  18 docs/s;   3409 sec\n",
            "[2021-05-02 07:18:56,250 INFO] Step 2950/50000; xent: 2.77; lr: 0.0000059;  18 docs/s;   3467 sec\n",
            "[2021-05-02 07:19:54,086 INFO] Step 3000/50000; xent: 2.69; lr: 0.0000060;  19 docs/s;   3525 sec\n",
            "[2021-05-02 07:19:54,103 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_3000.pt\n",
            "[2021-05-02 07:21:01,379 INFO] Step 3050/50000; xent: 2.70; lr: 0.0000061;  15 docs/s;   3592 sec\n",
            "[2021-05-02 07:21:35,566 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.137.bert.pt, number of examples: 2001\n",
            "[2021-05-02 07:21:59,886 INFO] Step 3100/50000; xent: 2.73; lr: 0.0000062;  18 docs/s;   3651 sec\n",
            "[2021-05-02 07:22:57,465 INFO] Step 3150/50000; xent: 2.75; lr: 0.0000063;  18 docs/s;   3708 sec\n",
            "[2021-05-02 07:23:55,023 INFO] Step 3200/50000; xent: 2.80; lr: 0.0000064;  18 docs/s;   3766 sec\n",
            "[2021-05-02 07:24:52,656 INFO] Step 3250/50000; xent: 2.70; lr: 0.0000065;  18 docs/s;   3823 sec\n",
            "[2021-05-02 07:25:18,739 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.13.bert.pt, number of examples: 2000\n",
            "[2021-05-02 07:25:51,021 INFO] Step 3300/50000; xent: 2.68; lr: 0.0000066;  18 docs/s;   3882 sec\n",
            "[2021-05-02 07:26:48,354 INFO] Step 3350/50000; xent: 2.70; lr: 0.0000067;  18 docs/s;   3939 sec\n",
            "[2021-05-02 07:27:46,309 INFO] Step 3400/50000; xent: 2.69; lr: 0.0000068;  19 docs/s;   3997 sec\n",
            "[2021-05-02 07:28:43,899 INFO] Step 3450/50000; xent: 2.70; lr: 0.0000069;  18 docs/s;   4055 sec\n",
            "[2021-05-02 07:29:00,685 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.135.bert.pt, number of examples: 2000\n",
            "[2021-05-02 07:29:42,142 INFO] Step 3500/50000; xent: 2.75; lr: 0.0000070;  17 docs/s;   4113 sec\n",
            "[2021-05-02 07:30:39,879 INFO] Step 3550/50000; xent: 2.75; lr: 0.0000071;  18 docs/s;   4170 sec\n",
            "[2021-05-02 07:31:37,445 INFO] Step 3600/50000; xent: 2.70; lr: 0.0000072;  18 docs/s;   4228 sec\n",
            "[2021-05-02 07:32:35,010 INFO] Step 3650/50000; xent: 2.73; lr: 0.0000073;  18 docs/s;   4286 sec\n",
            "[2021-05-02 07:32:43,951 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.71.bert.pt, number of examples: 1999\n",
            "[2021-05-02 07:33:33,573 INFO] Step 3700/50000; xent: 2.71; lr: 0.0000074;  18 docs/s;   4344 sec\n",
            "[2021-05-02 07:34:31,173 INFO] Step 3750/50000; xent: 2.71; lr: 0.0000075;  18 docs/s;   4402 sec\n",
            "[2021-05-02 07:35:29,112 INFO] Step 3800/50000; xent: 2.82; lr: 0.0000076;  18 docs/s;   4460 sec\n",
            "[2021-05-02 07:36:26,164 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.9.bert.pt, number of examples: 2000\n",
            "[2021-05-02 07:36:27,403 INFO] Step 3850/50000; xent: 2.67; lr: 0.0000077;  18 docs/s;   4518 sec\n",
            "[2021-05-02 07:37:24,927 INFO] Step 3900/50000; xent: 2.66; lr: 0.0000078;  18 docs/s;   4576 sec\n",
            "[2021-05-02 07:38:22,277 INFO] Step 3950/50000; xent: 2.73; lr: 0.0000079;  18 docs/s;   4633 sec\n",
            "[2021-05-02 07:39:19,789 INFO] Step 4000/50000; xent: 2.64; lr: 0.0000080;  18 docs/s;   4690 sec\n",
            "[2021-05-02 07:39:19,804 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_4000.pt\n",
            "[2021-05-02 07:40:23,070 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.105.bert.pt, number of examples: 2000\n",
            "[2021-05-02 07:40:32,243 INFO] Step 4050/50000; xent: 2.66; lr: 0.0000081;  14 docs/s;   4763 sec\n",
            "[2021-05-02 07:41:29,892 INFO] Step 4100/50000; xent: 2.76; lr: 0.0000082;  18 docs/s;   4821 sec\n",
            "[2021-05-02 07:42:27,693 INFO] Step 4150/50000; xent: 2.83; lr: 0.0000083;  18 docs/s;   4878 sec\n",
            "[2021-05-02 07:43:25,154 INFO] Step 4200/50000; xent: 2.76; lr: 0.0000084;  18 docs/s;   4936 sec\n",
            "[2021-05-02 07:44:06,897 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.50.bert.pt, number of examples: 1999\n",
            "[2021-05-02 07:44:24,215 INFO] Step 4250/50000; xent: 2.70; lr: 0.0000085;  17 docs/s;   4995 sec\n",
            "[2021-05-02 07:45:21,975 INFO] Step 4300/50000; xent: 2.73; lr: 0.0000086;  18 docs/s;   5053 sec\n",
            "[2021-05-02 07:46:19,937 INFO] Step 4350/50000; xent: 2.75; lr: 0.0000087;  18 docs/s;   5111 sec\n",
            "[2021-05-02 07:47:17,688 INFO] Step 4400/50000; xent: 2.68; lr: 0.0000088;  18 docs/s;   5168 sec\n",
            "[2021-05-02 07:47:50,569 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.106.bert.pt, number of examples: 2001\n",
            "[2021-05-02 07:48:16,138 INFO] Step 4450/50000; xent: 2.67; lr: 0.0000089;  18 docs/s;   5227 sec\n",
            "[2021-05-02 07:49:14,420 INFO] Step 4500/50000; xent: 2.70; lr: 0.0000090;  19 docs/s;   5285 sec\n",
            "[2021-05-02 07:50:11,689 INFO] Step 4550/50000; xent: 2.74; lr: 0.0000091;  18 docs/s;   5342 sec\n",
            "[2021-05-02 07:51:08,609 INFO] Step 4600/50000; xent: 2.68; lr: 0.0000092;  18 docs/s;   5399 sec\n",
            "[2021-05-02 07:51:32,601 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.38.bert.pt, number of examples: 2001\n",
            "[2021-05-02 07:52:07,438 INFO] Step 4650/50000; xent: 2.64; lr: 0.0000093;  18 docs/s;   5458 sec\n",
            "[2021-05-02 07:53:04,809 INFO] Step 4700/50000; xent: 2.67; lr: 0.0000094;  18 docs/s;   5515 sec\n",
            "[2021-05-02 07:54:02,572 INFO] Step 4750/50000; xent: 2.70; lr: 0.0000095;  18 docs/s;   5573 sec\n",
            "[2021-05-02 07:55:00,160 INFO] Step 4800/50000; xent: 2.74; lr: 0.0000096;  18 docs/s;   5631 sec\n",
            "[2021-05-02 07:55:16,122 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.24.bert.pt, number of examples: 1999\n",
            "[2021-05-02 07:55:58,953 INFO] Step 4850/50000; xent: 2.65; lr: 0.0000097;  18 docs/s;   5690 sec\n",
            "[2021-05-02 07:56:56,491 INFO] Step 4900/50000; xent: 2.72; lr: 0.0000098;  18 docs/s;   5747 sec\n",
            "[2021-05-02 07:57:54,131 INFO] Step 4950/50000; xent: 2.70; lr: 0.0000099;  18 docs/s;   5805 sec\n",
            "[2021-05-02 07:58:51,528 INFO] Step 5000/50000; xent: 2.67; lr: 0.0000100;  18 docs/s;   5862 sec\n",
            "[2021-05-02 07:58:51,531 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_5000.pt\n",
            "[2021-05-02 07:59:10,451 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.143.bert.pt, number of examples: 1084\n",
            "[2021-05-02 08:00:03,296 INFO] Step 5050/50000; xent: 2.68; lr: 0.0000101;  15 docs/s;   5934 sec\n",
            "[2021-05-02 08:01:00,307 INFO] Step 5100/50000; xent: 2.70; lr: 0.0000102;  18 docs/s;   5991 sec\n",
            "[2021-05-02 08:01:12,023 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.94.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:01:59,530 INFO] Step 5150/50000; xent: 2.68; lr: 0.0000103;  17 docs/s;   6050 sec\n",
            "[2021-05-02 08:02:56,934 INFO] Step 5200/50000; xent: 2.67; lr: 0.0000104;  18 docs/s;   6108 sec\n",
            "[2021-05-02 08:03:54,361 INFO] Step 5250/50000; xent: 2.72; lr: 0.0000105;  18 docs/s;   6165 sec\n",
            "[2021-05-02 08:04:51,542 INFO] Step 5300/50000; xent: 2.57; lr: 0.0000106;  18 docs/s;   6222 sec\n",
            "[2021-05-02 08:04:54,937 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.62.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:05:50,466 INFO] Step 5350/50000; xent: 2.74; lr: 0.0000107;  18 docs/s;   6281 sec\n",
            "[2021-05-02 08:06:48,214 INFO] Step 5400/50000; xent: 2.70; lr: 0.0000108;  18 docs/s;   6339 sec\n",
            "[2021-05-02 08:07:45,930 INFO] Step 5450/50000; xent: 2.63; lr: 0.0000109;  18 docs/s;   6397 sec\n",
            "[2021-05-02 08:08:37,236 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.139.bert.pt, number of examples: 2000\n",
            "[2021-05-02 08:08:44,199 INFO] Step 5500/50000; xent: 2.69; lr: 0.0000110;  18 docs/s;   6455 sec\n",
            "[2021-05-02 08:09:41,814 INFO] Step 5550/50000; xent: 2.66; lr: 0.0000111;  18 docs/s;   6512 sec\n",
            "[2021-05-02 08:10:39,046 INFO] Step 5600/50000; xent: 2.65; lr: 0.0000112;  18 docs/s;   6570 sec\n",
            "[2021-05-02 08:11:36,958 INFO] Step 5650/50000; xent: 2.65; lr: 0.0000113;  18 docs/s;   6628 sec\n",
            "[2021-05-02 08:12:20,841 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.88.bert.pt, number of examples: 2000\n",
            "[2021-05-02 08:12:35,930 INFO] Step 5700/50000; xent: 2.70; lr: 0.0000114;  18 docs/s;   6687 sec\n",
            "[2021-05-02 08:13:33,278 INFO] Step 5750/50000; xent: 2.65; lr: 0.0000115;  18 docs/s;   6744 sec\n",
            "[2021-05-02 08:14:30,962 INFO] Step 5800/50000; xent: 2.67; lr: 0.0000116;  18 docs/s;   6802 sec\n",
            "[2021-05-02 08:15:28,886 INFO] Step 5850/50000; xent: 2.74; lr: 0.0000117;  18 docs/s;   6860 sec\n",
            "[2021-05-02 08:16:04,143 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.92.bert.pt, number of examples: 1998\n",
            "[2021-05-02 08:16:27,345 INFO] Step 5900/50000; xent: 2.77; lr: 0.0000118;  17 docs/s;   6918 sec\n",
            "[2021-05-02 08:17:25,336 INFO] Step 5950/50000; xent: 2.66; lr: 0.0000119;  18 docs/s;   6976 sec\n",
            "[2021-05-02 08:18:22,702 INFO] Step 6000/50000; xent: 2.67; lr: 0.0000120;  18 docs/s;   7033 sec\n",
            "[2021-05-02 08:18:22,717 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_6000.pt\n",
            "[2021-05-02 08:19:35,714 INFO] Step 6050/50000; xent: 2.59; lr: 0.0000121;  15 docs/s;   7106 sec\n",
            "[2021-05-02 08:20:01,697 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.68.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:20:33,981 INFO] Step 6100/50000; xent: 2.62; lr: 0.0000122;  17 docs/s;   7165 sec\n",
            "[2021-05-02 08:21:31,607 INFO] Step 6150/50000; xent: 2.61; lr: 0.0000123;  18 docs/s;   7222 sec\n",
            "[2021-05-02 08:22:29,210 INFO] Step 6200/50000; xent: 2.78; lr: 0.0000124;  18 docs/s;   7280 sec\n",
            "[2021-05-02 08:23:26,797 INFO] Step 6250/50000; xent: 2.63; lr: 0.0000125;  18 docs/s;   7337 sec\n",
            "[2021-05-02 08:23:44,676 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.0.bert.pt, number of examples: 2000\n",
            "[2021-05-02 08:24:24,947 INFO] Step 6300/50000; xent: 2.65; lr: 0.0000126;  18 docs/s;   7396 sec\n",
            "[2021-05-02 08:25:22,859 INFO] Step 6350/50000; xent: 2.63; lr: 0.0000127;  18 docs/s;   7453 sec\n",
            "[2021-05-02 08:26:20,517 INFO] Step 6400/50000; xent: 2.71; lr: 0.0000128;  18 docs/s;   7511 sec\n",
            "[2021-05-02 08:27:17,674 INFO] Step 6450/50000; xent: 2.67; lr: 0.0000129;  18 docs/s;   7568 sec\n",
            "[2021-05-02 08:27:27,520 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.83.bert.pt, number of examples: 1999\n",
            "[2021-05-02 08:28:16,219 INFO] Step 6500/50000; xent: 2.67; lr: 0.0000130;  18 docs/s;   7627 sec\n",
            "[2021-05-02 08:29:13,493 INFO] Step 6550/50000; xent: 2.64; lr: 0.0000131;  18 docs/s;   7684 sec\n",
            "[2021-05-02 08:30:11,532 INFO] Step 6600/50000; xent: 2.62; lr: 0.0000132;  18 docs/s;   7742 sec\n",
            "[2021-05-02 08:31:08,766 INFO] Step 6650/50000; xent: 2.61; lr: 0.0000133;  18 docs/s;   7799 sec\n",
            "[2021-05-02 08:31:09,852 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.138.bert.pt, number of examples: 2000\n",
            "[2021-05-02 08:32:07,756 INFO] Step 6700/50000; xent: 2.64; lr: 0.0000134;  17 docs/s;   7858 sec\n",
            "[2021-05-02 08:33:05,882 INFO] Step 6750/50000; xent: 2.69; lr: 0.0000135;  19 docs/s;   7916 sec\n",
            "[2021-05-02 08:34:03,410 INFO] Step 6800/50000; xent: 2.73; lr: 0.0000136;  18 docs/s;   7974 sec\n",
            "[2021-05-02 08:34:52,911 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.72.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:35:02,187 INFO] Step 6850/50000; xent: 2.64; lr: 0.0000137;  18 docs/s;   8033 sec\n",
            "[2021-05-02 08:35:59,698 INFO] Step 6900/50000; xent: 2.66; lr: 0.0000138;  18 docs/s;   8090 sec\n",
            "[2021-05-02 08:36:57,583 INFO] Step 6950/50000; xent: 2.74; lr: 0.0000139;  18 docs/s;   8148 sec\n",
            "[2021-05-02 08:37:55,338 INFO] Step 7000/50000; xent: 2.61; lr: 0.0000140;  18 docs/s;   8206 sec\n",
            "[2021-05-02 08:37:55,355 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_7000.pt\n",
            "[2021-05-02 08:38:50,016 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.89.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:39:07,347 INFO] Step 7050/50000; xent: 2.58; lr: 0.0000141;  14 docs/s;   8278 sec\n",
            "[2021-05-02 08:40:04,653 INFO] Step 7100/50000; xent: 2.62; lr: 0.0000142;  18 docs/s;   8335 sec\n",
            "[2021-05-02 08:41:02,068 INFO] Step 7150/50000; xent: 2.71; lr: 0.0000143;  18 docs/s;   8393 sec\n",
            "[2021-05-02 08:41:59,507 INFO] Step 7200/50000; xent: 2.68; lr: 0.0000144;  18 docs/s;   8450 sec\n",
            "[2021-05-02 08:42:32,564 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.51.bert.pt, number of examples: 1999\n",
            "[2021-05-02 08:42:57,913 INFO] Step 7250/50000; xent: 2.64; lr: 0.0000145;  18 docs/s;   8509 sec\n",
            "[2021-05-02 08:43:55,834 INFO] Step 7300/50000; xent: 2.70; lr: 0.0000146;  18 docs/s;   8566 sec\n",
            "[2021-05-02 08:44:53,050 INFO] Step 7350/50000; xent: 2.64; lr: 0.0000147;  18 docs/s;   8624 sec\n",
            "[2021-05-02 08:45:50,779 INFO] Step 7400/50000; xent: 2.57; lr: 0.0000148;  18 docs/s;   8681 sec\n",
            "[2021-05-02 08:46:18,107 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.35.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:46:50,395 INFO] Step 7450/50000; xent: 2.65; lr: 0.0000149;  17 docs/s;   8741 sec\n",
            "[2021-05-02 08:47:48,306 INFO] Step 7500/50000; xent: 2.75; lr: 0.0000150;  18 docs/s;   8799 sec\n",
            "[2021-05-02 08:48:46,085 INFO] Step 7550/50000; xent: 2.67; lr: 0.0000151;  18 docs/s;   8857 sec\n",
            "[2021-05-02 08:49:43,985 INFO] Step 7600/50000; xent: 2.69; lr: 0.0000152;  18 docs/s;   8915 sec\n",
            "[2021-05-02 08:50:01,225 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.37.bert.pt, number of examples: 2001\n",
            "[2021-05-02 08:50:42,467 INFO] Step 7650/50000; xent: 2.68; lr: 0.0000153;  18 docs/s;   8973 sec\n",
            "[2021-05-02 08:51:40,227 INFO] Step 7700/50000; xent: 2.59; lr: 0.0000154;  18 docs/s;   9031 sec\n",
            "[2021-05-02 08:52:38,239 INFO] Step 7750/50000; xent: 2.65; lr: 0.0000155;  18 docs/s;   9089 sec\n",
            "[2021-05-02 08:53:36,153 INFO] Step 7800/50000; xent: 2.63; lr: 0.0000156;  18 docs/s;   9147 sec\n",
            "[2021-05-02 08:53:44,135 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.80.bert.pt, number of examples: 2000\n",
            "[2021-05-02 08:54:35,162 INFO] Step 7850/50000; xent: 2.65; lr: 0.0000157;  18 docs/s;   9206 sec\n",
            "[2021-05-02 08:55:32,932 INFO] Step 7900/50000; xent: 2.68; lr: 0.0000158;  18 docs/s;   9264 sec\n",
            "[2021-05-02 08:56:30,846 INFO] Step 7950/50000; xent: 2.71; lr: 0.0000159;  18 docs/s;   9321 sec\n",
            "[2021-05-02 08:57:45,016 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.52.bert.pt, number of examples: 1999\n",
            "[2021-05-02 08:57:46,278 INFO] Step 8000/50000; xent: 2.62; lr: 0.0000160;  14 docs/s;   9397 sec\n",
            "[2021-05-02 08:57:46,293 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_8000.pt\n",
            "[2021-05-02 08:58:58,924 INFO] Step 8050/50000; xent: 2.57; lr: 0.0000161;  14 docs/s;   9470 sec\n",
            "[2021-05-02 08:59:56,123 INFO] Step 8100/50000; xent: 2.60; lr: 0.0000162;  18 docs/s;   9527 sec\n",
            "[2021-05-02 09:00:53,954 INFO] Step 8150/50000; xent: 2.63; lr: 0.0000163;  18 docs/s;   9585 sec\n",
            "[2021-05-02 09:01:42,337 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.12.bert.pt, number of examples: 1998\n",
            "[2021-05-02 09:01:52,819 INFO] Step 8200/50000; xent: 2.73; lr: 0.0000164;  18 docs/s;   9643 sec\n",
            "[2021-05-02 09:02:50,472 INFO] Step 8250/50000; xent: 2.64; lr: 0.0000165;  18 docs/s;   9701 sec\n",
            "[2021-05-02 09:03:48,479 INFO] Step 8300/50000; xent: 2.68; lr: 0.0000166;  18 docs/s;   9759 sec\n",
            "[2021-05-02 09:04:46,312 INFO] Step 8350/50000; xent: 2.65; lr: 0.0000167;  18 docs/s;   9817 sec\n",
            "[2021-05-02 09:05:25,187 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.114.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:05:44,874 INFO] Step 8400/50000; xent: 2.61; lr: 0.0000168;  18 docs/s;   9875 sec\n",
            "[2021-05-02 09:06:42,744 INFO] Step 8450/50000; xent: 2.64; lr: 0.0000169;  18 docs/s;   9933 sec\n",
            "[2021-05-02 09:07:40,082 INFO] Step 8500/50000; xent: 2.64; lr: 0.0000170;  18 docs/s;   9991 sec\n",
            "[2021-05-02 09:08:37,926 INFO] Step 8550/50000; xent: 2.74; lr: 0.0000171;  18 docs/s;  10049 sec\n",
            "[2021-05-02 09:09:08,657 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.86.bert.pt, number of examples: 1999\n",
            "[2021-05-02 09:09:36,390 INFO] Step 8600/50000; xent: 2.70; lr: 0.0000172;  17 docs/s;  10107 sec\n",
            "[2021-05-02 09:10:34,114 INFO] Step 8650/50000; xent: 2.69; lr: 0.0000173;  18 docs/s;  10165 sec\n",
            "[2021-05-02 09:11:31,572 INFO] Step 8700/50000; xent: 2.68; lr: 0.0000174;  19 docs/s;  10222 sec\n",
            "[2021-05-02 09:12:29,458 INFO] Step 8750/50000; xent: 2.56; lr: 0.0000175;  18 docs/s;  10280 sec\n",
            "[2021-05-02 09:12:51,218 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.113.bert.pt, number of examples: 2000\n",
            "[2021-05-02 09:13:27,881 INFO] Step 8800/50000; xent: 2.68; lr: 0.0000176;  18 docs/s;  10338 sec\n",
            "[2021-05-02 09:14:25,696 INFO] Step 8850/50000; xent: 2.62; lr: 0.0000177;  18 docs/s;  10396 sec\n",
            "[2021-05-02 09:15:23,610 INFO] Step 8900/50000; xent: 2.72; lr: 0.0000178;  18 docs/s;  10454 sec\n",
            "[2021-05-02 09:16:21,018 INFO] Step 8950/50000; xent: 2.66; lr: 0.0000179;  18 docs/s;  10512 sec\n",
            "[2021-05-02 09:16:34,883 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.96.bert.pt, number of examples: 1999\n",
            "[2021-05-02 09:17:20,183 INFO] Step 9000/50000; xent: 2.67; lr: 0.0000180;  18 docs/s;  10571 sec\n",
            "[2021-05-02 09:17:20,198 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_9000.pt\n",
            "[2021-05-02 09:18:32,328 INFO] Step 9050/50000; xent: 2.67; lr: 0.0000181;  14 docs/s;  10643 sec\n",
            "[2021-05-02 09:19:29,763 INFO] Step 9100/50000; xent: 2.65; lr: 0.0000182;  18 docs/s;  10700 sec\n",
            "[2021-05-02 09:20:27,340 INFO] Step 9150/50000; xent: 2.67; lr: 0.0000183;  18 docs/s;  10758 sec\n",
            "[2021-05-02 09:20:31,831 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.131.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:21:26,142 INFO] Step 9200/50000; xent: 2.64; lr: 0.0000184;  18 docs/s;  10817 sec\n",
            "[2021-05-02 09:22:23,811 INFO] Step 9250/50000; xent: 2.72; lr: 0.0000185;  18 docs/s;  10874 sec\n",
            "[2021-05-02 09:23:21,487 INFO] Step 9300/50000; xent: 2.66; lr: 0.0000186;  18 docs/s;  10932 sec\n",
            "[2021-05-02 09:24:14,439 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.97.bert.pt, number of examples: 2000\n",
            "[2021-05-02 09:24:20,255 INFO] Step 9350/50000; xent: 2.70; lr: 0.0000187;  18 docs/s;  10991 sec\n",
            "[2021-05-02 09:25:17,931 INFO] Step 9400/50000; xent: 2.64; lr: 0.0000188;  18 docs/s;  11049 sec\n",
            "[2021-05-02 09:26:15,778 INFO] Step 9450/50000; xent: 2.61; lr: 0.0000189;  18 docs/s;  11106 sec\n",
            "[2021-05-02 09:27:13,328 INFO] Step 9500/50000; xent: 2.72; lr: 0.0000190;  18 docs/s;  11164 sec\n",
            "[2021-05-02 09:27:58,982 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.55.bert.pt, number of examples: 1999\n",
            "[2021-05-02 09:28:11,714 INFO] Step 9550/50000; xent: 2.70; lr: 0.0000191;  18 docs/s;  11222 sec\n",
            "[2021-05-02 09:29:09,573 INFO] Step 9600/50000; xent: 2.63; lr: 0.0000192;  18 docs/s;  11280 sec\n",
            "[2021-05-02 09:30:07,496 INFO] Step 9650/50000; xent: 2.65; lr: 0.0000193;  18 docs/s;  11338 sec\n",
            "[2021-05-02 09:31:05,339 INFO] Step 9700/50000; xent: 2.61; lr: 0.0000194;  18 docs/s;  11396 sec\n",
            "[2021-05-02 09:31:41,041 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.128.bert.pt, number of examples: 2000\n",
            "[2021-05-02 09:32:04,144 INFO] Step 9750/50000; xent: 2.64; lr: 0.0000195;  17 docs/s;  11455 sec\n",
            "[2021-05-02 09:33:02,125 INFO] Step 9800/50000; xent: 2.64; lr: 0.0000196;  18 docs/s;  11513 sec\n",
            "[2021-05-02 09:33:59,596 INFO] Step 9850/50000; xent: 2.69; lr: 0.0000197;  18 docs/s;  11570 sec\n",
            "[2021-05-02 09:34:57,325 INFO] Step 9900/50000; xent: 2.58; lr: 0.0000198;  18 docs/s;  11628 sec\n",
            "[2021-05-02 09:35:23,327 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.104.bert.pt, number of examples: 2000\n",
            "[2021-05-02 09:35:55,560 INFO] Step 9950/50000; xent: 2.57; lr: 0.0000199;  18 docs/s;  11686 sec\n",
            "[2021-05-02 09:36:53,402 INFO] Step 10000/50000; xent: 2.66; lr: 0.0000200;  18 docs/s;  11744 sec\n",
            "[2021-05-02 09:36:53,419 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_10000.pt\n",
            "[2021-05-02 09:38:04,850 INFO] Step 10050/50000; xent: 2.61; lr: 0.0000200;  15 docs/s;  11815 sec\n",
            "[2021-05-02 09:39:02,289 INFO] Step 10100/50000; xent: 2.64; lr: 0.0000199;  18 docs/s;  11873 sec\n",
            "[2021-05-02 09:39:19,623 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.118.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:40:01,442 INFO] Step 10150/50000; xent: 2.66; lr: 0.0000199;  17 docs/s;  11932 sec\n",
            "[2021-05-02 09:40:58,940 INFO] Step 10200/50000; xent: 2.58; lr: 0.0000198;  18 docs/s;  11990 sec\n",
            "[2021-05-02 09:41:56,685 INFO] Step 10250/50000; xent: 2.69; lr: 0.0000198;  18 docs/s;  12047 sec\n",
            "[2021-05-02 09:42:54,693 INFO] Step 10300/50000; xent: 2.48; lr: 0.0000197;  18 docs/s;  12105 sec\n",
            "[2021-05-02 09:43:02,530 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.47.bert.pt, number of examples: 2000\n",
            "[2021-05-02 09:43:53,392 INFO] Step 10350/50000; xent: 2.66; lr: 0.0000197;  18 docs/s;  12164 sec\n",
            "[2021-05-02 09:44:51,089 INFO] Step 10400/50000; xent: 2.67; lr: 0.0000196;  18 docs/s;  12222 sec\n",
            "[2021-05-02 09:45:48,866 INFO] Step 10450/50000; xent: 2.68; lr: 0.0000196;  18 docs/s;  12279 sec\n",
            "[2021-05-02 09:46:45,420 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.8.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:46:47,833 INFO] Step 10500/50000; xent: 2.68; lr: 0.0000195;  18 docs/s;  12338 sec\n",
            "[2021-05-02 09:47:45,863 INFO] Step 10550/50000; xent: 2.65; lr: 0.0000195;  18 docs/s;  12396 sec\n",
            "[2021-05-02 09:48:43,191 INFO] Step 10600/50000; xent: 2.64; lr: 0.0000194;  18 docs/s;  12454 sec\n",
            "[2021-05-02 09:49:41,059 INFO] Step 10650/50000; xent: 2.62; lr: 0.0000194;  18 docs/s;  12512 sec\n",
            "[2021-05-02 09:50:28,251 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.11.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:50:39,926 INFO] Step 10700/50000; xent: 2.60; lr: 0.0000193;  18 docs/s;  12571 sec\n",
            "[2021-05-02 09:51:37,509 INFO] Step 10750/50000; xent: 2.65; lr: 0.0000193;  18 docs/s;  12628 sec\n",
            "[2021-05-02 09:52:35,567 INFO] Step 10800/50000; xent: 2.57; lr: 0.0000192;  18 docs/s;  12686 sec\n",
            "[2021-05-02 09:53:33,235 INFO] Step 10850/50000; xent: 2.55; lr: 0.0000192;  18 docs/s;  12744 sec\n",
            "[2021-05-02 09:54:11,728 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.66.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:54:31,523 INFO] Step 10900/50000; xent: 2.66; lr: 0.0000192;  18 docs/s;  12802 sec\n",
            "[2021-05-02 09:55:29,507 INFO] Step 10950/50000; xent: 2.62; lr: 0.0000191;  18 docs/s;  12860 sec\n",
            "[2021-05-02 09:56:27,316 INFO] Step 11000/50000; xent: 2.62; lr: 0.0000191;  18 docs/s;  12918 sec\n",
            "[2021-05-02 09:56:27,332 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_11000.pt\n",
            "[2021-05-02 09:57:39,271 INFO] Step 11050/50000; xent: 2.74; lr: 0.0000190;  14 docs/s;  12990 sec\n",
            "[2021-05-02 09:58:08,457 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.36.bert.pt, number of examples: 2001\n",
            "[2021-05-02 09:58:37,344 INFO] Step 11100/50000; xent: 2.55; lr: 0.0000190;  18 docs/s;  13048 sec\n",
            "[2021-05-02 09:59:35,235 INFO] Step 11150/50000; xent: 2.58; lr: 0.0000189;  18 docs/s;  13106 sec\n",
            "[2021-05-02 10:00:33,057 INFO] Step 11200/50000; xent: 2.61; lr: 0.0000189;  18 docs/s;  13164 sec\n",
            "[2021-05-02 10:01:30,584 INFO] Step 11250/50000; xent: 2.66; lr: 0.0000189;  18 docs/s;  13221 sec\n",
            "[2021-05-02 10:01:52,355 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.73.bert.pt, number of examples: 2001\n",
            "[2021-05-02 10:02:29,543 INFO] Step 11300/50000; xent: 2.69; lr: 0.0000188;  17 docs/s;  13280 sec\n",
            "[2021-05-02 10:03:27,319 INFO] Step 11350/50000; xent: 2.59; lr: 0.0000188;  18 docs/s;  13338 sec\n",
            "[2021-05-02 10:04:24,809 INFO] Step 11400/50000; xent: 2.65; lr: 0.0000187;  18 docs/s;  13395 sec\n",
            "[2021-05-02 10:05:22,546 INFO] Step 11450/50000; xent: 2.62; lr: 0.0000187;  18 docs/s;  13453 sec\n",
            "[2021-05-02 10:05:36,219 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.33.bert.pt, number of examples: 1997\n",
            "[2021-05-02 10:06:21,147 INFO] Step 11500/50000; xent: 2.61; lr: 0.0000187;  18 docs/s;  13512 sec\n",
            "[2021-05-02 10:07:18,882 INFO] Step 11550/50000; xent: 2.70; lr: 0.0000186;  18 docs/s;  13569 sec\n",
            "[2021-05-02 10:08:16,873 INFO] Step 11600/50000; xent: 2.63; lr: 0.0000186;  18 docs/s;  13627 sec\n",
            "[2021-05-02 10:09:14,413 INFO] Step 11650/50000; xent: 2.59; lr: 0.0000185;  18 docs/s;  13685 sec\n",
            "[2021-05-02 10:09:19,056 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.121.bert.pt, number of examples: 1999\n",
            "[2021-05-02 10:10:13,655 INFO] Step 11700/50000; xent: 2.68; lr: 0.0000185;  18 docs/s;  13744 sec\n",
            "[2021-05-02 10:11:11,294 INFO] Step 11750/50000; xent: 2.60; lr: 0.0000185;  18 docs/s;  13802 sec\n",
            "[2021-05-02 10:12:08,990 INFO] Step 11800/50000; xent: 2.61; lr: 0.0000184;  18 docs/s;  13860 sec\n",
            "[2021-05-02 10:13:02,629 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.136.bert.pt, number of examples: 1998\n",
            "[2021-05-02 10:13:08,506 INFO] Step 11850/50000; xent: 2.60; lr: 0.0000184;  18 docs/s;  13919 sec\n",
            "[2021-05-02 10:14:06,536 INFO] Step 11900/50000; xent: 2.61; lr: 0.0000183;  18 docs/s;  13977 sec\n",
            "[2021-05-02 10:15:04,239 INFO] Step 11950/50000; xent: 2.64; lr: 0.0000183;  18 docs/s;  14035 sec\n",
            "[2021-05-02 10:16:01,930 INFO] Step 12000/50000; xent: 2.62; lr: 0.0000183;  18 docs/s;  14093 sec\n",
            "[2021-05-02 10:16:01,946 INFO] Saving checkpoint ../data/trained_models/reberta_2layers/model_step_12000.pt\n",
            "[2021-05-02 10:17:00,150 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.20.bert.pt, number of examples: 2000\n",
            "[2021-05-02 10:17:15,231 INFO] Step 12050/50000; xent: 2.62; lr: 0.0000182;  14 docs/s;  14166 sec\n",
            "[2021-05-02 10:18:12,937 INFO] Step 12100/50000; xent: 2.76; lr: 0.0000182;  18 docs/s;  14224 sec\n",
            "[2021-05-02 10:19:10,426 INFO] Step 12150/50000; xent: 2.58; lr: 0.0000181;  18 docs/s;  14281 sec\n",
            "[2021-05-02 10:20:07,672 INFO] Step 12200/50000; xent: 2.62; lr: 0.0000181;  18 docs/s;  14338 sec\n",
            "[2021-05-02 10:20:42,038 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.61.bert.pt, number of examples: 1999\n",
            "[2021-05-02 10:21:06,192 INFO] Step 12250/50000; xent: 2.66; lr: 0.0000181;  18 docs/s;  14397 sec\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 147, in <module>\n",
            "    train_ext(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 204, in train_ext\n",
            "    train_single_ext(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 247, in train_single_ext\n",
            "    trainer.train(train_iter_fct, args.train_steps)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 152, in train\n",
            "    report_stats)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 316, in _gradient_accumulation\n",
            "    (loss / loss.numel()).backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 147, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkl9MtIaJqTF"
      },
      "source": [
        "## Testing on Step 67000 41.86/19.06/38.32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlQULi4yJY_Z",
        "outputId": "d9cc12d4-f274-4a5f-d18e-2d57f1c2f5ca"
      },
      "source": [
        "!python3 train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -log_file ../logs/test_my_roberta_smaller.log -model_path ../data/trained_models/reberta_smaller -sep_optim true -use_interval false -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_roberta_ext_bert_cnndm -test_from ../data/trained_models/reberta_smaller/model_step_67000.pt -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-05-02 03:37:11,559 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_67000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/roberta_cnndm/tokenized', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='roberta', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=1, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_roberta_smaller.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/reberta_smaller', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_roberta_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/reberta_smaller/model_step_67000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-05-02 03:37:39,580 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 03:37:39,895 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 03:37:39,942 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 03:38:23,070 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-05-02 03:38:23,075 INFO] * number of parameters: 130161921\n",
            "[2021-05-02 03:39:40,754 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.1.bert.pt, number of examples: 2001\n",
            "[2021-05-02 03:41:02,316 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.2.bert.pt, number of examples: 2001\n",
            "[2021-05-02 03:42:24,063 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.3.bert.pt, number of examples: 2001\n",
            "[2021-05-02 03:43:45,708 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.4.bert.pt, number of examples: 2001\n",
            "[2021-05-02 03:45:07,453 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.5.bert.pt, number of examples: 1485\n",
            "11490\n",
            "11490\n",
            "2021-05-02 03:49:28,237 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-05-02 03:49:28,237 INFO] Writing summaries.\n",
            "2021-05-02 03:49:28,241 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmp07n5m9d1/system and model files to ../temp/tmp07n5m9d1/model.\n",
            "[2021-05-02 03:49:28,241 INFO] Processing summaries. Saving system files to ../temp/tmp07n5m9d1/system and model files to ../temp/tmp07n5m9d1/model.\n",
            "2021-05-02 03:49:28,242 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-03-46-07/candidate/.\n",
            "[2021-05-02 03:49:28,242 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-03-46-07/candidate/.\n",
            "2021-05-02 03:52:35,285 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmp07n5m9d1/system.\n",
            "[2021-05-02 03:52:35,285 INFO] Saved processed files to ../temp/tmp07n5m9d1/system.\n",
            "2021-05-02 03:52:35,286 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-03-46-07/reference/.\n",
            "[2021-05-02 03:52:35,286 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-03-46-07/reference/.\n",
            "2021-05-02 03:55:42,045 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmp07n5m9d1/model.\n",
            "[2021-05-02 03:55:42,045 INFO] Saved processed files to ../temp/tmp07n5m9d1/model.\n",
            "2021-05-02 03:55:42,258 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmp2vfeygid/rouge_conf.xml\n",
            "[2021-05-02 03:55:42,258 INFO] Written ROUGE configuration to ../temp/tmp2vfeygid/rouge_conf.xml\n",
            "2021-05-02 03:55:42,258 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmp2vfeygid/rouge_conf.xml\n",
            "[2021-05-02 03:55:42,258 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmp2vfeygid/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.51151 (95%-conf.int. 0.50911 - 0.51420)\n",
            "1 ROUGE-1 Average_P: 0.37659 (95%-conf.int. 0.37377 - 0.37905)\n",
            "1 ROUGE-1 Average_F: 0.41863 (95%-conf.int. 0.41642 - 0.42071)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.23274 (95%-conf.int. 0.23012 - 0.23547)\n",
            "1 ROUGE-2 Average_P: 0.17203 (95%-conf.int. 0.16984 - 0.17419)\n",
            "1 ROUGE-2 Average_F: 0.19064 (95%-conf.int. 0.18843 - 0.19280)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.46766 (95%-conf.int. 0.46517 - 0.47031)\n",
            "1 ROUGE-L Average_P: 0.34503 (95%-conf.int. 0.34240 - 0.34745)\n",
            "1 ROUGE-L Average_F: 0.38322 (95%-conf.int. 0.38109 - 0.38531)\n",
            "\n",
            "[2021-05-02 04:00:05,093 INFO] Rouges at step 67000 \n",
            ">> ROUGE-F(1/2/3/l): 41.86/19.06/38.32\n",
            "ROUGE-R(1/2/3/l): 51.15/23.27/46.77\n",
            "\n",
            "[2021-05-02 04:00:05,094 INFO] Validation xent: 7.01478 at step 67000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcbS7TUsP8Fx"
      },
      "source": [
        "## Testing on Step 45000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5XjUUpEP1rI",
        "outputId": "7a5aecad-7690-4d65-8423-076045d4cec0"
      },
      "source": [
        "!python3 train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -log_file ../logs/test_my_roberta_smaller.log -model_path ../data/trained_models/reberta_smaller -sep_optim true -use_interval false -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_roberta_ext_bert_cnndm -test_from ../data/trained_models/reberta_smaller/model_step_45000.pt -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-05-02 04:04:28,365 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_45000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/roberta_cnndm/tokenized', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='roberta', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=1, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_roberta_smaller.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/reberta_smaller', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_roberta_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/reberta_smaller/model_step_45000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-05-02 04:04:51,035 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 04:04:51,037 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 04:04:51,072 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 04:05:08,602 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-05-02 04:05:08,614 INFO] * number of parameters: 130161921\n",
            "[2021-05-02 04:06:23,766 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.1.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:07:42,363 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.2.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:09:02,494 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.3.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:10:23,863 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.4.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:11:45,293 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.5.bert.pt, number of examples: 1485\n",
            "11490\n",
            "11490\n",
            "2021-05-02 04:18:29,577 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-05-02 04:18:29,577 INFO] Writing summaries.\n",
            "2021-05-02 04:18:29,582 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmp1aoj1ptv/system and model files to ../temp/tmp1aoj1ptv/model.\n",
            "[2021-05-02 04:18:29,582 INFO] Processing summaries. Saving system files to ../temp/tmp1aoj1ptv/system and model files to ../temp/tmp1aoj1ptv/model.\n",
            "2021-05-02 04:18:29,582 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-04-12-45/candidate/.\n",
            "[2021-05-02 04:18:29,582 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-04-12-45/candidate/.\n",
            "2021-05-02 04:21:37,138 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmp1aoj1ptv/system.\n",
            "[2021-05-02 04:21:37,138 INFO] Saved processed files to ../temp/tmp1aoj1ptv/system.\n",
            "2021-05-02 04:21:37,139 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-04-12-45/reference/.\n",
            "[2021-05-02 04:21:37,139 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-04-12-45/reference/.\n",
            "2021-05-02 04:24:44,632 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmp1aoj1ptv/model.\n",
            "[2021-05-02 04:24:44,632 INFO] Saved processed files to ../temp/tmp1aoj1ptv/model.\n",
            "2021-05-02 04:24:44,836 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmpwdy9uyd3/rouge_conf.xml\n",
            "[2021-05-02 04:24:44,836 INFO] Written ROUGE configuration to ../temp/tmpwdy9uyd3/rouge_conf.xml\n",
            "2021-05-02 04:24:44,836 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpwdy9uyd3/rouge_conf.xml\n",
            "[2021-05-02 04:24:44,836 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpwdy9uyd3/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.53021 (95%-conf.int. 0.52744 - 0.53290)\n",
            "1 ROUGE-1 Average_P: 0.38263 (95%-conf.int. 0.38001 - 0.38516)\n",
            "1 ROUGE-1 Average_F: 0.42943 (95%-conf.int. 0.42731 - 0.43155)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.24827 (95%-conf.int. 0.24553 - 0.25111)\n",
            "1 ROUGE-2 Average_P: 0.17982 (95%-conf.int. 0.17745 - 0.18208)\n",
            "1 ROUGE-2 Average_F: 0.20119 (95%-conf.int. 0.19890 - 0.20359)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.48544 (95%-conf.int. 0.48277 - 0.48796)\n",
            "1 ROUGE-L Average_P: 0.35105 (95%-conf.int. 0.34851 - 0.35345)\n",
            "1 ROUGE-L Average_F: 0.39365 (95%-conf.int. 0.39144 - 0.39578)\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 157, in <module>\n",
            "    test_ext(args, device_id, cp, step)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 198, in test_ext\n",
            "    trainer.test(test_iter, step)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 290, in test\n",
            "    rouges = test_rouge(self.args.temp_dir, can_path, gold_path)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/others/utils.py\", line 94, in test_rouge\n",
            "    shutil.rmtree(tmp_dir)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 494, in rmtree\n",
            "    _rmtree_safe_fd(fd, path, onerror)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 432, in _rmtree_safe_fd\n",
            "    _rmtree_safe_fd(dirfd, fullname, onerror)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 450, in _rmtree_safe_fd\n",
            "    os.unlink(entry.name, dir_fd=topfd)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGzx91DTWV93"
      },
      "source": [
        "## Testing on Step 46000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBu5phcwV9Ee",
        "outputId": "825a40d7-00ef-46af-ddfc-7694e6c43b28"
      },
      "source": [
        "!python3 train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -log_file ../logs/test_my_roberta_smaller.log -model_path ../data/trained_models/reberta_smaller -sep_optim true -use_interval false -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_roberta_ext_bert_cnndm -test_from ../data/trained_models/reberta_smaller/model_step_46000.pt -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-05-02 04:31:14,193 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_46000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/roberta_cnndm/tokenized', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='roberta', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=1, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_roberta_smaller.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/reberta_smaller', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_roberta_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/reberta_smaller/model_step_46000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-05-02 04:31:30,082 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 04:31:30,083 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 04:31:30,128 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 04:31:38,355 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-05-02 04:31:38,361 INFO] * number of parameters: 130161921\n",
            "[2021-05-02 04:32:53,798 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.1.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:34:12,524 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.2.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:35:33,468 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.3.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:36:55,497 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.4.bert.pt, number of examples: 2001\n",
            "[2021-05-02 04:38:16,340 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.5.bert.pt, number of examples: 1485\n",
            "11490\n",
            "11490\n",
            "2021-05-02 04:45:09,430 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-05-02 04:45:09,430 INFO] Writing summaries.\n",
            "2021-05-02 04:45:09,554 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmpk7ayigfo/system and model files to ../temp/tmpk7ayigfo/model.\n",
            "[2021-05-02 04:45:09,554 INFO] Processing summaries. Saving system files to ../temp/tmpk7ayigfo/system and model files to ../temp/tmpk7ayigfo/model.\n",
            "2021-05-02 04:45:09,555 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-04-39-16/candidate/.\n",
            "[2021-05-02 04:45:09,555 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-04-39-16/candidate/.\n",
            "2021-05-02 04:48:27,107 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpk7ayigfo/system.\n",
            "[2021-05-02 04:48:27,107 INFO] Saved processed files to ../temp/tmpk7ayigfo/system.\n",
            "2021-05-02 04:48:27,108 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-04-39-16/reference/.\n",
            "[2021-05-02 04:48:27,108 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-04-39-16/reference/.\n",
            "2021-05-02 04:51:34,210 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpk7ayigfo/model.\n",
            "[2021-05-02 04:51:34,210 INFO] Saved processed files to ../temp/tmpk7ayigfo/model.\n",
            "2021-05-02 04:51:34,463 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmpstda6iwk/rouge_conf.xml\n",
            "[2021-05-02 04:51:34,463 INFO] Written ROUGE configuration to ../temp/tmpstda6iwk/rouge_conf.xml\n",
            "2021-05-02 04:51:34,464 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpstda6iwk/rouge_conf.xml\n",
            "[2021-05-02 04:51:34,464 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpstda6iwk/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.53191 (95%-conf.int. 0.52939 - 0.53462)\n",
            "1 ROUGE-1 Average_P: 0.38307 (95%-conf.int. 0.38041 - 0.38568)\n",
            "1 ROUGE-1 Average_F: 0.43007 (95%-conf.int. 0.42791 - 0.43222)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.25026 (95%-conf.int. 0.24759 - 0.25301)\n",
            "1 ROUGE-2 Average_P: 0.18071 (95%-conf.int. 0.17829 - 0.18301)\n",
            "1 ROUGE-2 Average_F: 0.20232 (95%-conf.int. 0.19996 - 0.20463)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.48722 (95%-conf.int. 0.48473 - 0.48985)\n",
            "1 ROUGE-L Average_P: 0.35157 (95%-conf.int. 0.34896 - 0.35404)\n",
            "1 ROUGE-L Average_F: 0.39438 (95%-conf.int. 0.39221 - 0.39651)\n",
            "\n",
            "[2021-05-02 04:59:33,159 INFO] Rouges at step 46000 \n",
            ">> ROUGE-F(1/2/3/l): 43.01/20.23/39.44\n",
            "ROUGE-R(1/2/3/l): 53.19/25.03/48.72\n",
            "\n",
            "[2021-05-02 04:59:33,159 INFO] Validation xent: 5.16639 at step 46000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q52Whn3_WXnc"
      },
      "source": [
        "## Testing on Step 47000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFvcQH2rV89Y",
        "outputId": "857cbc8f-37aa-4d55-b286-fb5ad96fa68d"
      },
      "source": [
        "!python3 train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -log_file ../logs/test_my_roberta_smaller.log -model_path ../data/trained_models/reberta_smaller -sep_optim true -use_interval false -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_roberta_ext_bert_cnndm -test_from ../data/trained_models/reberta_smaller/model_step_47000.pt -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-05-02 04:59:37,005 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_47000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/roberta_cnndm/tokenized', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='roberta', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=1, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_roberta_smaller.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/reberta_smaller', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_roberta_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/reberta_smaller/model_step_47000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-05-02 04:59:53,497 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 04:59:53,498 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 04:59:53,542 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 05:00:03,208 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-05-02 05:00:03,214 INFO] * number of parameters: 130161921\n",
            "[2021-05-02 05:01:18,938 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.1.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:02:37,917 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.2.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:03:58,517 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.3.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:05:19,534 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.4.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:06:40,421 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.5.bert.pt, number of examples: 1485\n",
            "11490\n",
            "11490\n",
            "2021-05-02 05:13:33,072 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-05-02 05:13:33,072 INFO] Writing summaries.\n",
            "2021-05-02 05:13:33,076 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmpmztc4gae/system and model files to ../temp/tmpmztc4gae/model.\n",
            "[2021-05-02 05:13:33,076 INFO] Processing summaries. Saving system files to ../temp/tmpmztc4gae/system and model files to ../temp/tmpmztc4gae/model.\n",
            "2021-05-02 05:13:33,077 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-05-07-40/candidate/.\n",
            "[2021-05-02 05:13:33,077 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-05-07-40/candidate/.\n",
            "2021-05-02 05:16:41,778 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpmztc4gae/system.\n",
            "[2021-05-02 05:16:41,778 INFO] Saved processed files to ../temp/tmpmztc4gae/system.\n",
            "2021-05-02 05:16:41,779 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-05-07-40/reference/.\n",
            "[2021-05-02 05:16:41,779 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-05-07-40/reference/.\n",
            "2021-05-02 05:19:51,331 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpmztc4gae/model.\n",
            "[2021-05-02 05:19:51,331 INFO] Saved processed files to ../temp/tmpmztc4gae/model.\n",
            "2021-05-02 05:19:51,548 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmps76q0ivz/rouge_conf.xml\n",
            "[2021-05-02 05:19:51,548 INFO] Written ROUGE configuration to ../temp/tmps76q0ivz/rouge_conf.xml\n",
            "2021-05-02 05:19:51,548 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmps76q0ivz/rouge_conf.xml\n",
            "[2021-05-02 05:19:51,548 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmps76q0ivz/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.52981 (95%-conf.int. 0.52725 - 0.53233)\n",
            "1 ROUGE-1 Average_P: 0.38443 (95%-conf.int. 0.38177 - 0.38680)\n",
            "1 ROUGE-1 Average_F: 0.43029 (95%-conf.int. 0.42817 - 0.43229)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.24852 (95%-conf.int. 0.24579 - 0.25130)\n",
            "1 ROUGE-2 Average_P: 0.18107 (95%-conf.int. 0.17870 - 0.18325)\n",
            "1 ROUGE-2 Average_F: 0.20203 (95%-conf.int. 0.19971 - 0.20417)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.48546 (95%-conf.int. 0.48288 - 0.48802)\n",
            "1 ROUGE-L Average_P: 0.35289 (95%-conf.int. 0.35029 - 0.35523)\n",
            "1 ROUGE-L Average_F: 0.39469 (95%-conf.int. 0.39255 - 0.39674)\n",
            "\n",
            "[2021-05-02 05:27:53,522 INFO] Rouges at step 47000 \n",
            ">> ROUGE-F(1/2/3/l): 43.03/20.20/39.47\n",
            "ROUGE-R(1/2/3/l): 52.98/24.85/48.55\n",
            "\n",
            "[2021-05-02 05:27:53,523 INFO] Validation xent: 5.21381 at step 47000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5U7jiMWY0R"
      },
      "source": [
        "## Testing on Step 48000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV028SzsV8zK",
        "outputId": "ad6f3be3-ffbb-443c-a0a9-623f9cf2969f"
      },
      "source": [
        "!python3 train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -log_file ../logs/test_my_roberta_smaller.log -model_path ../data/trained_models/reberta_smaller -sep_optim true -use_interval false -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_roberta_ext_bert_cnndm -test_from ../data/trained_models/reberta_smaller/model_step_48000.pt -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-05-02 05:27:57,353 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_48000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/roberta_cnndm/tokenized', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='roberta', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=1, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_roberta_smaller.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/reberta_smaller', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_roberta_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/reberta_smaller/model_step_48000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-05-02 05:28:13,215 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 05:28:13,216 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 05:28:13,245 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 05:28:24,124 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-05-02 05:28:24,130 INFO] * number of parameters: 130161921\n",
            "[2021-05-02 05:29:39,915 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.1.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:30:58,743 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.2.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:32:19,362 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.3.bert.pt, number of examples: 2001\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 157, in <module>\n",
            "    test_ext(args, device_id, cp, step)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 198, in test_ext\n",
            "    trainer.test(test_iter, step)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 288, in test\n",
            "    save_pred.write(pred[i].strip() + '\\n')\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGz0U5D8Waii"
      },
      "source": [
        "## Testing on Step 50000 43.23/20.39/39.67\n",
        "\n",
        "\n",
        "\n",
        "> ROUGE-F(1/2/3/l): 43.23/20.39/39.67 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q6s2Z_AWBrD",
        "outputId": "7ef192dc-5806-48ab-f36c-63c2796f6e0a"
      },
      "source": [
        "!python3 train.py -task ext -mode test -batch_size 3000 -test_batch_size 500 -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -log_file ../logs/test_my_roberta_smaller.log -model_path ../data/trained_models/reberta_smaller -sep_optim true -use_interval false -visible_gpus 1 -max_pos 512 -max_length 200 -alpha 0.95 -min_length 50 -result_path ../logs/my_roberta_ext_bert_cnndm -test_from ../data/trained_models/reberta_smaller/model_step_50000.pt -ext_layers 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "Testing...\n",
            "device:  cuda\n",
            "[2021-05-02 05:34:10,529 INFO] Loading checkpoint from ../data/trained_models/reberta_smaller/model_step_50000.pt\n",
            "Namespace(accum_count=1, alpha=0.95, batch_size=3000, beam_size=5, bert_data_path='../data/preprocessed_data/roberta_cnndm/tokenized', beta1=0.9, beta2=0.999, block_trigram=True, dec_dropout=0.2, dec_ff_size=2048, dec_heads=8, dec_hidden_size=768, dec_layers=6, enc_dropout=0.2, enc_ff_size=512, enc_hidden_size=512, enc_layers=6, encoder='roberta', ext_dropout=0.2, ext_ff_size=2048, ext_heads=8, ext_hidden_size=768, ext_layers=1, finetune_bert=True, generator_shard_size=32, gpu_ranks=[0], label_smoothing=0.1, large=False, load_from_extractive='', log_file='../logs/test_my_roberta_smaller.log', lr=1, lr_bert=0.002, lr_dec=0.002, max_grad_norm=0, max_length=200, max_pos=512, max_tgt_len=140, min_length=50, mode='test', model_path='../data/trained_models/reberta_smaller', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='../logs/my_roberta_ext_bert_cnndm', save_checkpoint_steps=5, seed=666, sep_optim=True, share_emb=False, task='ext', temp_dir='../temp', test_all=False, test_batch_size=500, test_from='../data/trained_models/reberta_smaller/model_step_50000.pt', test_start_from=-1, train_from='', train_steps=1000, use_bert_emb=False, use_interval=True, visible_gpus='1', warmup_steps=8000, warmup_steps_bert=8000, warmup_steps_dec=8000, world_size=1)\n",
            "[2021-05-02 05:34:27,644 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at ../temp/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "[2021-05-02 05:34:27,695 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-02 05:34:27,736 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at ../temp/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "[2021-05-02 05:34:36,813 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.0.bert.pt, number of examples: 2001\n",
            "gpu_rank 0\n",
            "[2021-05-02 05:34:36,826 INFO] * number of parameters: 130161921\n",
            "[2021-05-02 05:37:09,965 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.1.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:38:29,123 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.2.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:39:49,966 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.3.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:41:10,721 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.4.bert.pt, number of examples: 2001\n",
            "[2021-05-02 05:42:31,991 INFO] Loading test dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.test.5.bert.pt, number of examples: 1485\n",
            "11490\n",
            "11490\n",
            "2021-05-02 05:49:15,237 [MainThread  ] [INFO ]  Writing summaries.\n",
            "[2021-05-02 05:49:15,237 INFO] Writing summaries.\n",
            "2021-05-02 05:49:15,242 [MainThread  ] [INFO ]  Processing summaries. Saving system files to ../temp/tmpylq8hiqb/system and model files to ../temp/tmpylq8hiqb/model.\n",
            "[2021-05-02 05:49:15,242 INFO] Processing summaries. Saving system files to ../temp/tmpylq8hiqb/system and model files to ../temp/tmpylq8hiqb/model.\n",
            "2021-05-02 05:49:15,243 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-05-43-32/candidate/.\n",
            "[2021-05-02 05:49:15,243 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-05-43-32/candidate/.\n",
            "2021-05-02 05:52:22,055 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpylq8hiqb/system.\n",
            "[2021-05-02 05:52:22,055 INFO] Saved processed files to ../temp/tmpylq8hiqb/system.\n",
            "2021-05-02 05:52:22,056 [MainThread  ] [INFO ]  Processing files in ../temp/rouge-tmp-2021-05-02-05-43-32/reference/.\n",
            "[2021-05-02 05:52:22,056 INFO] Processing files in ../temp/rouge-tmp-2021-05-02-05-43-32/reference/.\n",
            "2021-05-02 05:55:28,283 [MainThread  ] [INFO ]  Saved processed files to ../temp/tmpylq8hiqb/model.\n",
            "[2021-05-02 05:55:28,283 INFO] Saved processed files to ../temp/tmpylq8hiqb/model.\n",
            "2021-05-02 05:55:28,487 [MainThread  ] [INFO ]  Written ROUGE configuration to ../temp/tmpq11q3610/rouge_conf.xml\n",
            "[2021-05-02 05:55:28,487 INFO] Written ROUGE configuration to ../temp/tmpq11q3610/rouge_conf.xml\n",
            "2021-05-02 05:55:28,488 [MainThread  ] [INFO ]  Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpq11q3610/rouge_conf.xml\n",
            "[2021-05-02 05:55:28,488 INFO] Running ROUGE with command /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/ROUGE-1.5.5.pl -e /content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/pyrouge/tools/ROUGE-1.5.5/data -c 95 -m -r 1000 -n 2 -a ../temp/tmpq11q3610/rouge_conf.xml\n",
            "---------------------------------------------\n",
            "1 ROUGE-1 Average_R: 0.53043 (95%-conf.int. 0.52771 - 0.53305)\n",
            "1 ROUGE-1 Average_P: 0.38704 (95%-conf.int. 0.38435 - 0.38958)\n",
            "1 ROUGE-1 Average_F: 0.43225 (95%-conf.int. 0.42998 - 0.43428)\n",
            "---------------------------------------------\n",
            "1 ROUGE-2 Average_R: 0.24997 (95%-conf.int. 0.24728 - 0.25272)\n",
            "1 ROUGE-2 Average_P: 0.18320 (95%-conf.int. 0.18080 - 0.18556)\n",
            "1 ROUGE-2 Average_F: 0.20392 (95%-conf.int. 0.20160 - 0.20624)\n",
            "---------------------------------------------\n",
            "1 ROUGE-L Average_R: 0.48624 (95%-conf.int. 0.48354 - 0.48880)\n",
            "1 ROUGE-L Average_P: 0.35553 (95%-conf.int. 0.35285 - 0.35805)\n",
            "1 ROUGE-L Average_F: 0.39672 (95%-conf.int. 0.39448 - 0.39877)\n",
            "\n",
            "[2021-05-02 06:03:21,401 INFO] Rouges at step 50000 \n",
            ">> ROUGE-F(1/2/3/l): 43.23/20.39/39.67\n",
            "ROUGE-R(1/2/3/l): 53.04/25.00/48.62\n",
            "\n",
            "[2021-05-02 06:03:21,401 INFO] Validation xent: 5.14096 at step 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlqNbwnI-Os2",
        "outputId": "83556107-4365-42ce-f5a2-0b1a8976edfd"
      },
      "source": [
        "!python3 train.py -task ext -encoder roberta -mode train -bert_data_path ../data/preprocessed_data/roberta_cnndm/tokenized -ext_dropout 0.1 -model_path ../data/trained_models/roberta_large -lr 2e-3 -visible_gpus 0 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -train_steps 50000 -accum_count 2 -log_file ../logs/ext_roberta2_cnndm -use_interval true -warmup_steps 10000 -max_pos 512 -ext_layers 2 -large true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  0\n",
            "----TRAIN EXTRACTIVE----\n",
            "[2021-05-03 06:55:13,557 INFO] Device ID 0\n",
            "[2021-05-03 06:55:13,558 INFO] Device cuda\n",
            "[2021-05-03 06:55:13,934 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at ../temp/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.2d28da311092e99a05f9ee17520204614d60b0bfdb32f8a75644df7737b6a748\n",
            "[2021-05-03 06:55:13,935 INFO] Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[2021-05-03 06:55:14,216 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at ../temp/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n",
            "[2021-05-03 06:55:32,548 INFO] ExtSummarizer(\n",
            "  (bert): Roberta(\n",
            "    (model): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n",
            "        (position_embeddings): Embedding(514, 1024)\n",
            "        (token_type_embeddings): Embedding(1, 1024)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (12): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (13): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (14): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (15): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (16): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (17): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (18): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (19): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (20): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (21): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (22): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (23): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ext_layer): ExtTransformerEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=1024, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2021-05-03 06:55:32,661 INFO] * number of parameters: 372162561\n",
            "[2021-05-03 06:55:32,661 INFO] Start training...\n",
            "[2021-05-03 06:55:32,893 INFO] Loading train dataset from ../data/preprocessed_data/roberta_cnndm/tokenized.train.123.bert.pt, number of examples: 2000\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 147, in <module>\n",
            "    train_ext(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 204, in train_ext\n",
            "    train_single_ext(args, device_id)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/train_extractive.py\", line 247, in train_single_ext\n",
            "    trainer.train(train_iter_fct, args.train_steps)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 152, in train\n",
            "    report_stats)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/trainer_ext.py\", line 312, in _gradient_accumulation\n",
            "    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/model_builder.py\", line 277, in forward\n",
            "    top_vec = self.bert(src, None, mask_src)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/.shortcut-targets-by-id/13NCxThpoCnV67kBccYli2itah05-zt7J/nn4nlp_project/files/models/model_builder.py\", line 190, in forward\n",
            "    top_vec, _ = self.model(x,  segs, attention_mask=mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_transformers/modeling_roberta.py\", line 178, in forward\n",
            "    return super(RobertaModel, self).forward(input_ids, token_type_ids, attention_mask, position_ids, head_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_transformers/modeling_bert.py\", line 710, in forward\n",
            "    head_mask=head_mask)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_transformers/modeling_bert.py\", line 431, in forward\n",
            "    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_transformers/modeling_bert.py\", line 411, in forward\n",
            "    intermediate_output = self.intermediate(attention_output)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_transformers/modeling_bert.py\", line 383, in forward\n",
            "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_transformers/modeling_bert.py\", line 142, in gelu\n",
            "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.90 GiB total capacity; 14.91 GiB already allocated; 33.75 MiB free; 14.99 GiB reserved in total by PyTorch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA9IfCqJ3aQX"
      },
      "source": [
        "#ls -1 | wc -l\n",
        "#!mv dm/*.story .\n",
        "#!find dm -name '*.story' -exec mv {} . \\;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2_V5HGKl9Vy"
      },
      "source": [
        "while True: pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}