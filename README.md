# TextSummarization
Repo for proj4 of Neural Networks for NLP, Spring 2021, Carnegie Mellon University.

**Abstract**

The success of BERT on many NLP tasks has led to its adoption for document encoding in state-of-the-art text summarization models. We explore extractive text summarization using pretrained transformers. Specifically, we fine-tune RoBERTa, which combines both word and sentence representations in a single very large Transformer. We found that removing interval segments from RoBERTa performed the best, improving on our results presented in project 3 and on the results of the modified version of BERT called BERTSUMEXT given by Liu et al. in the CNN/Daily Mail dataset. Additionally, we test two more models, Transformer XL and a modified version of BERT, which lay the ground for the next steps of this research.
